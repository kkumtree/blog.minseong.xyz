<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>argocd on kkumtree</title>
    <link>https://blog.minseong.xyz/tags/argocd/</link>
    <description>Recent content in argocd on kkumtree</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko-kr</language>
    <lastBuildDate>Sat, 22 Nov 2025 20:56:43 +0900</lastBuildDate><atom:link href="https://blog.minseong.xyz/tags/argocd/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ArgoCD Cluster 및 Prefix 관리 - CI/CD 스터디 6주차</title>
      <link>https://blog.minseong.xyz/post/argocd-cluster-management-and-prefix/</link>
      <pubDate>Sat, 22 Nov 2025 20:56:43 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/argocd-cluster-management-and-prefix/</guid>
      <description>CloudNet@에서 진행하고 있는 CI/CD Study 6주차에는 ArgoCD를 마지막으로 다루었습니다.
Cluster를 추가해보고 Gitea를 붙이기 전에, ArgoCD를 Prefix로 라우팅하려고 했는데 로그아웃하고 나서 원치않는 경로로 빠지는 바람에
이것저것 살펴보고 수정을 하여 원하는 대로 구동되도록 셋업했습니다.
0. 실습 준비 해당 구성들은 아래 GitHub에 탑재되어 있습니다.
https://github.com/kkumtree/ci-cd-cloudnet-study 의 6w 폴더
이전 포스팅 Tailscale을 타고, ArgoCD에 접근해보기을 하였다면, 리소스 정리를 합니다.
kind 배포 시, 포트 점유로 오류가 발생합니다.
sudo tailscale serve --tcp 443 off 이후 실습을 위한 배포를 합니다.</description>
    </item>
    
    <item>
      <title>Tailscale을 타고, ArgoCD에 접근해보기</title>
      <link>https://blog.minseong.xyz/post/playing-argocd-with-tailscale/</link>
      <pubDate>Mon, 17 Nov 2025 10:23:03 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/playing-argocd-with-tailscale/</guid>
      <description>이전 게시물, ArgoCD with Ingress의 도메인 설정을 하다가 문득, Tailscale의 serve기능을 활용하여 Tailscale 내부 네트워크(이하, tailnet)에서만 접근 가능한 ArgoCD 서버를 구축하면 되지 않을까? 하여 구성해보았습니다.
kind를 운용 중인 Host와 접속할 Client들에 Tailscale 설치되어 있어야합니다.
1. Tailscale과 Serve 전통적인 중앙집중식(Hub-Spoke) VPN이 아래와 같다면,
Tailscale의 경우, Mesh 네트워크의 형태를 가지며 Wireguard를 활용합니다.
구분 전통적 중앙집중식 VPN Tailscale (메쉬 VPN) 네트워크 구조 중앙 서버를 통한 모든 트래픽 경유​ P2P 직접 연결, 분산형 메쉬 네트워크​ 데이터 경로 클라이언트 → VPN 서버 → 목적지​ 클라이언트 → 목적지 (직접 연결)​ 프로토콜 OpenVPN(TCP/UDP), IKEv2, L2TP WireGuard(UDP 기반) 성능 중앙 서버 병목 발생, 지연 증가​ 직접 연결로 지연 최소화, 빠른 속도 설정 복잡도 서버 구축, 포트 포워딩 필요​ 로그인만으로 즉시 사용 가능​ NAT 통과 수동 포트 포워딩 필요​ 자동 NAT Traversal 지원 확장성 서버 용량에 따라 제한​ 각 노드 독립적, 확장 용이 보안 중앙 서버가 모든 트래픽 확인 가능​ 종단 간 암호화, P2P 전송​ Tailscale의 serve와 같은 경우는, ngrok의 기본 기능과 유사한 funnel과 달리</description>
    </item>
    
    <item>
      <title>ArgoCD with Ingress - CI/CD 스터디 5주차</title>
      <link>https://blog.minseong.xyz/post/argocd-ingress/</link>
      <pubDate>Sun, 16 Nov 2025 17:38:34 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/argocd-ingress/</guid>
      <description>CloudNet@에서 진행하고 있는 CI/CD Study 5주차에는 ArgoCD를 좀더 다루었습니다.
0. 실습 준비 해당 구성들은 아래 GitHub에 탑재되어 있습니다. https://github.com/kkumtree/ci-cd-cloudnet-study 의 5w 폴더
우선 80/443 포트를 사용할 수 있는지 확인하여야합니다. 아닌 경우, 다른 포트를 사용해야합니다.
실제로 해보았을 경우 tailscale이 포트를 사용하는 것으로 오인하여 해당 서비스를 중지해보았습니다.
다만, 단순히 kind YAML을 잘못 작성한 것으로 보입니다.
(1) kind 및 kube-ops-view 이번에는 Ingress의 배포를 하기 위한 밑작업으로
Control Node에 라벨링을 진행합니다.
이는 다음에 이어질 ingress-nginx 배포 시, nodeSeletor 조건으로 사용합니다.</description>
    </item>
    
    <item>
      <title>ArgoCD 101 - CI/CD 스터디 4주차</title>
      <link>https://blog.minseong.xyz/post/argocd-hello-world/</link>
      <pubDate>Sun, 09 Nov 2025 08:44:34 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/argocd-hello-world/</guid>
      <description>CloudNet@에서 진행하고 있는 CI/CD Study 4주차에는 ArgoCD를 다루기 시작했습니다.
Kubernetes(이하, k8s) 상에서 ArgoCD는 Controller보다는 Operator에 가까운 포지션을 갖는다고 하여,
이번 기회에 실습을 하면서 체감을 하는 것에 목적을 두었습니다.
Controller: live state(실제 상태)와 desired state(원하는 상태)가 일치하는지 관찰 및 지속적 조정 Operator: Controller가 k8s 내부 object에서 동작한다면, Operator는 k8s 외의 것들도 다룰 수 있음 해당 구성들은 아래 GitHub에 탑재되어 있습니다.
https://github.com/kkumtree/ci-cd-cloudnet-study 의 4w 폴더
0. 실습 준비 이전 게시물, Jenkins, git and kubernetes의 kind 및 kube-ops-view 설정과 동일하여 생략합니다.</description>
    </item>
    
  </channel>
</rss>
