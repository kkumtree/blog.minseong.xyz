<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CloudNet@ on kkumtree</title>
    <link>https://blog.minseong.xyz/tags/cloudnet@/</link>
    <description>Recent content in CloudNet@ on kkumtree</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko-kr</language>
    <lastBuildDate>Sun, 30 Nov 2025 08:59:34 +0900</lastBuildDate><atom:link href="https://blog.minseong.xyz/tags/cloudnet@/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Vault 101 in Kubernetes - CI/CD 스터디 7주차</title>
      <link>https://blog.minseong.xyz/post/vault-101-in-kubernetes/</link>
      <pubDate>Sun, 30 Nov 2025 08:59:34 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/vault-101-in-kubernetes/</guid>
      <description>CloudNet@에서 진행하고 있는 CI/CD Study 7주차에는 Vault를 다루었습니다.
자세한 설명은 해당 공식 페이지에서 해주고 있지만, 그저 1password 같은 패스워드 관리 서비스가 엔드유저 대상이라면 Vault는 인프라 관리자 대상으로 사용되는 것으로 알고 있는 제게는 흥미로운 주차였습니다.
이번 스터디에서는 계속해서 kind로 로컬 Kubernetes(k8s)를 활용했기에, 이번에도 비슷하게 배포해보겠습니다.
0. 실습 환경 준비 - kind 클러스터 배포 해당 구성들은 아래 GitHub에 탑재되어 있습니다.
https://github.com/kkumtree/ci-cd-cloudnet-study 의 7w 폴더
kind create cluster --name vault --image kindest/node:v1.32.8 --config - &amp;lt;&amp;lt;EOF kind: Cluster apiVersion: kind.</description>
    </item>
    
    <item>
      <title>ArgoCD Cluster 및 Prefix 관리 - CI/CD 스터디 6주차</title>
      <link>https://blog.minseong.xyz/post/argocd-cluster-management-and-prefix/</link>
      <pubDate>Sat, 22 Nov 2025 20:56:43 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/argocd-cluster-management-and-prefix/</guid>
      <description>CloudNet@에서 진행하고 있는 CI/CD Study 6주차에는 ArgoCD를 마지막으로 다루었습니다.
Cluster를 추가해보고 Gitea를 붙이기 전에, ArgoCD를 Prefix로 라우팅하려고 했는데 로그아웃하고 나서 원치않는 경로로 빠지는 바람에
이것저것 살펴보고 수정을 하여 원하는 대로 구동되도록 셋업했습니다.
0. 실습 준비 해당 구성들은 아래 GitHub에 탑재되어 있습니다.
https://github.com/kkumtree/ci-cd-cloudnet-study 의 6w 폴더
이전 포스팅 Tailscale을 타고, ArgoCD에 접근해보기을 하였다면, 리소스 정리를 합니다.
kind 배포 시, 포트 점유로 오류가 발생합니다.
sudo tailscale serve --tcp 443 off 이후 실습을 위한 배포를 합니다.</description>
    </item>
    
    <item>
      <title>Tailscale을 타고, ArgoCD에 접근해보기</title>
      <link>https://blog.minseong.xyz/post/playing-argocd-with-tailscale/</link>
      <pubDate>Mon, 17 Nov 2025 10:23:03 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/playing-argocd-with-tailscale/</guid>
      <description>이전 게시물, ArgoCD with Ingress의 도메인 설정을 하다가 문득, Tailscale의 serve기능을 활용하여 Tailscale 내부 네트워크(이하, tailnet)에서만 접근 가능한 ArgoCD 서버를 구축하면 되지 않을까? 하여 구성해보았습니다.
kind를 운용 중인 Host와 접속할 Client들에 Tailscale 설치되어 있어야합니다.
1. Tailscale과 Serve 전통적인 중앙집중식(Hub-Spoke) VPN이 아래와 같다면,
Tailscale의 경우, Mesh 네트워크의 형태를 가지며 Wireguard를 활용합니다.
구분 전통적 중앙집중식 VPN Tailscale (메쉬 VPN) 네트워크 구조 중앙 서버를 통한 모든 트래픽 경유​ P2P 직접 연결, 분산형 메쉬 네트워크​ 데이터 경로 클라이언트 → VPN 서버 → 목적지​ 클라이언트 → 목적지 (직접 연결)​ 프로토콜 OpenVPN(TCP/UDP), IKEv2, L2TP WireGuard(UDP 기반) 성능 중앙 서버 병목 발생, 지연 증가​ 직접 연결로 지연 최소화, 빠른 속도 설정 복잡도 서버 구축, 포트 포워딩 필요​ 로그인만으로 즉시 사용 가능​ NAT 통과 수동 포트 포워딩 필요​ 자동 NAT Traversal 지원 확장성 서버 용량에 따라 제한​ 각 노드 독립적, 확장 용이 보안 중앙 서버가 모든 트래픽 확인 가능​ 종단 간 암호화, P2P 전송​ Tailscale의 serve와 같은 경우는, ngrok의 기본 기능과 유사한 funnel과 달리</description>
    </item>
    
    <item>
      <title>ArgoCD with Ingress - CI/CD 스터디 5주차</title>
      <link>https://blog.minseong.xyz/post/argocd-ingress/</link>
      <pubDate>Sun, 16 Nov 2025 17:38:34 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/argocd-ingress/</guid>
      <description>CloudNet@에서 진행하고 있는 CI/CD Study 5주차에는 ArgoCD를 좀더 다루었습니다.
0. 실습 준비 해당 구성들은 아래 GitHub에 탑재되어 있습니다. https://github.com/kkumtree/ci-cd-cloudnet-study 의 5w 폴더
우선 80/443 포트를 사용할 수 있는지 확인하여야합니다. 아닌 경우, 다른 포트를 사용해야합니다.
실제로 해보았을 경우 tailscale이 포트를 사용하는 것으로 오인하여 해당 서비스를 중지해보았습니다.
다만, 단순히 kind YAML을 잘못 작성한 것으로 보입니다.
(1) kind 및 kube-ops-view 이번에는 Ingress의 배포를 하기 위한 밑작업으로
Control Node에 라벨링을 진행합니다.
이는 다음에 이어질 ingress-nginx 배포 시, nodeSeletor 조건으로 사용합니다.</description>
    </item>
    
    <item>
      <title>ArgoCD 101 - CI/CD 스터디 4주차</title>
      <link>https://blog.minseong.xyz/post/argocd-hello-world/</link>
      <pubDate>Sun, 09 Nov 2025 08:44:34 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/argocd-hello-world/</guid>
      <description>CloudNet@에서 진행하고 있는 CI/CD Study 4주차에는 ArgoCD를 다루기 시작했습니다.
Kubernetes(이하, k8s) 상에서 ArgoCD는 Controller보다는 Operator에 가까운 포지션을 갖는다고 하여,
이번 기회에 실습을 하면서 체감을 하는 것에 목적을 두었습니다.
Controller: live state(실제 상태)와 desired state(원하는 상태)가 일치하는지 관찰 및 지속적 조정 Operator: Controller가 k8s 내부 object에서 동작한다면, Operator는 k8s 외의 것들도 다룰 수 있음 해당 구성들은 아래 GitHub에 탑재되어 있습니다.
https://github.com/kkumtree/ci-cd-cloudnet-study 의 4w 폴더
0. 실습 준비 이전 게시물, Jenkins, git and kubernetes의 kind 및 kube-ops-view 설정과 동일하여 생략합니다.</description>
    </item>
    
    <item>
      <title>Jenkins, git and kubernetes - CI/CD 스터디 3주차</title>
      <link>https://blog.minseong.xyz/post/jenkins-ci-cd-kubernetes/</link>
      <pubDate>Sun, 02 Nov 2025 08:51:39 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/jenkins-ci-cd-kubernetes/</guid>
      <description>CloudNet@에서 진행하고 있는 CI/CD Study 3주차에는 Jenkins와 ArgoCD을 다뤘습니다.
이번에는 kubernetes(이하, k8s)에 self-host Git과 Jenkins를 배포 후 CI/CD 부분을 다루도록 하겠습니다.
하다보니 개인적으로, 아래 3가지가 주로 기억에 남았던 것 같습니다.
Docker UDS의 GID Gitea와 Multibranch Pipeline의 결합 Local PV의 Taint 및 Node 지정 더불어, Gitea에 대해 Basic Auth를 통한 CLI 접근을 막아보는 것도 새로이 해보았습니다.
해당 구성들은 아래 GitHub에 탑재되어 있습니다.
https://github.com/kkumtree/ci-cd-cloudnet-study 의 3w 폴더
0. 실습 준비 (1) kind kind 설치의 경우 다음 포스트를 참고할 수 있습니다.</description>
    </item>
    
    <item>
      <title>Helm 템플릿으로 재사용성 높이기 - CI/CD 스터디 2주차</title>
      <link>https://blog.minseong.xyz/post/helm-template-reusing-statements/</link>
      <pubDate>Sun, 26 Oct 2025 03:16:04 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/helm-template-reusing-statements/</guid>
      <description>이번에는 재사용성을 위해서 _helpers.tpl 파일을 활용해보겠습니다.
기존의 Helm 101 - CI/CD 스터디 2주차에서 이어집니다.
1. _helpers.tpl을 통한 공통 변수 재사용 (1) 공통사항 숙지 이전에 생성한 deployment.yaml과 service.yaml의 selector 부분에 공통점이 있습니다.
# `{{ .Values.replicaCount }} ## deployment.yaml spec.selector.matchLabels spec.template.metadata.labels ## service.yaml spec.selector (2) 템플릿 생성 해당 label을 추가/삭제하려면, 여러 필드를 업데이트를 하여야합니다.
대신 _helpers.tpl파일을 생성하여 구성읋 합니다.
_helpers.tpl 뿐만 아니라, 맨 앞에 _로 시작하기만 하면 되며,
이 파일은 k8s manifest 파일로 취급되지 않습니다.</description>
    </item>
    
    <item>
      <title>Helm 101 - CI/CD 스터디 2주차</title>
      <link>https://blog.minseong.xyz/post/helm-hello-world/</link>
      <pubDate>Fri, 24 Oct 2025 01:17:39 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/helm-hello-world/</guid>
      <description>CloudNet@에서 진행하고 있는 CI/CD Study 2주차에는 Helm과 Tekton을 다뤘습니다.
이번에는 Helm의 기본적인 부분을 다루도록 하겠습니다.
1. Helm의 역할과 실습 준비사항 Helm? 템플릿 기반 솔루션. 즉, 버전 관리 및 공유, 배포가 가능한 아티팩트를 생성하도록 돕습니다.
Helm chart(차트)
공유 가능한 Kubernetes(쿠버네티스, 이하 k8s) 패키지며, 차트 간 의존성 등 다양한 요소를 포함합니다. k8s를 다루면 Helm을 많이 사용하게 되는데, 그 이유 중 하나가 chart 개념입니다. Rolling Update for ConfigMap
애플리케이션의 설정값은 일반적으로 k8s의 ConfigMap에 대응되는 속성입니다.</description>
    </item>
    
    <item>
      <title>VSCode DevContainer - CI/CD 스터디 1주차</title>
      <link>https://blog.minseong.xyz/post/vscode-devcontainer-usage/</link>
      <pubDate>Sun, 19 Oct 2025 20:50:46 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/vscode-devcontainer-usage/</guid>
      <description>한가위 연휴의 끝과 함께, CloudNet@에서 진행하고 있는 CI/CD Study에 참여하게 되었습니다.
이번에는 핸즈온용으로 즐겨쓰는 GitHub CodeSpace와 연관된,
Visual Studio 상에서의 Dev Containers 활용에 대해 다뤄보고자 합니다.
사용 OS환경은 Ubuntu Desktop 24.04 LTS 이며,
아래의 문서에서 안내된대로 차근차근 따라해보며 좀 더 이해를 해보고자 합니다.
Developing inside a Container
0. Docker 설치 내용이 길어, 아래의 포스트로 나누었습니다.
Ubuntu Docker 설치 작성 기준, Dev Container는 Ubuntu Snap 패키지(snapcraft)로 설치된 Docker에는 지원되지 않는다고 합니다. 사용자($USER)를 docker 그룹에 추가하여야합니다.</description>
    </item>
    
    <item>
      <title>TFC(Terraform Cloud) drift 알림 설정</title>
      <link>https://blog.minseong.xyz/post/notification-about-terraform-cloud-drift/</link>
      <pubDate>Sun, 15 Oct 2023 00:10:33 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/notification-about-terraform-cloud-drift/</guid>
      <description>CloudNet@에서의 Terraform 스터디가 끝나고 나서,
테라폼을 실제 운영 상황에 도입하면서 마주할 수 밖에 없는 드리프트(drift) 상황에 대해
이해해보는 시간을 가졌습니다.
참고) TFC에서의 Drift Detection 기능은 현재 TFC Plus 에디션에서 지원됩니다. 1. 용어 이해해보기 사실은 작년부터 테라폼을 접하고나서, IaC라는 개념에 꽂히기만 했지
운영 입장에서 마주했었던 수많은 시행착오들을 흔한 유저에러로만 생각해왔었습니다.
스터디에 참여하면서 종종 &amp;lsquo;드리프트&amp;rsquo;라는 단어를 듣고, 찾아보니
상당부분이 이에 속하는 상황이라는 것을 알 수 있었습니다.
(1) Drift? 글 작성을 위해 찾아본 기술적 Drift는 본래 주행에 있어서의 그것과</description>
    </item>
    
    <item>
      <title>Understanding terraform module</title>
      <link>https://blog.minseong.xyz/post/architecting-aws-with-terraform-module/</link>
      <pubDate>Wed, 04 Oct 2023 11:24:13 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/architecting-aws-with-terraform-module/</guid>
      <description>This week is last week of CloudNet@ group study about terraform.
In this study, my personal goal is making AWS architecture only with terraform and one tfstate file.
Basic knowledge about AWS resources is required. 1. Terraform without Module Before, I already met terraform for maintaining AWS in production level.
But at that time, our team maintain them as folder structure which is used by terraformer
# example structure $ tree .</description>
    </item>
    
    <item>
      <title>IAM STS를 이용한 Terraform Cloud 권한 부여</title>
      <link>https://blog.minseong.xyz/post/terraform-cloud-with-iam-sts/</link>
      <pubDate>Wed, 13 Sep 2023 20:54:28 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/terraform-cloud-with-iam-sts/</guid>
      <description>이번에는 Terraform Cloud가 얼마나 좋은지 더 알아보기 위해,
스터디에서 지속적으로 장점이 강조되어 왔던 Terraform Cloud에
IAM STS를 이용한 권한 부여 도전 및 적용 성공에 대해 써보려고 합니다.
Terraform의 상태 저장을 위해 보통 AWS S3를 사용하는데,
알다시피 S3 기록은 무료지만, 불러오는 것은 유료입니다.
(전기는 국산이지만, 원료는 수입입니다)
그래서 스터디용으로는 Terraform을 불러올 때마다,
상태 값을 S3말고, 로컬에 저장했었는데요.
밖에서는 노트북, 집에서는 데스크탑으로 하려니
이걸 GitHub의 Private Repo에 저장할까? 하다가
Terraform Cloud를 써보기로 했습니다.</description>
    </item>
    
    <item>
      <title>Terraform resource 이해하기 w/AWS VPC</title>
      <link>https://blog.minseong.xyz/post/terraform-basic-resource-concept/</link>
      <pubDate>Fri, 08 Sep 2023 22:41:14 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/terraform-basic-resource-concept/</guid>
      <description>이번에는 CloudNet@를 통해 학습한 내용을 기반으로,
AZ를 대상으로 한 data 조회 AWS VPC 생성 예제로 살펴보는 output resource 이름 변경 순으로 알아보도록 하겠습니다.
교재로 사용한 [테라폼으로 시작하는 IaC] 도 참고하였습니다.
기본 설정 aws-cli에 리전을 ap-northeast-2을 설정하였습니다. $ aws configure list Name Value Type Location ---- ----- ---- -------- profile &amp;lt;not set&amp;gt; None None access_key ****************2U5J shared-credentials-file secret_key ****************Z0co shared-credentials-file region ap-northeast-2 config-file ~/.aws/config 1. data 조회 data는 사용자가 정의하는 resource 및 리소스에 대한 스펙과 반대로,</description>
    </item>
    
    <item>
      <title>Terraform 시작하기 w/Minimal Ubuntu</title>
      <link>https://blog.minseong.xyz/post/terraform-hello-world-tfenv/</link>
      <pubDate>Thu, 31 Aug 2023 22:21:08 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/terraform-hello-world-tfenv/</guid>
      <description>이번에는 CloudNet@에서 진행하는 Terraform 스터디(이하, T101)에 참여했습니다.
Terraform을 쓰면 왜 좋은지는 자세하고 전문적인 글이 있으므로, 참고하시면 좋을 것 같습니다. (링크: 44bits)
예전에 테라폼을 썼던 적이 있지만, Module화가 어렵기도 하고
이번 기회에 테라폼 신간을 다시 복기하는 마음으로 참여했습니다.
사용한 교재는 [테라폼으로 시작하는 IaC] 입니다.
이번에는 Terraform 초기 셋업에 대해, 살펴보고
시험삼아 Canonical 공식 Minimal Ubuntu(ARM64) AMI를 설치해보겠습니다.
Terraform 설치 tfenv 사용과 .tf 작성 따라하기 Hello World in terraform Terraform 써보기 순으로 진행합니다.</description>
    </item>
    
    <item>
      <title>AWS EKS 스터디 7주차 - Automation</title>
      <link>https://blog.minseong.xyz/post/aws-eks-study-week7/</link>
      <pubDate>Sat, 10 Jun 2023 15:13:19 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/aws-eks-study-week7/</guid>
      <description>EKS 스터디도 마지막 7주차를 맞이했습니다.
이번에는 AWS Controller for k8s(ACK)와 flux를 가볍게 실습해보고
자동화에 대해 맛보기를 해보았습니다.
앞서 학습해본 IRSA 개념 외에도 CRD(CustomResourceDefinition)을 활용합니다.
1. 실습환경 배포 실습을 위한 YAML파일이 변경된거 말고는 6주차와 유사합니다.
curl -O https://s3.ap-northeast-2.amazonaws.com/cloudformation.cloudneta.net/K8S/eks-oneclick6.yaml # 이하 중략 # CERT_ARN(ACM)의 경우에는 /etc/profile에 환경변수 저장을 안해둬서 # 세션이 만료되면, 다시 재설정 필요 CERT_ARN=`aws acm list-certificates --query &amp;#39;CertificateSummaryList[].CertificateArn[]&amp;#39; --output text` echo $CERT_ARN 2. ACK(AWS Controller for k8s) 웹콘솔에 접근하지 않고도, AWS 서비스 리소스를 직접 k8s에서 정의 및 사용가능 순서: ACK 컨트롤러 설치 -&amp;gt; IRSA 설정 -&amp;gt; AWS 리소스 컨트롤 같은 패턴으로 이루어져있는데, Cloudformation을 쓰다보니 중간중간 대기 시간 발생 (23/05/29) GA: 17개 서비스, Preview: 10개 서비스 2-1.</description>
    </item>
    
    <item>
      <title>AWS EKS 스터디 6주차 - Security</title>
      <link>https://blog.minseong.xyz/post/aws-eks-study-week6/</link>
      <pubDate>Sun, 04 Jun 2023 06:56:52 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/aws-eks-study-week6/</guid>
      <description>이번에는 보안을 위한 인증 및 인가, 그리고 IRSA를 중심으로 EKS의 보안에 대해 학습해보았습니다.
kops 스터디 때에는 잘 몰랐는데, RBAC 뿐만 아니라 복기하다보니&amp;hellip;
[4-1] projected Volume [4-2] AWS Load Balancer Controller IRSA 및 LB Pod mutating 위의 두 가지가 중요한 파트를 차지하고 있었음을 알 수 있었습니다.
Network(2주차)가 매번 뭔가 일부가 아리송하였다면
Security는 복기하다가 이론적으로는 간단(과연?)해보여도
실제 구동방식 이해 자체가 초반에 안되서, 사흘 남짓 걸린 덕에 더 어려웠던 것 같습니다.
그 외 myeks-bastion-2에 접속 시, 함께 진행할 때는 ssh {Public IP}로 잘 접속되는 걸 봤는데 정작 혼자 할 땐 접속이 되지않았습니다.</description>
    </item>
    
    <item>
      <title>AWS EKS 스터디 5주차 - Autoscaling</title>
      <link>https://blog.minseong.xyz/post/aws-eks-study-week5/</link>
      <pubDate>Mon, 22 May 2023 19:23:37 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/aws-eks-study-week5/</guid>
      <description>이번 주차는 오토스케일링을 메인으로 하여, 수평/수직 프로비저닝을 학습해보았습니다.
마지막에는 고성능 오토스케일러인 Karpenter를 별도로 실습해보았습니다. 특히..
HPA custom metrics(사용자 정의 메트릭) 적용
YAML 설정값을 CPU로 맞춘 것을 잊고, 프로비저닝을 잘못 예측한 것도 함께 공유합니다.
AutoScaling
HPA: Horizontal Pod Autoscaler VPA: Vertical Pod Autoscaler CA: Cluster Autoscaler 각 CSP 의존적, 워커 노드 레벨에서의 오토스케일링 1. 실습 환경 배포 4주차의 초기 배포 내용에 p8s 및 Grafana를 추가하여 배포 verticalPodAutoscaler 활성화 추천 대시보드: 15757, 17900, 15172 curl -O https://s3.</description>
    </item>
    
    <item>
      <title>AWS EKS 스터디 4주차 - Observability</title>
      <link>https://blog.minseong.xyz/post/aws-eks-study-week4/</link>
      <pubDate>Sun, 21 May 2023 06:13:52 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/aws-eks-study-week4/</guid>
      <description>이번 주차에는 Observability에 대해 스터디가 진행되었습니다.
자원 모니터링 툴들의 적용 및 사용이 중심입니다.
그나저나 k8s 1.26에서 metrics의 일부 명칭이 바뀌는 걸 보고 식겁했습니다.
(etcd_db_total_size_bytes 대신, apiserver_storage_db_total_size_in_bytes 으로 변경)
또한 kubecost의 경우, cloudformation 스택 제거 후에도 볼륨 데이터가 남아있어서 별도로 삭제해야 했습니다.
1. 실습환경 배포 NAT게이트웨이, EBS addon, IAM role, ISRA for LB/EFS, PreCommand 포함 노드: t3.xlarge t3a.xlarge(AMD)는 서울 리전 b AZ(ap-northeast-2b)에서 미지원 더 많은 값들이 입력되어서, 생성 완료까지 더 많은 시간이 소요 (약 20여분 이내) curl -O https://s3.</description>
    </item>
    
    <item>
      <title>AWS EKS 스터디 3주차 - Storage</title>
      <link>https://blog.minseong.xyz/post/aws-eks-study-week3/</link>
      <pubDate>Fri, 12 May 2023 05:36:38 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/aws-eks-study-week3/</guid>
      <description>이번 주차에는 스토리지에 대해 실습을 진행해보았습니다. 지난번 kOps 스터디에서 다루었던 내용이지만, 부족했던 내용을 보충하면서 작성을 해보았습니다.
주요한 내용은&amp;hellip;
NodeAffinity를 이용한 라벨링 AWS EBS controller의 경우, AWS managed policy를 활용 AWS Volume SnapShots Controller를 통한 볼륨 백업 AWS EFS controller에서의 동적 프로비저닝 AWS EKS 신규 노드그룹 생성 별도로 kube-ops-view의 경우, 웹으로 확인할 수 있을 때까지 시간이 소요된다는 점이 있습니다.
1. 실습 환경 배포 2주차에 실습했던 내용들을 미리 배포 AWS LB ExternalDNS kube-ops-view context 이름 변경 지난 번까지 pkos가 뜨는 현상이 있었는데, 닉네임을 별도 지정할 수 있음 EFS 생성 관련 cloudformation이 추가되었음 EFS FS ID 조회를 하기 위해 aws-cli 필터 활용 (출처: AWS Docs) # 실습 YAML 파일 curl -O https://s3.</description>
    </item>
    
    <item>
      <title>AWS EKS 스터디 2주차 - Network</title>
      <link>https://blog.minseong.xyz/post/aws-eks-study-week2/</link>
      <pubDate>Sun, 07 May 2023 07:30:52 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/aws-eks-study-week2/</guid>
      <description># 아쉽게도 신규 항목인 istio, kube-ops-view는 실습 실패 - istio: `myhome.yaml` 을 어떻게 생성할지 몰라서 중단 - kube-ops-view: A레코드에 제대로 잡히지 않음 지난 1주차에 이어, 이번 주에는 EKS의 네트워크 구성에 대해 알아보는 시간이었습니다.
직전 스터디에서도 바로 광탈당하나?하며 밤과 주말을 하얗게 불태웠을 정도로
가장 고난도라고 생각했던 네트워크를 다시 만나니 이제 1% 친근감이 느껴지고 있네요.
자 그럼 해보도록 합시다.
1. cloudformation을 활용한 EKS 원클릭 구성 학습을 위해, 이번에도 가시다님이 준비해주신 원클릭 배포 yaml을 활용하여 배포.</description>
    </item>
    
    <item>
      <title>AWS EKS 스터디 1주차</title>
      <link>https://blog.minseong.xyz/post/aws-eks-study-week1/</link>
      <pubDate>Sun, 30 Apr 2023 03:00:15 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/aws-eks-study-week1/</guid>
      <description>최근 CloudNet@에서 진행하고 있는
AWS EKS Workshop Study(이하, AEWS)에 참여하게 되었습니다.
k8s가 워낙 인기가 많기도 하지만, 지난 kOps 스터디를 통해 관리요소가 참 많은 것을 느꼈었고,
좀더 수월하게 이해를 해보고자 AWS 서비스인
EKS(Elastic Kubernetes Service)를 이번 기회에 살펴보기로 했습니다.
EKS 사용에 있어 고려사항 EKS는 관리형 서비스(managed service)이기에 아래와 같은 장점이 있습니다.
클러스터링을 위한 Control Plane(일명, 마스터 노드)을 AWS에서 관리해줍니다. 워커노드는 사용자가 AMI를 구성하여 이를 사용 AWS에서 제공하는 Fargate로 VM을 할당하여 사용 kOps와도 유사하지만, 다른 AWS 서비스와의 연동이 용이합니다.</description>
    </item>
    
    <item>
      <title>kops로 손쉽게 spot instance 요청하기</title>
      <link>https://blog.minseong.xyz/post/how-to-request-spot-with-kops/</link>
      <pubDate>Mon, 06 Mar 2023 18:43:51 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/how-to-request-spot-with-kops/</guid>
      <description>kops로 충분히 spot instance를 굴릴 수 있지 않을까? 란 생각을 pkos 스터디 내내 하고 있었습니다만,
이미 t3.small 돌렸다가 제대로 노드들이 작동하지도 않았음을 맛봤기 때문에&amp;hellip;
우선순위는 주차별 과제 제출이었기 때문에 이제야 시범 테스트를 해보고, 글을 작성해봅니다. 요약하자면 복습용으로는 충분히 가능하다. 라는 판단입니다. 참고로, 손쉽게 === 야매 의 느낌으로 기술해보았습니다. 글 읽기 귀찮다! 싶으면 여기에서 원글을 확인하실 수 있습니다. 왜 스팟 인스턴스인가? 이미 이전 포스팅의 서두에서 언급한 바 있으나,
아무리 클라우드 서비스가 합리적이어도 On-demand 인스턴스를 학습용으로 사용하기엔 살짝 비효율적이라는 생각이 들었습니다.</description>
    </item>
    
    <item>
      <title>AWS 트러블 슈팅 - 7주 간의 k8s 실무 스터디를 마치며</title>
      <link>https://blog.minseong.xyz/post/basic-aws-troubleshooting/</link>
      <pubDate>Sat, 25 Feb 2023 22:07:23 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/basic-aws-troubleshooting/</guid>
      <description>최근 CloudNet@에서 진행하고 있는 Production Kubernetes Online Study(이하, PKOS)도 마지막 주차가 끝났습니다.
남은 계획은 4월부터 시작되는 휴식기간(a.k.a. 계약만료 후)중 복습 겸, 포스팅을 해보는 것입니다.
오늘은 아래와 같이 실패를 거듭하여 추가로 알게된 소소한 트러블슈팅을 정리해보려고 합니다.
(물론, 새벽에 겨우겨우 주차별 과제 제출하고 기절하는 바람에, 맛점하다 깨닫고 소스라치게 휴대폰으로 수동 삭제한게 비용의 주 요인이긴 하네요;)
불필요한 Try &amp;amp; Fail을 줄이고, 즐거운 k8s 학습되시기 바랍니다.
1. AWS 계정 잠금 해제 상황 처음 1주차 안내가 끝나고 Route53 Hostname을 구매하려고 하니, 자꾸 결제가 실패하는 문제가 발생했습니다.</description>
    </item>
    
    <item>
      <title>Traefik을 활용한 minikube 예제 구현시도 w/Apple Silicon</title>
      <link>https://blog.minseong.xyz/post/traefik-with-minikube-in-m1/</link>
      <pubDate>Sun, 05 Feb 2023 04:54:06 +0900</pubDate>
      
      <guid>https://blog.minseong.xyz/post/traefik-with-minikube-in-m1/</guid>
      <description>주변으로부터 피드백을 받은 내용이 있어 새로운 글로 보완예정입니다. - colima는 containerd처럼 cri가 아닌, Docker engine과 containerd 사이의 물건으로 추정됩니다. - 도커 엔진은 현재 containerd를 통해 프로세스를 관리. - colima도 docker shim 구조는 탈피했을 거라고 추측 중. - 도커 엔진과 containerd 사이의 컨테이너 엔진(관리도구?)로 보임 - colima 시작 시, 특정 런타임을 선택할 수 있습니다. 문서를 잘 읽어봅시다. `colima start --runtime containerd` - k8s를 위한 colima 시작 명령어는 별도로 있습니다. colima github를 참고해주세요.</description>
    </item>
    
  </channel>
</rss>
